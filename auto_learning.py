#!/usr/bin/env python3
"""
è‡ªä¸»å­¦ä¹ æ¨¡å— - è‡ªåŠ¨ä¸‹è½½å®«å´éªé£æ ¼å›¾ç‰‡å¹¶è¿›è¡Œæ·±åº¦å­¦ä¹ 
"""

import os
import time
import random
from PIL import Image
import cv2
import numpy as np

class GhibliAutoLearner:
    """å®«å´éªé£æ ¼è‡ªä¸»å­¦ä¹ å™¨"""
    
    def __init__(self, download_folder="temp/learning"):
        self.download_folder = download_folder
        self.learning_images = []
        os.makedirs(download_folder, exist_ok=True)
        
        # å®«å´éªç›¸å…³æœç´¢å…³é”®è¯
        self.search_keywords = [
            "å®«å´éªåŠ¨æ¼«", "å‰åœåŠ›å·¥ä½œå®¤", "åƒä¸åƒå¯»", "é¾™çŒ«", "å“ˆå°”çš„ç§»åŠ¨åŸå ¡",
            "å¤©ç©ºä¹‹åŸ", "å¹½çµå…¬ä¸»", "é­”å¥³å®…æ€¥ä¾¿", "é£ä¹‹è°·", "çº¢çŒª",
            "æ‚¬å´–ä¸Šçš„é‡‘é±¼å§¬", "èµ·é£äº†", "ä¾§è€³å€¾å¬", "çŒ«çš„æŠ¥æ©"
        ]
        
        # ç”¨æˆ·ä»£ç†åˆ—è¡¨
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
        ]
    
    def get_random_user_agent(self):
        """è·å–éšæœºç”¨æˆ·ä»£ç†"""
        return random.choice(self.user_agents)
    
    def search_ghibli_images(self, keyword, max_images=10):
        """æœç´¢å®«å´éªé£æ ¼å›¾ç‰‡ - ä¸»è¦ä½¿ç”¨å¿…åº”æœç´¢"""
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
        
        # ä¸»è¦ä½¿ç”¨å¿…åº”æœç´¢ï¼ˆæ›´å¯é ï¼‰
        downloaded_count = self._search_backup_images(keyword, max_images)
        
        # å¦‚æœå¿…åº”æœç´¢å¤±è´¥ï¼Œå†å°è¯•ç™¾åº¦
        if downloaded_count == 0:
            print(f"âš ï¸ å¿…åº”æœç´¢å¤±è´¥ï¼Œå°è¯•ç™¾åº¦æœç´¢...")
            downloaded_count = self._search_baidu_images(keyword, max_images)
        
        return downloaded_count
    
    def _search_baidu_images(self, keyword, max_images):
        """ç™¾åº¦å›¾ç‰‡æœç´¢ - ä½¿ç”¨æ›´å¯é çš„æ–¹æ³•"""
        headers = {
            'User-Agent': self.get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://image.baidu.com/',
        }
        
        try:
            # ä½¿ç”¨æ›´ç®€å•çš„ç™¾åº¦å›¾ç‰‡æœç´¢URL
            search_url = f"https://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&dyTabStr=MCwzLDYsMiw0LDUsNyw4LDksMTAsMTEsMTIsMTMsMTQsMTUsMTYsMTcsMTgsMTksMjAsMjEsMjIsMjMsMjQsMjUsMjYsMjcsMjgsMjksMzAsMzEsMzIsMzMsMzQsMzUsMzYsMzcsMzgsMzksNDAsNDEsNDIsNDMsNDQsNDUsNDYsNDcsNDgsNDksNTAsNTEsNTIsNTMsNTQsNTUsNTYsNTcsNTgsNTksNjAsNjEsNjIsNjMsNjQsNjUsNjYsNjcsNjgsNjksNzAsNzEsNzIsNzMsNzQsNzUsNzYsNzcsNzgsNzksODAsODEsODIsODMsODQsODUsODYsODcsODgsODksOTAsOTEsOTIsOTMsOTQsOTUsOTYsOTcsOTgsOTksMTAwLDEwMSwxMDIsMTAzLDEwNCwxMDUsMTA2LDEwNywxMDgsMTA5LDExMCwxMTEsMTEyLDExMywxMTQsMTE1LDExNiwxMTcsMTE4LDExOSwxMjAsMTIxLDEyMiwxMjMsMTI0LDEyNSwxMjYsMTI3LDEyOCwxMjksMTMwLDEzMSwxMzIsMTMzLDEzNCwxMzUsMTM2LDEzNywxMzg&word={keyword}"
            
            response = requests.get(search_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # ä»HTMLé¡µé¢ä¸­æå–å›¾ç‰‡URL
            image_urls = self.extract_image_urls_from_html(response.text)
            
            # å¦‚æœHTMLè§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
            if not image_urls:
                print("âš ï¸ HTMLè§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ³•...")
                image_urls = self.extract_image_urls_backup(response.text)
            
            downloaded_count = 0
            for i, img_url in enumerate(image_urls[:max_images]):
                if self.download_image(img_url, f"{keyword}_{i}"):
                    downloaded_count += 1
                    time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            return downloaded_count
            
        except Exception as e:
            print(f"âŒ ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return 0
    
    def _search_backup_images(self, keyword, max_images):
        """å¤‡ç”¨å›¾ç‰‡æœç´¢æº - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å›¾ç‰‡ï¼Œå…¶æ¬¡ä½¿ç”¨ç½‘ç»œå›¾ç‰‡"""
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨æœ¬åœ°å›¾ç‰‡
        local_images = self._get_local_ghibli_images(keyword)
        if local_images:
            print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°å›¾ç‰‡æº: {keyword}")
            copied_count = 0
            
            for i, img_path in enumerate(local_images[:max_images]):
                if self._copy_local_image(img_path, f"local_{keyword}_{i}"):
                    copied_count += 1
            
            if copied_count > 0:
                print(f"âœ… æœ¬åœ°å›¾ç‰‡æºä½¿ç”¨æˆåŠŸ {copied_count} å¼ å›¾ç‰‡")
                return copied_count
        
        # å¦‚æœæœ¬åœ°å›¾ç‰‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„ç½‘ç»œå›¾ç‰‡
        predefined_urls = self.get_predefined_ghibli_images(keyword)
        
        if predefined_urls:
            print(f"ğŸ” ä½¿ç”¨é¢„å®šä¹‰ç½‘ç»œå›¾ç‰‡æº: {keyword}")
            downloaded_count = 0
            
            for i, img_url in enumerate(predefined_urls[:max_images]):
                if self.download_image(img_url, f"predefined_{keyword}_{i}"):
                    downloaded_count += 1
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            if downloaded_count > 0:
                print(f"âœ… é¢„å®šä¹‰å›¾ç‰‡æºä¸‹è½½æˆåŠŸ {downloaded_count} å¼ å›¾ç‰‡")
            
            return downloaded_count
        
        # å¦‚æœé¢„å®šä¹‰å›¾ç‰‡æºå¤±è´¥ï¼Œå°è¯•å¿…åº”æœç´¢
        return self._search_bing_fallback(keyword, max_images)
    
    def _get_local_ghibli_images(self, keyword):
        """è·å–æœ¬åœ°å®«å´éªé£æ ¼å›¾ç‰‡è·¯å¾„ - ç®€åŒ–ç‰ˆæœ¬"""
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ¬åœ°å›¾ç‰‡æ–‡ä»¶å¤¹
        local_folders = [
            "ghibli_images",
            "static/ghibli_images",
            "images/ghibli",
            "static/images",
            "images"
        ]
        
        all_image_files = []
        
        for folder in local_folders:
            if os.path.exists(folder) and os.path.isdir(folder):
                # æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
                for file in os.listdir(folder):
                    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                        all_image_files.append(os.path.join(folder, file))
        
        # éšæœºé€‰æ‹©å›¾ç‰‡ï¼Œé¿å…æ¯æ¬¡éƒ½ä½¿ç”¨ç›¸åŒçš„å›¾ç‰‡
        import random
        random.shuffle(all_image_files)
        
        return all_image_files
    
    def _copy_local_image(self, source_path, filename):
        """å¤åˆ¶æœ¬åœ°å›¾ç‰‡åˆ°å­¦ä¹ æ–‡ä»¶å¤¹"""
        try:
            # è¯»å–å›¾ç‰‡
            image = Image.open(source_path)
            
            # ä¿å­˜åˆ°å­¦ä¹ æ–‡ä»¶å¤¹
            filepath = os.path.join(self.download_folder, f"{filename}.jpg")
            image.save(filepath, "JPEG", quality=90, optimize=True)
            
            print(f"âœ… å¤åˆ¶æœ¬åœ°å›¾ç‰‡: {filename} ({image.size[0]}x{image.size[1]})")
            self.learning_images.append(filepath)
            return True
            
        except Exception as e:
            print(f"âŒ å¤åˆ¶æœ¬åœ°å›¾ç‰‡å¤±è´¥ {filename}: {e}")
            return False
    
    def get_predefined_ghibli_images(self, keyword):
        """è·å–é¢„å®šä¹‰çš„å®«å´éªé£æ ¼å›¾ç‰‡URL"""
        
        # å®«å´éªé£æ ¼å­¦ä¹ å›¾ç‰‡ - ä½¿ç”¨åŠ¨æ¼«é£æ ¼å›¾ç‰‡
        # è¿™äº›æ˜¯å…¬å¼€çš„åŠ¨æ¼«é£æ ¼å›¾ç‰‡ï¼Œæ›´æ¥è¿‘å®«å´éªé£æ ¼
        ghibli_images = {
            'å®«å´éªåŠ¨æ¼«': [
                "https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800&h=600&fit=crop",  # å¹»æƒ³åŠ¨æ¼«é£æ ¼
                "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=800&h=600&fit=crop",  # è‰ºæœ¯åŠ¨æ¼«é£æ ¼
                "https://images.unsplash.com/photo-1637858868799-7f26a0640eb6?w=800&h=600&fit=crop",  # åŠ¨æ¼«æ’ç”»é£æ ¼
            ],
            'å‰åœåŠ›å·¥ä½œå®¤': [
                "https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800&h=600&fit=crop",
                "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=800&h=600&fit=crop",
                "https://images.unsplash.com/photo-1637858868799-7f26a0640eb6?w=800&h=600&fit=crop",
            ],
            'åƒä¸åƒå¯»': [
                "https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800&h=600&fit=crop",
                "https://images.unsplash.com/photo-1578662996442-48f60103fc96?w=800&h=600&fit=crop",
                "https://images.unsplash.com/photo-1637858868799-7f26a0640eb6?w=800&h=600&fit=crop",
            ]
        }
        
        return ghibli_images.get(keyword, [])
    
    def _search_bing_fallback(self, keyword, max_images):
        """å¤‡ç”¨å¿…åº”æœç´¢"""
        headers = {
            'User-Agent': self.get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        
        try:
            english_keywords = {
                'å®«å´éªåŠ¨æ¼«': 'studio ghibli anime wallpaper',
                'å‰åœåŠ›å·¥ä½œå®¤': 'studio ghibli wallpaper',
                'åƒä¸åƒå¯»': 'spirited away wallpaper',
                'é¾™çŒ«': 'totoro wallpaper',
            }
            
            english_keyword = english_keywords.get(keyword, keyword)
            
            search_url = f"https://www.bing.com/images/search?q={english_keyword}&qft=+filterui:imagesize-large"
            
            print(f"ğŸ” å¤‡ç”¨å¿…åº”æœç´¢: {english_keyword}")
            
            response = requests.get(search_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            if len(response.text) < 1000:
                print("âš ï¸ å¿…åº”å“åº”å†…å®¹è¿‡çŸ­")
                return 0
            
            image_urls = self._extract_bing_image_urls(response.text)
            
            print(f"ğŸ“· æ‰¾åˆ° {len(image_urls)} ä¸ªå›¾ç‰‡URL")
            
            downloaded_count = 0
            for i, img_url in enumerate(image_urls[:max_images]):
                if self.download_image(img_url, f"bing_{keyword}_{i}"):
                    downloaded_count += 1
                    time.sleep(3)  # æ›´é•¿çš„é—´éš”
            
            return downloaded_count
            
        except Exception as e:
            print(f"âŒ å¤‡ç”¨å¿…åº”æœç´¢å¤±è´¥: {e}")
            return 0
    
    def _extract_bing_image_urls(self, html_content):
        """ä»å¿…åº”æœç´¢ç»“æœä¸­æå–å›¾ç‰‡URL - æ”¹è¿›ç‰ˆæœ¬"""
        try:
            import re
            
            # æ–¹æ³•1: æŸ¥æ‰¾çœŸå®çš„å›¾ç‰‡URLï¼ˆæ¥è‡ªç½‘ç«™å†…å®¹ï¼‰
            pattern = r'https?:[^"\'\s<>]+\.(?:jpg|jpeg|png|webp)'
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            
            # æ¸…ç†URL
            clean_urls = []
            for url in matches:
                # å½»åº•æ¸…ç†URL
                url = url.replace('&quot;', '').replace('"', '').replace('\\', '')
                
                # è¿‡æ»¤æ‰æ˜æ˜¾æ— æ•ˆçš„URL
                if (url.startswith('http') and 
                    len(url) > 30 and 
                    ' ' not in url and
                    'bing.net/th/id/OIP-C' not in url and
                    'facebook' not in url.lower() and
                    'logo' not in url.lower()):
                    
                    # ä¿®å¤URLæ ¼å¼é—®é¢˜
                    if ':/' in url and '://' not in url:
                        url = url.replace(':/', '://', 1)
                    
                    clean_urls.append(url)
            
            # å»é‡
            clean_urls = list(set(clean_urls))
            
            print(f"ğŸ” æå–åˆ° {len(clean_urls)} ä¸ªæœ‰æ•ˆå›¾ç‰‡URL")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªURLç”¨äºè°ƒè¯•
            if clean_urls:
                for i, url in enumerate(clean_urls[:3]):
                    print(f"  {i+1}. {url[:100]}...")
            
            return clean_urls
            
        except Exception as e:
            print(f"âŒ å¿…åº”è§£æå¤±è´¥: {e}")
            return []
    
    def clean_image_url(self, url):
        """æ¸…ç†å’ŒéªŒè¯å›¾ç‰‡URL - æ”¹è¿›ç‰ˆæœ¬"""
        if not url:
            return None
        
        # æ¸…ç†URLä¸­çš„å¤šä½™å­—ç¬¦
        url = url.strip()
        
        # å¤„ç†å¸¸è§çš„URLæ ¼å¼é—®é¢˜
        url = url.replace('&quot;', '').replace('"', '')
        
        # æå–çœŸæ­£çš„å›¾ç‰‡URLï¼ˆå¤„ç†murl:å‰ç¼€ï¼‰
        if 'murl:' in url:
            parts = url.split('murl:')
            if len(parts) > 1:
                url = parts[1]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡URL
        if not any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
            return None
        
        # æ ‡å‡†åŒ–URLæ ¼å¼
        if url.startswith('//'):
            url = 'https:' + url
        elif not url.startswith('http'):
            return None
        
        # è¿‡æ»¤æ‰æ˜æ˜¾æ— æ•ˆçš„URL
        if len(url) < 15 or ' ' in url or 'murl:' in url:
            return None
        
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if not url.startswith('http://') and not url.startswith('https://'):
            return None
        
        return url
    
    def extract_image_urls_from_json(self, json_content):
        """ä»JSONæ•°æ®ä¸­æå–å›¾ç‰‡URL - æ”¹è¿›ç‰ˆæœ¬"""
        try:
            import json
            # æ¸…ç†JSONæ•°æ®ï¼Œå¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
            cleaned_content = json_content.strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„JSON
            if not cleaned_content or cleaned_content[0] not in ['{', '[']:
                print("âš ï¸ è¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                return []
            
            # å°è¯•è§£æJSON
            data = json.loads(cleaned_content)
            
            image_urls = []
            
            # å¤„ç†ä¸åŒçš„JSONç»“æ„
            if isinstance(data, dict):
                if 'data' in data:
                    for item in data['data']:
                        if isinstance(item, dict):
                            # å°è¯•å¤šç§å¯èƒ½çš„URLå­—æ®µ
                            for field in ['middleURL', 'thumbURL', 'objURL', 'hoverURL', 'fromURL']:
                                if field in item and item[field]:
                                    image_urls.append(item[field])
                                    break
            elif isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        for field in ['middleURL', 'thumbURL', 'objURL', 'hoverURL', 'fromURL']:
                            if field in item and item[field]:
                                image_urls.append(item[field])
                                break
            
            return list(set(image_urls))  # å»é‡
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"âŒ JSONå¤„ç†å¤±è´¥: {e}")
            return []
    
    def extract_image_urls_from_html(self, html_content):
        """ä»HTMLé¡µé¢ä¸­æå–å›¾ç‰‡URL - ä¸»è¦æ–¹æ³•"""
        try:
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            image_urls = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
            for img in soup.find_all('img'):
                src = img.get('src', '')
                data_src = img.get('data-src', '')
                data_url = img.get('data-url', '')
                
                # æ£€æŸ¥srcå±æ€§
                if src and self.is_valid_image_url(src):
                    full_url = self.normalize_url(src)
                    if full_url:
                        image_urls.append(full_url)
                
                # æ£€æŸ¥data-srcå±æ€§ï¼ˆæ‡’åŠ è½½å›¾ç‰‡ï¼‰
                if data_src and self.is_valid_image_url(data_src):
                    full_url = self.normalize_url(data_src)
                    if full_url:
                        image_urls.append(full_url)
                
                # æ£€æŸ¥data-urlå±æ€§
                if data_url and self.is_valid_image_url(data_url):
                    full_url = self.normalize_url(data_url)
                    if full_url:
                        image_urls.append(full_url)
            
            # æ–¹æ³•2: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾éšè—çš„å›¾ç‰‡URL
            import re
            patterns = [
                r'"objURL"\s*:\s*"([^"]+)"',
                r'"middleURL"\s*:\s*"([^"]+)"',
                r'"thumbURL"\s*:\s*"([^"]+)"',
                r'"hoverURL"\s*:\s*"([^"]+)"',
                r'"URL"\s*:\s*"([^"]+)"',
                r'data-imgurl="([^"]+)"',
                r'data-original="([^"]+)"',
                r'data-src="([^"]+)"',
                r'data-url="([^"]+)"'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, html_content)
                for url in matches:
                    if self.is_valid_image_url(url):
                        full_url = self.normalize_url(url)
                        if full_url:
                            image_urls.append(full_url)
            
            # æ–¹æ³•3: æŸ¥æ‰¾èƒŒæ™¯å›¾ç‰‡
            style_patterns = [
                r'background-image\s*:\s*url\(["\']?([^"\'\)]+)["\']?\)',
                r'background\s*:\s*url\(["\']?([^"\'\)]+)["\']?\)'
            ]
            
            for pattern in style_patterns:
                matches = re.findall(pattern, html_content)
                for url in matches:
                    if self.is_valid_image_url(url):
                        full_url = self.normalize_url(url)
                        if full_url:
                            image_urls.append(full_url)
            
            return list(set(image_urls))  # å»é‡
            
        except Exception as e:
            print(f"âŒ HTMLè§£æå¤±è´¥: {e}")
            return []
    
    def extract_image_urls_backup(self, html_content):
        """å¤‡ç”¨å›¾ç‰‡URLæå–æ–¹æ³•"""
        try:
            import re
            image_urls = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡URLæ¨¡å¼
            url_patterns = [
                r'https?:[^"\'\s<>]+\.(?:jpg|jpeg|png|webp|gif|bmp)',
                r'//[^"\'\s<>]+\.(?:jpg|jpeg|png|webp|gif|bmp)',
                r'/[^"\'\s<>]+\.(?:jpg|jpeg|png|webp|gif|bmp)',
            ]
            
            for pattern in url_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for url in matches:
                    if self.is_valid_image_url(url):
                        full_url = self.normalize_url(url)
                        if full_url:
                            image_urls.append(full_url)
            
            return list(set(image_urls))
            
        except Exception as e:
            print(f"âŒ å¤‡ç”¨è§£æå¤±è´¥: {e}")
            return []
    
    def is_valid_image_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡URL"""
        if not url or len(url) < 10:
            return False
        
        # æ£€æŸ¥å›¾ç‰‡æ‰©å±•å
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp']
        url_lower = url.lower()
        
        # æ£€æŸ¥æ‰©å±•å
        if any(ext in url_lower for ext in valid_extensions):
            return True
        
        # æ£€æŸ¥å¸¸è§çš„å›¾ç‰‡URLæ¨¡å¼
        if any(pattern in url_lower for pattern in ['image', 'img', 'pic', 'photo']):
            return True
        
        return False
    
    def normalize_url(self, url):
        """æ ‡å‡†åŒ–URLæ ¼å¼"""
        if not url:
            return None
        
        # å¤„ç†ç›¸å¯¹URL
        if url.startswith('//'):
            return 'https:' + url
        elif url.startswith('/'):
            return 'https://image.baidu.com' + url
        elif url.startswith('http'):
            return url
        
        return None
    
    def download_image(self, image_url, filename):
        """ä¸‹è½½å•å¼ å›¾ç‰‡ - æ”¹è¿›ç‰ˆæœ¬"""
        max_retries = 2
        
        # é¦–å…ˆéªŒè¯URL
        if not self.validate_image_url(image_url):
            print(f"âš ï¸ æ— æ•ˆURLï¼Œè·³è¿‡: {filename}")
            return False
        
        for attempt in range(max_retries):
            try:
                headers = {
                    'User-Agent': self.get_random_user_agent(),
                    'Referer': 'https://www.bing.com/',
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9'
                }
                
                # å¤„ç†URLæ ¼å¼
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif not image_url.startswith('http'):
                    image_url = 'https://' + image_url
                
                # ä¿®å¤URLæ ¼å¼é—®é¢˜
                if ':/' in image_url and '://' not in image_url:
                    image_url = image_url.replace(':/', '://', 1)
                
                print(f"  ğŸ“¥ å°è¯•ä¸‹è½½: {filename}")
                print(f"     URL: {image_url[:100]}...")
                
                response = requests.get(image_url, headers=headers, timeout=15)
                response.raise_for_status()
                
                # æ£€æŸ¥å›¾ç‰‡æ ¼å¼å’Œå¤§å°
                if len(response.content) < 10240:  # 10KBä»¥ä¸‹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆå›¾ç‰‡
                    print(f"âš ï¸ å›¾ç‰‡å¤ªå°({len(response.content)}å­—èŠ‚)ï¼Œè·³è¿‡: {filename}")
                    return False
                
                # éªŒè¯å›¾ç‰‡æ ¼å¼
                try:
                    image = Image.open(io.BytesIO(response.content))
                    
                    # æ£€æŸ¥å›¾ç‰‡æ ¼å¼
                    if image.format not in ['JPEG', 'PNG', 'WEBP']:
                        print(f"âš ï¸ ä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼({image.format}): {filename}")
                        return False
                    
                    # è¿‡æ»¤æ‰å¤ªå°çš„å›¾ç‰‡
                    if image.size[0] < 200 or image.size[1] < 200:
                        print(f"âš ï¸ å›¾ç‰‡å°ºå¯¸å¤ªå°({image.size[0]}x{image.size[1]}): {filename}")
                        return False
                    
                    # æ£€æŸ¥å›¾ç‰‡è´¨é‡ï¼ˆé¿å…ä¸‹è½½æŸåçš„å›¾ç‰‡ï¼‰
                    if image.mode == 'P':  # è°ƒè‰²æ¿æ¨¡å¼ï¼Œå¯èƒ½æœ‰é—®é¢˜
                        image = image.convert('RGB')
                    
                    # ä¿å­˜å›¾ç‰‡
                    filepath = os.path.join(self.download_folder, f"{filename}.jpg")
                    image.save(filepath, "JPEG", quality=90, optimize=True)
                    
                    print(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} ({image.size[0]}x{image.size[1]})")
                    self.learning_images.append(filepath)
                    return True
                    
                except Exception as img_error:
                    print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥ {filename}: {img_error}")
                    if attempt == max_retries - 1:
                        return False
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥ {filename} (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return False
                time.sleep(2)  # é‡è¯•å‰ç­‰å¾…æ›´é•¿æ—¶é—´
            
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥ {filename}: {e}")
                return False
        
        return False
    
    def validate_image_url(self, url):
        """éªŒè¯å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or len(url) < 10:
            return False
        
        # æ£€æŸ¥URLæ ¼å¼
        if not url.startswith('http'):
            return False
        
        # æ£€æŸ¥å›¾ç‰‡æ‰©å±•åæˆ–å›¾ç‰‡æœåŠ¡åŸŸå
        if (not any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']) and
            not any(service in url.lower() for service in ['picsum.photos', 'unsplash.com', 'placeholder.com'])):
            return False
        
        # è¿‡æ»¤æ‰æ˜æ˜¾æ— æ•ˆçš„URL
        if any(bad in url.lower() for bad in ['logo', 'icon', 'avatar', 'thumb']):
            return False
        
        # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦
        if ' ' in url or '\n' in url or '\t' in url:
            return False
        
        return True
    
    def preprocess_learning_images(self):
        """é¢„å¤„ç†å­¦ä¹ å›¾ç‰‡"""
        print("ğŸ”„ é¢„å¤„ç†å­¦ä¹ å›¾ç‰‡...")
        
        processed_images = []
        for img_path in self.learning_images:
            try:
                # è¯»å–å›¾ç‰‡
                image = cv2.imread(img_path)
                if image is None:
                    continue
                
                # è°ƒæ•´å¤§å°ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
                h, w = image.shape[:2]
                max_size = 800
                if max(h, w) > max_size:
                    scale = max_size / max(h, w)
                    new_w, new_h = int(w * scale), int(h * scale)
                    image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
                
                # å¢å¼ºå›¾ç‰‡è´¨é‡
                image = self.enhance_image_quality(image)
                
                # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
                cv2.imwrite(img_path, image)
                processed_images.append(img_path)
                
            except Exception as e:
                print(f"âŒ é¢„å¤„ç†å¤±è´¥ {img_path}: {e}")
        
        self.learning_images = processed_images
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆå›¾ç‰‡: {len(processed_images)} å¼ ")
    
    def enhance_image_quality(self, image):
        """å¢å¼ºå›¾ç‰‡è´¨é‡"""
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºäº®åº¦å’Œå¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        # åˆå¹¶é€šé“
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # è½»å¾®é™å™ª
        enhanced = cv2.medianBlur(enhanced, 3)
        
        return enhanced
    
    def start_auto_learning(self, max_total_images=100):
        """å¼€å§‹è‡ªä¸»å­¦ä¹  - æ”¹è¿›ç‰ˆæœ¬ï¼Œå¢åŠ æ ·æœ¬æ•°é‡"""
        print("ğŸ¯ å¼€å§‹å®«å´éªé£æ ¼è‡ªä¸»å­¦ä¹ ...")
        
        total_downloaded = 0
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å›¾ç‰‡ï¼Œç¡®ä¿è´¨é‡
        local_images = self._get_local_ghibli_images("all")
        
        if local_images:
            print(f"ğŸ“ å‘ç° {len(local_images)} å¼ æœ¬åœ°å®«å´éªé£æ ¼å›¾ç‰‡")
            for img_path in local_images[:min(30, len(local_images))]:
                if total_downloaded >= max_total_images:
                    break
                if self._copy_local_image(img_path, f"local_{total_downloaded}"):
                    total_downloaded += 1
        
        # å¦‚æœæœ¬åœ°å›¾ç‰‡ä¸è¶³ï¼Œå†ä½¿ç”¨ç½‘ç»œæœç´¢
        if total_downloaded < max_total_images:
            for keyword in self.search_keywords:
                if total_downloaded >= max_total_images:
                    break
                
                downloaded = self.search_ghibli_images(keyword, max_images=8)
                total_downloaded += downloaded
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        if total_downloaded > 0:
            self.preprocess_learning_images()
            print(f"ğŸ‰ è‡ªä¸»å­¦ä¹ å®Œæˆï¼å…±æ”¶é›† {len(self.learning_images)} å¼ å®«å´éªé£æ ¼å›¾ç‰‡")
            
            # å¢å¼ºå­¦ä¹ æ•ˆæœ
            self.enhance_learning_quality()
        else:
            print("âš ï¸ æœªä¸‹è½½åˆ°ä»»ä½•å›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤é£æ ¼")
        
        return self.learning_images
    
    def enhance_learning_quality(self):
        """å¢å¼ºå­¦ä¹ è´¨é‡ - ä½¿ç”¨æ›´é«˜çº§çš„å›¾åƒå¤„ç†æŠ€æœ¯"""
        print("ğŸ”§ å¢å¼ºå­¦ä¹ å›¾ç‰‡è´¨é‡...")
        
        enhanced_images = []
        for img_path in self.learning_images:
            try:
                # è¯»å–å›¾ç‰‡
                image = cv2.imread(img_path)
                if image is None:
                    continue
                
                # 1. é«˜è´¨é‡ç¼©æ”¾
                h, w = image.shape[:2]
                max_size = 1024  # æé«˜åˆ†è¾¨ç‡
                if max(h, w) > max_size:
                    scale = max_size / max(h, w)
                    new_w, new_h = int(w * scale), int(h * scale)
                    image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
                
                # 2. é«˜çº§è‰²å½©å¢å¼º
                image = self.advanced_color_enhancement(image)
                
                # 3. ç»†èŠ‚å¢å¼º
                image = self.enhance_details(image)
                
                # 4. é™å™ªå¤„ç†
                image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)
                
                # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
                cv2.imwrite(img_path, image, [cv2.IMWRITE_JPEG_QUALITY, 95])
                enhanced_images.append(img_path)
                
            except Exception as e:
                print(f"âŒ å›¾ç‰‡å¢å¼ºå¤±è´¥ {img_path}: {e}")
        
        self.learning_images = enhanced_images
        print(f"âœ… å›¾ç‰‡è´¨é‡å¢å¼ºå®Œæˆï¼Œæœ‰æ•ˆå›¾ç‰‡: {len(enhanced_images)} å¼ ")
    
    def advanced_color_enhancement(self, image):
        """é«˜çº§è‰²å½©å¢å¼º - æ¨¡æ‹Ÿå®«å´éªé£æ ¼è‰²å½©"""
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºäº®åº¦å’Œå¯¹æ¯”åº¦ï¼ˆå®«å´éªé£æ ¼ç‰¹ç‚¹ï¼šæ˜äº®ã€é«˜å¯¹æ¯”åº¦ï¼‰
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        # å¢å¼ºè‰²å½©é¥±å’Œåº¦ï¼ˆå®«å´éªé£æ ¼è‰²å½©é²œè‰³ï¼‰
        a = cv2.addWeighted(a, 1.3, a, 0, 0)
        b = cv2.addWeighted(b, 1.3, b, 0, 0)
        
        # åˆå¹¶é€šé“
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # åº”ç”¨æŸ”å’Œæ»¤é•œï¼ˆå®«å´éªé£æ ¼æŸ”å’Œï¼‰
        soft = cv2.GaussianBlur(enhanced, (3, 3), 0)
        result = cv2.addWeighted(enhanced, 0.8, soft, 0.2, 0)
        
        return result
    
    def enhance_details(self, image):
        """å¢å¼ºå›¾ç‰‡ç»†èŠ‚"""
        # ä½¿ç”¨éé”åŒ–æ©è”½å¢å¼ºç»†èŠ‚
        gaussian = cv2.GaussianBlur(image, (0, 0), 3.0)
        unsharp_mask = cv2.addWeighted(image, 1.5, gaussian, -0.5, 0)
        
        return unsharp_mask
    
    def cleanup_learning_files(self):
        """æ¸…ç†å­¦ä¹ æ–‡ä»¶"""
        print("ğŸ§¹ æ¸…ç†å­¦ä¹ æ–‡ä»¶...")
        
        if os.path.exists(self.download_folder):
            for file in os.listdir(self.download_folder):
                file_path = os.path.join(self.download_folder, file)
                try:
                    os.remove(file_path)
                except Exception as e:
                    print(f"âŒ åˆ é™¤å¤±è´¥ {file_path}: {e}")
            
            try:
                os.rmdir(self.download_folder)
                print("âœ… å­¦ä¹ æ–‡ä»¶æ¸…ç†å®Œæˆ")
            except:
                print("âš ï¸ æ–‡ä»¶å¤¹åˆ é™¤å¤±è´¥ï¼Œå¯èƒ½ä»æœ‰æ–‡ä»¶")


def test_auto_learning():
    """æµ‹è¯•è‡ªä¸»å­¦ä¹ åŠŸèƒ½ - æ”¹è¿›ç‰ˆæœ¬"""
    learner = GhibliAutoLearner()
    
    try:
        # åªæµ‹è¯•å‰3ä¸ªå…³é”®è¯ï¼Œé¿å…è¿‡å¤šå¤±è´¥
        test_keywords = learner.search_keywords[:3]
        print(f"ğŸ§ª æµ‹è¯•å…³é”®è¯: {test_keywords}")
        
        total_downloaded = 0
        for keyword in test_keywords:
            if total_downloaded >= 5:  # æœ€å¤šä¸‹è½½5å¼ 
                break
            
            downloaded = learner.search_ghibli_images(keyword, max_images=2)
            total_downloaded += downloaded
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        if total_downloaded > 0:
            learner.preprocess_learning_images()
            print(f"âœ… è‡ªä¸»å­¦ä¹ æµ‹è¯•æˆåŠŸï¼Œè·å¾— {len(learner.learning_images)} å¼ å›¾ç‰‡")
        else:
            print("âš ï¸ è‡ªä¸»å­¦ä¹ æµ‹è¯•å®Œæˆï¼Œä½†æœªè·å¾—å›¾ç‰‡")
        
        # æ¸…ç†æ–‡ä»¶
        learner.cleanup_learning_files()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        learner.cleanup_learning_files()
    except Exception as e:
        print(f"âŒ è‡ªä¸»å­¦ä¹ æµ‹è¯•å¤±è´¥: {e}")
        learner.cleanup_learning_files()


if __name__ == "__main__":
    test_auto_learning()