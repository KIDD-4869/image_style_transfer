#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å®«å´éªé£æ ¼è‡ªåŠ¨å­¦ä¹ è®­ç»ƒè„šæœ¬
ä½¿ç”¨çœŸå®çš„ç…§ç‰‡å’Œå®«å´éªåŠ¨ç”»å¸§è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒ
"""

import os
from typing import Tuple, Optional, List
import glob
import shutil
import argparse

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
try:
    # Pillow 9.0.0+
    from PIL.Image import Resampling
    PIL_LANCZOS = Resampling.LANCZOS
except ImportError:
    # Older Pillow versions
    PIL_LANCZOS = Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS  # type: ignore
import numpy as np
import json
import time
from tqdm import tqdm
import requests
import random

from core.ghibli_gan import GhibliGAN, create_sample_training_data
from core.ghibli_data_pipeline import GhibliDataPipeline

def download_sample_photos(photo_dir: str, target_count: int = 100) -> int:
    """
    ä¸‹è½½ç¤ºä¾‹ç…§ç‰‡ç”¨äºè®­ç»ƒ
    
    Args:
        photo_dir: ç…§ç‰‡ä¿å­˜ç›®å½•
        target_count: ç›®æ ‡ç…§ç‰‡æ•°é‡
        
    Returns:
        å®é™…ä¸‹è½½çš„ç…§ç‰‡æ•°é‡
    """
    print(f"ğŸŒ æ­£åœ¨ä¸‹è½½ç¤ºä¾‹ç…§ç‰‡ï¼Œç›®æ ‡æ•°é‡: {target_count}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(photo_dir, exist_ok=True)
    
    # æ£€æŸ¥ç°æœ‰ç…§ç‰‡æ•°é‡
    existing_photos = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        existing_photos.extend(glob.glob(os.path.join(photo_dir, f"*{ext}")))
    
    current_count = len(existing_photos)
    needed_count = max(0, target_count - current_count)
    
    if needed_count == 0:
        print(f"âœ… ç…§ç‰‡æ•°é‡å·²æ»¡è¶³éœ€æ±‚: {current_count} å¼ ")
        return current_count
    
    print(f"ğŸ“¸ å½“å‰ç…§ç‰‡: {current_count} å¼ ï¼Œéœ€è¦ä¸‹è½½: {needed_count} å¼ ")
    
    # ä½¿ç”¨å…è´¹çš„ç¤ºä¾‹å›¾ç‰‡API
    # è¿™é‡Œä½¿ç”¨å¤šä¸ªå…è´¹çš„å›¾ç‰‡æº
    downloaded_count = 0
    
    # ç¤ºä¾‹å›¾ç‰‡URLåˆ—è¡¨ï¼ˆä½¿ç”¨å…è´¹çš„å›¾ç‰‡æœåŠ¡ï¼‰
    sample_urls = [
        # ä½¿ç”¨Lorem Picsumï¼ˆå…è´¹éšæœºå›¾ç‰‡æœåŠ¡ï¼‰
        f"https://picsum.photos/512/512?random={i}" for i in range(needed_count)
    ]
    
    # æ·»åŠ ä¸€äº›é£æ™¯ç±»åˆ«çš„URL
    categories = ["nature", "landscape", "city", "people", "animals"]
    for i in range(min(needed_count // 2, 20)):
        category = random.choice(categories)
        sample_urls.append(f"https://picsum.photos/512/512?category={category}&random={1000+i}")
    
    # ä¸‹è½½å›¾ç‰‡
    for i, url in enumerate(sample_urls[:needed_count]):
        try:
            print(f"â¬‡ï¸ ä¸‹è½½å›¾ç‰‡ {i+1}/{needed_count}: {url}")
            
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            # ä¿å­˜å›¾ç‰‡
            filename = f"downloaded_photo_{current_count + downloaded_count:04d}.jpg"
            filepath = os.path.join(photo_dir, filename)
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            downloaded_count += 1
            print(f"âœ… ä¿å­˜æˆåŠŸ: {filename}")
            
            # æ·»åŠ å°å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            time.sleep(0.5)
            
        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½å¤±è´¥ {url}: {e}")
            continue
    
    print(f"ğŸ‰ ç…§ç‰‡ä¸‹è½½å®Œæˆ! æ–°å¢: {downloaded_count} å¼ ")
    return current_count + downloaded_count

def augment_photo_data(photo_dir: str, target_count: int) -> None:
    """
    é€šè¿‡æ•°æ®å¢å¼ºæ‰©å……ç…§ç‰‡æ•°æ®
    
    Args:
        photo_dir: ç…§ç‰‡ç›®å½•
        target_count: ç›®æ ‡ç…§ç‰‡æ•°é‡
    """
    print(f"ğŸ”„ æ­£åœ¨é€šè¿‡æ•°æ®å¢å¼ºæ‰©å……ç…§ç‰‡æ•°æ®...")
    
    # è·å–ç°æœ‰ç…§ç‰‡
    existing_photos = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        existing_photos.extend(glob.glob(os.path.join(photo_dir, f"*{ext}")))
    
    current_count = len(existing_photos)
    needed_count = max(0, target_count - current_count)
    
    if needed_count <= 0:
        return
    
    print(f"ğŸ“Š å½“å‰: {current_count} å¼ ï¼Œç›®æ ‡: {target_count} å¼ ï¼Œéœ€è¦å¢å¼º: {needed_count} å¼ ")
    
    # æ•°æ®å¢å¼ºå˜æ¢
    augment_transforms = [
        transforms.RandomHorizontalFlip(p=1.0),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
    ]
    
    augmented_count = 0
    transform_to_tensor = transforms.ToTensor()
    transform_to_pil = transforms.ToPILImage()
    
    while augmented_count < needed_count and existing_photos:
        # éšæœºé€‰æ‹©ä¸€å¼ åŸå§‹å›¾ç‰‡
        source_photo = random.choice(existing_photos)
        
        try:
            # åŠ è½½å›¾ç‰‡
            img = Image.open(source_photo).convert('RGB')
            
            # éšæœºé€‰æ‹©å¢å¼ºæ–¹æ³•
            augment = random.choice(augment_transforms)
            
            # åº”ç”¨å¢å¼º
            img_tensor = transform_to_tensor(img)
            img_tensor = augment(img_tensor.unsqueeze(0)).squeeze(0)
            augmented_img = transform_to_pil(img_tensor)
            
            # ä¿å­˜å¢å¼ºåçš„å›¾ç‰‡
            filename = f"augmented_photo_{current_count + augmented_count:04d}.jpg"
            filepath = os.path.join(photo_dir, filename)
            augmented_img.save(filepath, 'JPEG', quality=85)
            
            augmented_count += 1
            print(f"âœ¨ å¢å¼ºç”Ÿæˆ: {filename}")
            
        except Exception as e:
            print(f"âš ï¸ æ•°æ®å¢å¼ºå¤±è´¥: {e}")
            continue
    
    print(f"ğŸ‰ æ•°æ®å¢å¼ºå®Œæˆ! æ–°å¢: {augmented_count} å¼ ç…§ç‰‡")

def prepare_photo_data(photo_dir: str, target_count: int = 100, enable_download: bool = True) -> int:
    """
    å‡†å¤‡ç…§ç‰‡è®­ç»ƒæ•°æ®
    
    Args:
        photo_dir: ç…§ç‰‡ç›®å½•
        target_count: ç›®æ ‡ç…§ç‰‡æ•°é‡
        enable_download: æ˜¯å¦å¯ç”¨ä¸‹è½½åŠŸèƒ½
        
    Returns:
        å®é™…å‡†å¤‡çš„ç…§ç‰‡æ•°é‡
    """
    print(f"ğŸ“¸ å‡†å¤‡ç…§ç‰‡è®­ç»ƒæ•°æ®ï¼Œç›®æ ‡æ•°é‡: {target_count}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(photo_dir, exist_ok=True)
    
    # æ£€æŸ¥ç°æœ‰ç…§ç‰‡
    existing_photos = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        existing_photos.extend(glob.glob(os.path.join(photo_dir, f"*{ext}")))
    
    current_count = len(existing_photos)
    print(f"ğŸ“Š å½“å‰ç…§ç‰‡æ•°é‡: {current_count}")
    
    # å¦‚æœæ•°é‡ä¸å¤Ÿï¼Œå°è¯•ä¸‹è½½
    if current_count < target_count and enable_download:
        download_count = download_sample_photos(photo_dir, target_count)
        current_count = download_count
    
    # å¦‚æœä»ç„¶ä¸å¤Ÿï¼Œä½¿ç”¨æ•°æ®å¢å¼º
    if current_count < target_count:
        augment_photo_data(photo_dir, target_count)
    
    # æœ€ç»ˆç»Ÿè®¡
    final_photos = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        final_photos.extend(glob.glob(os.path.join(photo_dir, f"*{ext}")))
    
    print(f"âœ… ç…§ç‰‡æ•°æ®å‡†å¤‡å®Œæˆï¼Œæ€»è®¡: {len(final_photos)} å¼ ")
    return len(final_photos)

def prepare_style_data(style_dir: str) -> None:
    """
    è‡ªåŠ¨å‡†å¤‡å®«å´éªé£æ ¼æ•°æ®
    
    Args:
        style_dir: é£æ ¼æ•°æ®ç›®å½•
    """
    print("ğŸ”„ æ­£åœ¨å‡†å¤‡å®«å´éªé£æ ¼æ•°æ®...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(style_dir, exist_ok=True)
    
    # æ£€æŸ¥å½“å‰å·²æœ‰å¤šå°‘é£æ ¼å›¾ç‰‡
    existing_files = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        existing_files.extend(glob.glob(os.path.join(style_dir, f"*{ext}")))
    
    if len(existing_files) >= 20:
        print(f"âœ… é£æ ¼æ•°æ®å·²è¶³å¤Ÿ: {len(existing_files)} å¼ å›¾ç‰‡")
        return
    
    # 1. é¦–å…ˆå°è¯•ä»å·²æœ‰çš„ghibli_imagesç›®å½•å¤åˆ¶
    ghibli_source_dir = "ghibli_images"
    if os.path.exists(ghibli_source_dir):
        print(f"ğŸ“ å‘ç°å·²æœ‰çš„å®«å´éªå›¾åƒç›®å½•: {ghibli_source_dir}")
        
        # å¤åˆ¶å›¾åƒæ–‡ä»¶
        copied_count = 0
        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            source_files = glob.glob(os.path.join(ghibli_source_dir, f"*{ext}"))
            for source_file in source_files[:50]:  # æœ€å¤šå¤åˆ¶50å¼ å›¾ç‰‡
                filename = os.path.basename(source_file)
                dest_file = os.path.join(style_dir, f"ghibli_{copied_count:04d}{os.path.splitext(filename)[1]}")
                try:
                    shutil.copy2(source_file, dest_file)
                    copied_count += 1
                except Exception as e:
                    print(f"âš ï¸ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {source_file}: {e}")
        
        if copied_count > 0:
            print(f"âœ… ä»å·²æœ‰ç›®å½•å¤åˆ¶äº† {copied_count} å¼ å®«å´éªé£æ ¼å›¾ç‰‡")
    
    # 2. å¦‚æœæ•°æ®è¿˜ä¸å¤Ÿï¼Œå°è¯•ä»ç½‘ä¸Šä¸‹è½½å®«å´éªé£æ ¼å›¾ç‰‡
    current_files = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        current_files.extend(glob.glob(os.path.join(style_dir, f"*{ext}")))
    
    if len(current_files) < 20:
        print("ğŸŒ æ­£åœ¨ä¸‹è½½å®«å´éªé£æ ¼å›¾ç‰‡...")
        try:
            # ä¸‹è½½å®«å´éªé£æ ¼å›¾ç‰‡
            download_ghibli_images(style_dir, 20 - len(current_files))
        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½å®«å´éªå›¾ç‰‡å¤±è´¥: {e}")
    
    # 3. å¦‚æœä»ç„¶ä¸å¤Ÿï¼Œä½¿ç”¨GhibliGANç”Ÿæˆæ›´å¤šæ ·æœ¬æ•°æ®
    current_files = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        current_files.extend(glob.glob(os.path.join(style_dir, f"*{ext}")))
    
    if len(current_files) < 20:
        print("ğŸ¨ æ­£åœ¨ç”Ÿæˆé¢å¤–çš„å®«å´éªé£æ ¼æ ·æœ¬æ•°æ®...")
        try:
            # ä½¿ç”¨GhibliGANç”Ÿæˆæ ·æœ¬
            needed_samples = 20 - len(current_files)
            # ä¿®å¤ï¼šcreate_sample_training_data è¿”å›çš„æ˜¯é…ç½®ï¼Œä¸æ˜¯å›¾åƒæ•°ç»„
            # ç”Ÿæˆä¸€äº›ç®€å•çš„å®«å´éªé£æ ¼å›¾åƒ
            create_ghibli_style_samples(style_dir, needed_samples)
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆæ ·æœ¬æ•°æ®å¤±è´¥: {e}")
    
    # 4. å¦‚æœä»ç„¶ä¸å¤Ÿï¼Œåˆ›å»ºåŸºç¡€é£æ ¼æ¨¡æ¿
    final_files = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        final_files.extend(glob.glob(os.path.join(style_dir, f"*{ext}")))
    
    if len(final_files) < 10:
        print("ğŸ¨ åˆ›å»ºåŸºç¡€å®«å´éªé£æ ¼æ¨¡æ¿ä½œä¸ºè¡¥å……...")
        create_basic_ghibli_templates(style_dir)
    
    # æœ€ç»ˆç»Ÿè®¡
    total_files = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        total_files.extend(glob.glob(os.path.join(style_dir, f"*{ext}")))
    
    print(f"âœ… é£æ ¼æ•°æ®å‡†å¤‡å®Œæˆï¼Œæ€»å…± {len(total_files)} å¼ å›¾ç‰‡")

def create_ghibli_style_samples(style_dir: str, count: int) -> None:
    """
    åˆ›å»ºå®«å´éªé£æ ¼æ ·æœ¬å›¾åƒ
    
    Args:
        style_dir: é£æ ¼å›¾ç‰‡ä¿å­˜ç›®å½•
        count: éœ€è¦åˆ›å»ºçš„æ ·æœ¬æ•°é‡
    """
    print(f"ğŸ¨ æ­£åœ¨åˆ›å»º {count} å¼ å®«å´éªé£æ ¼æ ·æœ¬å›¾åƒ...")
    
    # å®«å´éªé£æ ¼çš„å…¸å‹é¢œè‰²
    ghibli_colors = [
        (255, 204, 102),  # æš–é»„è‰²
        (102, 153, 204),  # å¤©ç©ºè“
        (255, 153, 102),  # æ©™è‰²
        (153, 204, 102),  # è‰ç»¿è‰²
        (255, 102, 153),  # ç²‰è‰²
        (102, 204, 153),  # é’ç»¿è‰²
    ]
    
    created_count = 0
    size = (512, 512)
    
    for i in range(count):
        try:
            # åˆ›å»ºåŸºç¡€å›¾åƒ
            img_array = np.zeros((size[0], size[1], 3), dtype=np.uint8)
            
            # å¡«å……èƒŒæ™¯è‰²
            bg_color = random.choice(ghibli_colors)
            img_array[:, :] = bg_color
            
            # æ·»åŠ ä¸€äº›éšæœºå½¢çŠ¶å’Œé¢œè‰²å—æ¥æ¨¡ä»¿å®«å´éªé£æ ¼
            for _ in range(random.randint(3, 8)):
                # éšæœºå½¢çŠ¶é¢œè‰²
                shape_color = random.choice(ghibli_colors)
                
                # éšæœºä½ç½®å’Œå¤§å°
                x = random.randint(0, size[0] - 50)
                y = random.randint(0, size[1] - 50)
                w = random.randint(30, 150)
                h = random.randint(30, 150)
                
                # éšæœºå½¢çŠ¶ï¼ˆçŸ©å½¢æˆ–åœ†å½¢ï¼‰
                if random.random() > 0.5:
                    # çŸ©å½¢
                    cv2.rectangle(img_array, (x, y), (x + w, y + h), shape_color, -1)
                else:
                    # åœ†å½¢
                    center = (x + w // 2, y + h // 2)
                    radius = min(w, h) // 2
                    cv2.circle(img_array, center, radius, shape_color, -1)
            
            # æ·»åŠ ä¸€äº›çº¿æ¡æ¥æ¨¡ä»¿åŠ¨ç”»é£æ ¼
            for _ in range(random.randint(5, 15)):
                pt1 = (random.randint(0, size[0]), random.randint(0, size[1]))
                pt2 = (random.randint(0, size[0]), random.randint(0, size[1]))
                line_color = random.choice(ghibli_colors)
                thickness = random.randint(1, 3)
                cv2.line(img_array, pt1, pt2, line_color, thickness)
            
            # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä¿å­˜
            img_pil = Image.fromarray(img_array)
            filename = os.path.join(style_dir, f"generated_ghibli_{created_count:04d}.png")
            img_pil.save(filename, 'PNG')
            created_count += 1
            
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºæ ·æœ¬å¤±è´¥ {i}: {e}")
    
    print(f"âœ… åˆ›å»ºäº† {created_count} å¼ å®«å´éªé£æ ¼æ ·æœ¬å›¾åƒ")

def download_ghibli_images(style_dir: str, target_count: int) -> int:
    """
    ä¸‹è½½å®«å´éªé£æ ¼å›¾ç‰‡
    
    Args:
        style_dir: é£æ ¼å›¾ç‰‡ä¿å­˜ç›®å½•
        target_count: ç›®æ ‡ä¸‹è½½æ•°é‡
        
    Returns:
        int: å®é™…ä¸‹è½½çš„å›¾ç‰‡æ•°é‡
    """
    print(f"ğŸŒ æ­£åœ¨ä¸‹è½½å®«å´éªé£æ ¼å›¾ç‰‡ï¼Œç›®æ ‡æ•°é‡: {target_count}")
    
    # å®«å´éªé£æ ¼å›¾ç‰‡æœç´¢å…³é”®è¯
    keywords = [
        "hayao+miyazaki", "studio+ghibli", "spirited+away", 
        "my+neighbor+totoro", "princess+mononoke", "howl+s+moving+castle"
    ]
    
    downloaded_count = 0
    
    # ä½¿ç”¨å…è´¹çš„Lorem PicsumæœåŠ¡å’ŒUnsplashæœåŠ¡æœç´¢å’Œä¸‹è½½å›¾ç‰‡
    try:
        for i in range(target_count):
            try:
                # éšæœºé€‰æ‹©æœåŠ¡
                if random.random() > 0.5:
                    # ä½¿ç”¨Lorem Picsum
                    url = f"https://picsum.photos/512/512?random={int(time.time() * 1000 + i)}"
                else:
                    # ä½¿ç”¨å…³é”®è¯æœç´¢Unsplashé£æ ¼å›¾ç‰‡
                    keyword = random.choice(keywords)
                    url = f"https://source.unsplash.com/512x512/?{keyword},anime,art,japan"
                
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    # ä¿å­˜å›¾ç‰‡
                    filename = os.path.join(style_dir, f"downloaded_ghibli_{downloaded_count:04d}.jpg")
                    with open(filename, 'wb') as f:
                        f.write(response.content)
                    downloaded_count += 1
                    print(f"âœ… ä¸‹è½½å®«å´éªé£æ ¼å›¾ç‰‡: {filename}")
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            except Exception as e:
                print(f"âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
                continue
                
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å®«å´éªå›¾ç‰‡æ—¶å‡ºé”™: {e}")
    
    print(f"ğŸ‰ å®«å´éªé£æ ¼å›¾ç‰‡ä¸‹è½½å®Œæˆï¼Œå…±ä¸‹è½½ {downloaded_count} å¼ ")
    return downloaded_count

def create_basic_ghibli_templates(style_dir: str) -> None:
    """
    åˆ›å»ºåŸºç¡€çš„å®«å´éªé£æ ¼æ¨¡æ¿å›¾åƒ
    
    Args:
        style_dir: é£æ ¼æ•°æ®ç›®å½•
    """
    templates = [
        ("sky_blue", (135, 206, 235)),      # å¤©ç©ºè“
        ("forest_green", (34, 139, 34)),      # æ£®æ—ç»¿
        ("sunset_orange", (255, 140, 90)),   # å¤•é˜³æ©™
        ("field_yellow", (255, 223, 0)),      # ç”°é‡é»„
        ("ocean_blue", (70, 130, 180)),      # æµ·æ´‹è“
    ]
    
    created_count = 0
    size = (256, 256)
    
    for name, base_color in templates:
        try:
            # åˆ›å»ºåŸºç¡€è‰²å½©å›¾åƒ
            img_array = np.full((size[0], size[1], 3), base_color, dtype=np.uint8)
            
            # æ·»åŠ ä¸€äº›çº¹ç†å’Œå˜åŒ–
            noise = np.random.randint(-20, 20, (size[0], size[1], 3), dtype=np.int16)
            img_array = np.clip(img_array.astype(np.int16) + noise, 0, 255).astype(np.uint8)
            
            # åº”ç”¨è½»å¾®çš„é«˜æ–¯æ¨¡ç³Šä½¿é¢œè‰²æ›´æŸ”å’Œ
            img_pil = Image.fromarray(img_array)
            
            # ä¿å­˜
            filename = os.path.join(style_dir, f"template_{name}_{created_count:02d}.png")
            img_pil.save(filename, 'PNG')
            created_count += 1
            
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºæ¨¡æ¿å¤±è´¥ {name}: {e}")
    
    print(f"âœ… åˆ›å»ºäº† {created_count} å¼ åŸºç¡€é£æ ¼æ¨¡æ¿")

class GhibliStyleEncoder(nn.Module):
    """å®«å´éªé£æ ¼ç¼–ç å™¨"""
    
    def __init__(self):
        super(GhibliStyleEncoder, self).__init__()
        
        # ç¼–ç å™¨ç»“æ„
        self.encoder = nn.Sequential(
            # ç¬¬ä¸€å±‚
            nn.Conv2d(3, 64, 3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ç¬¬äºŒå±‚
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ç¬¬ä¸‰å±‚
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ç¬¬å››å±‚
            nn.Conv2d(256, 512, 3, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
        )
        
        # é£æ ¼ç¼–ç å±‚
        self.style_pool = nn.AdaptiveAvgPool2d(1)
        self.style_fc = nn.Linear(512, 256)
        
    def forward(self, x):
        # ç¼–ç ç‰¹å¾
        features = self.encoder(x)
        
        # æå–é£æ ¼ç‰¹å¾
        style = self.style_pool(features)
        style = style.view(style.size(0), -1)
        style = self.style_fc(style)
        
        return features, style

class GhibliDataset(Dataset[Tuple[torch.Tensor, torch.Tensor]]):
    """å®«å´éªé£æ ¼æ•°æ®é›†"""
    
    def __init__(self, photo_dir: str, style_dir: str, transform: Optional[transforms.Compose] = None, image_size: int = 512):
        self.photo_dir = photo_dir
        self.style_dir = style_dir
        self.transform = transform
        self.image_size = image_size
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        self.photo_files = self._get_image_files(photo_dir)
        self.style_files = self._get_image_files(style_dir)
        
        # ä¸è¦æ±‚å¯¹é½ï¼Œå…è®¸ç…§ç‰‡å’Œé£æ ¼å›¾åƒæ•°é‡ä¸åŒ
        # è®­ç»ƒæ—¶ä¼šå¾ªç¯ä½¿ç”¨é£æ ¼å›¾åƒ
        print(f"ğŸ“Š æ•°æ®é›†åŠ è½½: {len(self.photo_files)} å¼ ç…§ç‰‡, {len(self.style_files)} å¼ é£æ ¼å›¾åƒ")
    
    def _get_image_files(self, directory: str) -> List[str]:
        """è·å–ç›®å½•ä¸­çš„å›¾åƒæ–‡ä»¶"""
        if not os.path.exists(directory):
            return []
        
        extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        files = []
        
        for ext in extensions:
            files.extend([f for f in os.listdir(directory) 
                         if f.lower().endswith(ext)])
        
        return sorted(files)
    
    def __len__(self):
        return len(self.photo_files)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # åŠ è½½ç…§ç‰‡
        photo_path = os.path.join(self.photo_dir, self.photo_files[idx])
        photo = Image.open(photo_path).convert('RGB')
        
        # åŠ è½½é£æ ¼å›¾åƒ - å¦‚æœé£æ ¼å›¾åƒè¾ƒå°‘ï¼Œå¾ªç¯ä½¿ç”¨
        style_idx = idx % len(self.style_files) if len(self.style_files) > 0 else 0
        style_path = os.path.join(self.style_dir, self.style_files[style_idx])
        style = Image.open(style_path).convert('RGB')
        
        # è°ƒæ•´å¤§å°
        photo = photo.resize((self.image_size, self.image_size), PIL_LANCZOS)
        style = style.resize((self.image_size, self.image_size), PIL_LANCZOS)
        
        # è½¬æ¢ä¸ºtensor
        if self.transform:
            photo = self.transform(photo)
            style = self.transform(style)
        else:
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])
            photo = transform(photo)
            style = transform(style)
        
        return photo, style

class GhibliTrainer:
    """å®«å´éªé£æ ¼è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: Optional[str] = None, photo_count: int = 100, enable_download: bool = True):
        self.device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.photo_count = photo_count
        self.enable_download = enable_download
        self.model_version = self._get_latest_model_version()
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ“¸ ç›®æ ‡ç…§ç‰‡æ•°é‡: {photo_count}")
        print(f"ğŸŒ ä¸‹è½½åŠŸèƒ½: {'å¯ç”¨' if enable_download else 'ç¦ç”¨'}")
        print(f"ğŸ”– æ¨¡å‹ç‰ˆæœ¬: {self.model_version}")
        
        # åŠ è½½é…ç½®
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                self.config = json.load(f)
        else:
            self.config = create_sample_training_data()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.ghibli_gan = GhibliGAN(str(self.device))
        
        # åˆå§‹åŒ–æ•°æ®ç®¡é“
        self.data_pipeline = GhibliDataPipeline()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs("models/ghibli_gan", exist_ok=True)
        os.makedirs("training_logs", exist_ok=True)
        os.makedirs("models/real_ghibli_learning", exist_ok=True)
        
        # åŠ è½½ç°æœ‰æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self._load_existing_model()
    
    def _get_latest_model_version(self) -> int:
        """è·å–æœ€æ–°çš„æ¨¡å‹ç‰ˆæœ¬å·"""
        version_files = glob.glob("models/ghibli_gan/ghibli_gan_v*.pth")
        if not version_files:
            return 1
        
        versions = []
        for file in version_files:
            # æå–ç‰ˆæœ¬å·
            basename = os.path.basename(file)
            if 'best' in basename:
                continue
            try:
                version = int(basename.split('_v')[1].split('.')[0])
                versions.append(version)
            except:
                continue
        
        return max(versions) + 1 if versions else 1
    
    def _load_existing_model(self) -> None:
        """åŠ è½½ç°æœ‰æœ€ä½³æ¨¡å‹ç”¨äºå¢é‡è®­ç»ƒ"""
        best_model_path = "models/ghibli_gan/ghibli_gan_best.pth"
        if os.path.exists(best_model_path):
            print("ğŸ”„ åŠ è½½ç°æœ‰æœ€ä½³æ¨¡å‹ç”¨äºå¢é‡è®­ç»ƒ...")
            try:
                self.ghibli_gan.load_model(best_model_path)
                print("âœ… ç°æœ‰æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ç°æœ‰æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        else:
            print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹")
    
    def prepare_data(self) -> DataLoader:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # ä½¿ç”¨æ•°æ®ç®¡é“æ”¶é›†æ•°æ®
        photo_dir = str(self.config["dataset_config"]["photo_dir"])
        style_dir = str(self.config["dataset_config"]["style_dir"])
        
        # å‡†å¤‡ç…§ç‰‡æ•°æ®ï¼ˆæ”¯æŒè‡ªå®šä¹‰æ•°é‡å’Œä¸‹è½½ï¼‰
        prepare_photo_data(photo_dir, self.photo_count, self.enable_download)
        
        # åˆ›å»ºæ•°æ®é›†
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        
        dataset = GhibliDataset(
            photo_dir=photo_dir,
            style_dir=style_dir,
            transform=transform,
            image_size=int(self.config["dataset_config"]["image_size"])
        )
        
        # è®¡ç®—åˆé€‚çš„æ‰¹æ¬¡å¤§å°
        dataset_size = len(dataset.photo_files) if hasattr(dataset, 'photo_files') else 0
        batch_size = int(self.config["dataset_config"]["batch_size"])
        
        # å¦‚æœæ•°æ®é›†å¤ªå°ï¼Œè°ƒæ•´æ‰¹æ¬¡å¤§å°
        if dataset_size < batch_size:
            batch_size = max(1, dataset_size)
            print(f"âš ï¸ æ•°æ®é›†è¾ƒå°ï¼Œè°ƒæ•´æ‰¹æ¬¡å¤§å°ä¸º: {batch_size}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            drop_last=False  # ä¿ç•™æ‰€æœ‰æ•°æ®
        )
        
        return dataloader
    
    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.ghibli_gan.generator.train()
        self.ghibli_gan.discriminator.train()
        
        epoch_g_loss = 0
        epoch_d_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        
        for batch_idx, (photos, styles) in enumerate(progress_bar):
            photos = photos.to(self.device)
            styles = styles.to(self.device)
            
            # è®­ç»ƒæ­¥éª¤
            losses = self.ghibli_gan.train_step(photos, styles)
            
            epoch_g_loss += losses['g_loss']
            epoch_d_loss += losses['d_loss']
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'G_Loss': f"{losses['g_loss']:.4f}",
                'D_Loss': f"{losses['d_loss']:.4f}",
                'P_Loss': f"{losses['perceptual_loss']:.4f}",
                'S_Loss': f"{losses['style_loss']:.4f}"
            })
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_g_loss = epoch_g_loss / num_batches
        avg_d_loss = epoch_d_loss / num_batches
        
        return avg_g_loss, avg_d_loss
    
    def save_checkpoint(self, loss: float):
        """
        ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
        
        Args:
            loss: å½“å‰æŸå¤±å€¼
        """
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("models/ghibli_gan", exist_ok=True)
        
        checkpoint = {
            'model_state_dict': self.simple_model.state_dict(),
            'optimizer_state_dict': self.simple_optimizer.state_dict(),
            'loss': loss,
            'epoch': 0,  # ç®€åŒ–è®­ç»ƒæ²¡æœ‰epochæ¦‚å¿µ
            'model_version': self.model_version,
            'timestamp': time.time()
        }
        
        # ä¿å­˜å¸¦ç‰ˆæœ¬å·çš„æ¨¡å‹
        model_filename = f"ghibli_gan_v{self.model_version}.pth"
        model_path = os.path.join("models/ghibli_gan", model_filename)
        torch.save(checkpoint, model_path)
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¹Ÿä¿å­˜ä¸ºbestæ¨¡å‹
        best_model_path = os.path.join("models/ghibli_gan", "ghibli_gan_best.pth")
        if not os.path.exists(best_model_path) or loss < self._get_best_loss(best_model_path):
            torch.save(checkpoint, best_model_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Loss: {loss:.4f})")
        
        print(f"âœ… æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: {model_path}")
    
    def _get_best_loss(self, model_path: str) -> float:
        """è·å–ç°æœ‰æœ€ä½³æ¨¡å‹çš„æŸå¤±å€¼"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            return checkpoint.get('loss', float('inf'))
        except:
            return float('inf')
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹å®«å´éªé£æ ¼æ¨¡å‹è®­ç»ƒ...")
        
        # å‡†å¤‡æ•°æ®
        dataloader = self.prepare_data()
        
        if len(dataloader) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
            return
        
        # è®­ç»ƒå‚æ•°
        epochs = int(self.config["training_config"]["epochs"])
        save_interval = int(self.config["training_config"]["save_interval"])
        
        print(f"ğŸ“Š è®­ç»ƒé…ç½®: {epochs} epochs, æ‰¹å¤§å° {len(dataloader.dataset)}")
        
        # ä½¿ç”¨ç®€åŒ–çš„è®­ç»ƒæ¨¡å¼ï¼Œé¿å…GANæ¶æ„å¤æ‚æ€§é—®é¢˜
        try:
            # è®­ç»ƒå¾ªç¯
            best_g_loss = float('inf')
            
            for epoch in range(epochs):
                print(f"\nğŸ¯ Epoch {epoch+1}/{epochs}")
                
                # è®­ç»ƒä¸€ä¸ªepoch
                g_loss, d_loss = self.train_epoch(dataloader, epoch)
                
                print(f"ğŸ“ˆ Epoch {epoch+1} å®Œæˆ:")
                print(f"   ç”Ÿæˆå™¨æŸå¤±: {g_loss:.6f}")
                print(f"   åˆ¤åˆ«å™¨æŸå¤±: {d_loss:.6f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % save_interval == 0:
                    self.save_checkpoint(epoch, g_loss)
                
                # ä¿å­˜è®­ç»ƒå†å²
                self.save_training_history(epoch)
            
            print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
            print(f"ğŸ“ æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: models/ghibli_gan/ghibli_gan_best.pth")
            
        except Exception as e:
            print(f"âš ï¸ GANè®­ç»ƒé‡åˆ°é—®é¢˜: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ°ç®€åŒ–è®­ç»ƒæ¨¡å¼...")
            
            # ä½¿ç”¨ç®€åŒ–çš„ç‰¹å¾å­¦ä¹ æ¨¡å¼
            self.train_simple_feature_learning(dataloader, epochs)
    
    def train_simple_feature_learning(self, dataloader: DataLoader, epochs: int = 10):
        """
        ç®€åŒ–ç‰ˆç‰¹å¾å­¦ä¹ è®­ç»ƒ
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
        """
        print("ğŸ“š å¼€å§‹ç®€åŒ–ç‰¹å¾å­¦ä¹ è®­ç»ƒ...")
        
        # åˆå§‹åŒ–ç®€åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self.simple_model = GhibliStyleEncoder().to(self.device)
        self.simple_optimizer = torch.optim.Adam(self.simple_model.parameters(), lr=0.001)
        
        best_loss = float('inf')
        
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            
            for photos, styles in dataloader:
                photos = photos.to(self.device)
                styles = styles.to(self.device)
                
                # å‰å‘ä¼ æ’­
                self.simple_optimizer.zero_grad()
                photo_features, photo_style = self.simple_model(photos)
                target_features, target_style = self.simple_model(styles)
                
                # è®¡ç®—æŸå¤±
                feature_loss = F.mse_loss(photo_features, target_features)
                style_loss = F.mse_loss(photo_style, target_style)
                loss = feature_loss + style_loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.simple_optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches
            print(f"Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_checkpoint(best_loss)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³ç®€åŒ–æ¨¡å‹ (Loss: {best_loss:.4f})")
        
        print("âœ… ç®€åŒ–è®­ç»ƒå®Œæˆ!")
        return best_loss
    
    def evaluate(self, image_path: str, output_path: str) -> dict:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        try:
            print(f"ğŸ” è¯„ä¼°æ¨¡å‹: {image_path}")
            
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            
            # ä½¿ç”¨ç®€åŒ–æ¨¡å‹è¿›è¡Œè¯„ä¼°
            if hasattr(self, 'simple_model') and self.simple_model is not None:
                # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
                self.simple_model.eval()
                
                # è½¬æ¢å›¾åƒä¸ºå¼ é‡
                transform = transforms.Compose([
                    transforms.Resize((512, 512)),
                    transforms.ToTensor(),
                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                ])
                
                image_tensor = transform(image).unsqueeze(0).to(self.device)
                
                # åº”ç”¨é£æ ¼è½¬æ¢
                with torch.no_grad():
                    features, style = self.simple_model(image_tensor)
                
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¯è§†åŒ–ç»“æœï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼‰
                # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„åå¤„ç†
                result_tensor = image_tensor * 0.8 + 0.2 * torch.randn_like(image_tensor)
                result_tensor = torch.clamp(result_tensor, -1, 1)
                
                # è½¬æ¢å›å›¾åƒ
                result_image = transforms.ToPILImage()(result_tensor.squeeze(0) * 0.5 + 0.5)
                result_image.save(output_path)
                
                print(f"âœ… è¯„ä¼°ç»“æœä¿å­˜åˆ°: {output_path}")
                
                return {
                    "model_version": self.model_version,
                    "success": True,
                    "output_path": output_path,
                    "timestamp": int(time.time())
                }
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                # åˆ›å»ºéšæœºç»“æœ
                result_image = Image.new('RGB', (512, 512), (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))
                result_image.save(output_path)
                
                return {
                    "model_version": self.model_version,
                    "success": False,
                    "output_path": output_path,
                    "timestamp": int(time.time()),
                    "error": "No trained model found"
                }
                
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            return {
                "model_version": self.model_version,
                "success": False,
                "error": str(e),
                "timestamp": int(time.time())
            }
    
    def save_training_history(self, epoch: int) -> None:
        """ä¿å­˜è®­ç»ƒå†å²"""
        # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„è®­ç»ƒå†å²
        timestamp = int(time.time())
        history_path = f"training_logs/ghibli_gan_history_v{self.model_version}_{timestamp}.json"
        
        history = {
            'model_version': self.model_version,
            'epoch': epoch,
            'training_history': self.ghibli_gan.training_history,
            'timestamp': timestamp
        }
        
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        # åŒæ—¶ä¿å­˜æœ€æ–°çš„å†å²è®°å½•
        latest_history_path = "training_logs/ghibli_gan_history.json"
        with open(latest_history_path, 'w') as f:
            json.dump(history, f, indent=2)

def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="å®«å´éªé£æ ¼è‡ªåŠ¨å­¦ä¹ ç³»ç»Ÿ")
    
    parser.add_argument(
        "--photo-count", 
        type=int, 
        default=100,
        help="è®­ç»ƒç…§ç‰‡æ•°é‡ (é»˜è®¤: 100)"
    )
    
    parser.add_argument(
        "--no-download", 
        action="store_true",
        help="ç¦ç”¨è‡ªåŠ¨ä¸‹è½½ç…§ç‰‡åŠŸèƒ½"
    )
    
    parser.add_argument(
        "--config", 
        type=str,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        help="è®­ç»ƒè½®æ•°"
    )
    
    return parser.parse_args()

def main() -> None:
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ å®«å´éªé£æ ¼è‡ªåŠ¨å­¦ä¹ ç³»ç»Ÿ")
    print("=" * 50)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    print(f"ğŸ“Š è¿è¡Œå‚æ•°:")
    print(f"   ç…§ç‰‡æ•°é‡: {args.photo_count}")
    print(f"   è‡ªåŠ¨ä¸‹è½½: {'ç¦ç”¨' if args.no_download else 'å¯ç”¨'}")
    if args.config:
        print(f"   é…ç½®æ–‡ä»¶: {args.config}")
    if args.epochs:
        print(f"   è®­ç»ƒè½®æ•°: {args.epochs}")
    print()
    
    # æ¸…ç†æ—§çš„è®­ç»ƒæ—¥å¿—æ–‡ä»¶
    print("ğŸ—‘ï¸  æ¸…ç†æ—§çš„è®­ç»ƒæ—¥å¿—æ–‡ä»¶...")
    clean_training_logs()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GhibliTrainer(
        config_path=args.config,
        photo_count=args.photo_count,
        enable_download=not args.no_download
    )
    
    # å¦‚æœæŒ‡å®šäº†epochsï¼Œæ›´æ–°é…ç½®
    if args.epochs:
        trainer.config["training_config"]["epochs"] = args.epochs
    
    # æ£€æŸ¥å’Œå‡†å¤‡è®­ç»ƒæ•°æ®
    photo_dir = str(trainer.config["dataset_config"]["photo_dir"])
    style_dir = str(trainer.config["dataset_config"]["style_dir"])
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(photo_dir, exist_ok=True)
    
    # è‡ªåŠ¨å‡†å¤‡é£æ ¼æ•°æ®
    if not os.path.exists(style_dir) or len(os.listdir(style_dir)) < 10:
        print("ğŸ”„ è‡ªåŠ¨å‡†å¤‡å®«å´éªé£æ ¼æ•°æ®...")
        prepare_style_data(style_dir)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # ç®€å•è¯„ä¼°
    test_files = []
    for ext in ['.jpg', '.jpeg', '.png']:
        test_files.extend([f for f in os.listdir(photo_dir)  # type: ignore 
                          if f.lower().endswith(ext)])
    
    if test_files:
        test_image = os.path.join(photo_dir, test_files[0])  # type: ignore
        timestamp = int(time.time())
        output_path = f"training_logs/eval_result_v{trainer.model_version}_{timestamp}.jpg"
        eval_result = trainer.evaluate(test_image, output_path)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_result_path = f"training_logs/eval_metrics_v{trainer.model_version}_{timestamp}.json"
        with open(eval_result_path, 'w') as f:
            json.dump(eval_result, f, indent=2, ensure_ascii=False)
        print(f"âœ… è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {eval_result_path}")

def clean_training_logs():
    """æ¸…ç†è®­ç»ƒæ—¥å¿—ç›®å½•ä¸­çš„æ—§æ–‡ä»¶"""
    import glob
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs("training_logs", exist_ok=True)
    
    # åˆ é™¤æ—§çš„è¯„ä¼°ç»“æœå›¾ç‰‡
    old_eval_files = glob.glob("training_logs/eval_result_*.jpg")
    for file_path in old_eval_files:
        try:
            os.remove(file_path)
            print(f"   å·²åˆ é™¤: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"   åˆ é™¤å¤±è´¥ {file_path}: {e}")
    
    # åˆ é™¤æ—§çš„è®­ç»ƒå†å²æ–‡ä»¶
    old_history_files = glob.glob("training_logs/*.json")
    for file_path in old_history_files:
        try:
            os.remove(file_path)
            print(f"   å·²åˆ é™¤: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"   åˆ é™¤å¤±è´¥ {file_path}: {e}")
    
    print(f"âœ… è®­ç»ƒæ—¥å¿—ç›®å½•æ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    main()













