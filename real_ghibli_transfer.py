#!/usr/bin/env python3
"""
çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - åŸºäºæ·±åº¦å­¦ä¹ å’Œé£æ ¼è¿ç§»
"""

import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import torch.optim as optim
from torch.autograd import Variable
import os
import io
import base64
import time
import threading
from flask import Flask, render_template, request, jsonify

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'static/uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024

# åˆ›å»ºä¸Šä¼ ç›®å½•
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨è½¬æ¢è¿›åº¦
conversion_progress = {}
conversion_results = {}

class RealGhibliStyleTransfer:
    """çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - åŸºäºæ·±åº¦å­¦ä¹ """
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.vgg = self._load_vgg().to(self.device)
        self.style_layers = ['3', '8', '15', '22']  # VGGå±‚ç”¨äºé£æ ¼æå–
        self.content_layers = ['22']  # VGGå±‚ç”¨äºå†…å®¹æå–
        self.progress_callback = None
        self.task_id = None
        
    def _load_vgg(self):
        """åŠ è½½é¢„è®­ç»ƒçš„VGG19æ¨¡å‹"""
        vgg = models.vgg19(pretrained=True).features
        # å†»ç»“å‚æ•°
        for param in vgg.parameters():
            param.requires_grad = False
        return vgg
    
    def _extract_features(self, x, model, layers):
        """ä»VGGæ¨¡å‹ä¸­æå–ç‰¹å¾"""
        features = {}
        for name, layer in model._modules.items():
            x = layer(x)
            if name in layers:
                features[name] = x
        return features
    
    def _gram_matrix(self, x):
        """è®¡ç®—GramçŸ©é˜µï¼ˆé£æ ¼ç‰¹å¾ï¼‰"""
        batch_size, channels, height, width = x.size()
        features = x.view(batch_size * channels, height * width)
        gram = torch.mm(features, features.t())
        return gram.div(batch_size * channels * height * width)
    
    def _load_ghibli_style_images(self):
        """åŠ è½½å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡"""
        style_folder = 'temp'
        style_images = []
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        
        for ext in image_extensions:
            import glob
            pattern = os.path.join(style_folder, ext)
            style_images.extend(glob.glob(pattern))
        
        if not style_images:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤é£æ ¼")
            return None
        
        print(f"ğŸ¨ åŠ è½½äº† {len(style_images)} å¼ å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡")
        return style_images
    
    def _create_ghibli_style_tensor(self, target_size=512):
        """åˆ›å»ºå®«å´éªé£æ ¼ç‰¹å¾å¼ é‡"""
        style_images = self._load_ghibli_style_images()
        
        if not style_images:
            # å¦‚æœæ²¡æœ‰å‚è€ƒå›¾ç‰‡ï¼Œåˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾
            return self._create_default_ghibli_style(target_size)
        
        # åŠ è½½å¹¶å¤„ç†é£æ ¼å›¾ç‰‡
        style_tensors = []
        transform = transforms.Compose([
            transforms.Resize(target_size),
            transforms.CenterCrop(target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        for style_path in style_images:
            try:
                style_img = Image.open(style_path).convert('RGB')
                style_tensor = transform(style_img).unsqueeze(0).to(self.device)
                style_tensors.append(style_tensor)
            except Exception as e:
                print(f"âŒ åŠ è½½é£æ ¼å›¾ç‰‡ {style_path} å¤±è´¥: {e}")
        
        if not style_tensors:
            return self._create_default_ghibli_style(target_size)
        
        # å¹³å‡æ‰€æœ‰é£æ ¼å›¾ç‰‡çš„ç‰¹å¾
        style_features = {}
        for style_tensor in style_tensors:
            features = self._extract_features(style_tensor, self.vgg, self.style_layers)
            for layer, feature in features.items():
                if layer not in style_features:
                    style_features[layer] = []
                style_features[layer].append(self._gram_matrix(feature))
        
        # è®¡ç®—å¹³å‡é£æ ¼ç‰¹å¾
        avg_style_features = {}
        for layer, gram_list in style_features.items():
            avg_gram = torch.stack(gram_list).mean(dim=0)
            avg_style_features[layer] = avg_gram
        
        return avg_style_features
    
    def _create_default_ghibli_style(self, target_size):
        """åˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾"""
        print("ğŸ¨ ä½¿ç”¨é»˜è®¤å®«å´éªé£æ ¼ç‰¹å¾")
        
        # åˆ›å»ºå…·æœ‰å®«å´éªé£æ ¼ç‰¹å¾çš„é»˜è®¤é£æ ¼
        # å®«å´éªé£æ ¼ç‰¹ç‚¹ï¼šæŸ”å’Œè‰²å½©ã€æ¢¦å¹»å…‰å½±ã€ç®€æ´çº¿æ¡
        default_style = {}
        
        # è¿™é‡Œåº”è¯¥åŸºäºå®«å´éªçš„è‰ºæœ¯ç‰¹ç‚¹åˆ›å»ºé£æ ¼ç‰¹å¾
        # ç”±äºæ—¶é—´å…³ç³»ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
        
        return default_style
    
    def apply_real_ghibli_style(self, content_image, num_steps=100, style_weight=1000, content_weight=1):
        """åº”ç”¨çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢"""
        
        # é¢„å¤„ç†å†…å®¹å›¾åƒ
        content_tensor = self._preprocess_image(content_image).to(self.device)
        
        # åˆ›å»ºå®«å´éªé£æ ¼ç‰¹å¾
        style_features = self._create_ghibli_style_tensor()
        
        if not style_features:
            # å¦‚æœæ— æ³•åˆ›å»ºé£æ ¼ç‰¹å¾ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ä½œä¸ºå¤‡é€‰
            return self._fallback_traditional_method(content_image)
        
        # æå–å†…å®¹ç‰¹å¾
        content_features = self._extract_features(content_tensor, self.vgg, self.content_layers)
        
        # åˆå§‹åŒ–è¾“å‡ºå›¾åƒï¼ˆä½¿ç”¨å†…å®¹å›¾åƒä½œä¸ºèµ·ç‚¹ï¼‰
        input_img = content_tensor.clone().requires_grad_(True)
        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæ›´ç¨³å®š
        optimizer = optim.Adam([input_img], lr=0.01)
        
        # é£æ ¼è¿ç§»ä¼˜åŒ–
        print("ğŸ”„ å¼€å§‹é£æ ¼è¿ç§»ä¼˜åŒ–...")
        
        for step in range(num_steps):
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            input_img.data.clamp_(0, 1)
            features = self._extract_features(input_img, self.vgg, self.style_layers + self.content_layers)
            
            # è®¡ç®—æŸå¤±
            style_loss = 0
            content_loss = 0
            
            # é£æ ¼æŸå¤±
            for layer in self.style_layers:
                if layer in features:
                    target_gram = style_features[layer]
                    current_gram = self._gram_matrix(features[layer])
                    style_loss += F.mse_loss(current_gram, target_gram)
            
            # å†…å®¹æŸå¤±
            for layer in self.content_layers:
                if layer in features:
                    target_content = content_features[layer]
                    current_content = features[layer]
                    content_loss += F.mse_loss(current_content, target_content)
            
            # æ€»æŸå¤± - æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            total_loss = style_weight * style_loss + content_weight * content_loss
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºnan
            if torch.isnan(total_loss):
                print(f"âš ï¸ æ­¥éª¤ {step+1}: æ£€æµ‹åˆ°nanæŸå¤±ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ³•")
                return self._fallback_traditional_method(content_image)
            
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_([input_img], max_norm=1.0)
            
            optimizer.step()
            
            # æ›´æ–°è¿›åº¦
            progress = int((step + 1) / num_steps * 100)
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, progress, step + 1, num_steps, total_loss.item())
            
            if (step + 1) % 20 == 0:
                print(f"æ­¥éª¤ {step+1}/{num_steps}, æ€»æŸå¤±: {total_loss.item():.4f}")
        
        # åå¤„ç†è¾“å‡ºå›¾åƒ
        output_tensor = input_img.data.clamp(0, 1)
        result_image = self._postprocess_image(output_tensor)
        
        return result_image
    
    def set_progress_callback(self, callback, task_id):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
        self.task_id = task_id
    
    def _preprocess_image(self, image):
        """é¢„å¤„ç†å›¾åƒ"""
        transform = transforms.Compose([
            transforms.Resize(512),
            transforms.CenterCrop(512),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).unsqueeze(0)
    
    def _postprocess_image(self, tensor):
        """åå¤„ç†å¼ é‡ä¸ºå›¾åƒ"""
        # åå½’ä¸€åŒ–
        tensor = tensor.squeeze(0).cpu()
        tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        tensor = torch.clamp(tensor, 0, 1)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        transform = transforms.ToPILImage()
        image = transform(tensor)
        
        return image
    
    def _fallback_traditional_method(self, image):
        """å¤‡é€‰ä¼ ç»Ÿæ–¹æ³•"""
        print("âš ï¸ ä½¿ç”¨å¤‡é€‰ä¼ ç»Ÿæ–¹æ³•")
        
        # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
        img_np = np.array(image)
        
        # è½¬æ¢ä¸ºBGRæ ¼å¼
        if len(img_np.shape) == 3 and img_np.shape[2] == 3:
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        else:
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        
        # é«˜è´¨é‡çš„å®«å´éªé£æ ¼å¤„ç†
        
        # 1. ä¿æŒåŸå§‹åˆ†è¾¨ç‡
        h, w = img_bgr.shape[:2]
        max_size = 2000
        if max(h, w) > max_size:
            scale = max_size / max(h, w)
            new_w, new_h = int(w * scale), int(h * scale)
            img_bgr = cv2.resize(img_bgr, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
        
        # 2. æ™ºèƒ½è¾¹ç¼˜ä¿ç•™ï¼ˆé‡ç‚¹æ”¹è¿›äººç‰©åŒºåŸŸï¼‰
        # ä½¿ç”¨å¯¼å‘æ»¤æ³¢ä¿æŒè¾¹ç¼˜
        guided = cv2.ximgproc.guidedFilter(
            guide=img_bgr, 
            src=img_bgr, 
            radius=10, 
            eps=0.01
        )
        
        # 3. å®«å´éªé£æ ¼è‰²å½©è°ƒæ•´
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç¡®çš„è‰²å½©è°ƒæ•´
        lab = cv2.cvtColor(guided, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºè‰²å½©é²œè‰³åº¦ï¼ˆå®«å´éªé£æ ¼ç‰¹ç‚¹ï¼‰
        a = cv2.addWeighted(a, 1.2, a, 0, 0)
        b = cv2.addWeighted(b, 1.2, b, 0, 0)
        
        # è°ƒæ•´äº®åº¦å’Œå¯¹æ¯”åº¦
        l = cv2.createCLAHE(clipLimit=2.0).apply(l)
        
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # 4. æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ
        h, w = enhanced.shape[:2]
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½©
        light_mask = 1.0 - (distance / max_distance) * 0.15
        light_mask = np.clip(light_mask, 0.85, 1.0)
        
        # åº”ç”¨å…‰ç…§æ•ˆæœ
        final = enhanced.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        # è½¬æ¢å›RGB
        result_rgb = cv2.cvtColor(final, cv2.COLOR_BGR2RGB)
        
        return result_rgb

# åˆ›å»ºçœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢æ¨¡å‹
real_ghibli_model = RealGhibliStyleTransfer()

def update_progress(task_id, progress, current_step, total_steps, loss):
    """æ›´æ–°è½¬æ¢è¿›åº¦"""
    conversion_progress[task_id] = {
        'progress': progress,
        'current_step': current_step,
        'total_steps': total_steps,
        'loss': loss,
        'timestamp': time.time()
    }
    print(f"ğŸ“Š ä»»åŠ¡ {task_id}: {progress}% (æ­¥éª¤ {current_step}/{total_steps}, æŸå¤±: {loss:.4f})")

def convert_image_async(task_id, image):
    """å¼‚æ­¥è½¬æ¢å›¾åƒ"""
    try:
        # è®¾ç½®è¿›åº¦å›è°ƒ
        real_ghibli_model.set_progress_callback(update_progress, task_id)
        
        # å¼€å§‹è½¬æ¢
        result_image = real_ghibli_model.apply_real_ghibli_style(image, num_steps=100)
        
        # ä¿å­˜ç»“æœ
        conversion_results[task_id] = {
            'success': True,
            'result_image': result_image,
            'completed': True
        }
        
        # æ›´æ–°è¿›åº¦ä¸ºå®Œæˆ
        update_progress(task_id, 100, 100, 100, 0)
        
    except Exception as e:
        conversion_results[task_id] = {
            'success': False,
            'error': str(e),
            'completed': True
        }
        print(f"âŒ ä»»åŠ¡ {task_id} è½¬æ¢å¤±è´¥: {e}")

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')

@app.route('/progress/<task_id>')
def get_progress(task_id):
    """è·å–è½¬æ¢è¿›åº¦"""
    if task_id in conversion_progress:
        return jsonify(conversion_progress[task_id])
    else:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

@app.route('/result/<task_id>')
def get_result(task_id):
    """è·å–è½¬æ¢ç»“æœ"""
    if task_id in conversion_results:
        result = conversion_results[task_id]
        if result['completed']:
            if result['success']:
                # è½¬æ¢ä¸ºbase64
                result_image = result['result_image']
                
                # æ£€æŸ¥ç»“æœç±»å‹å¹¶æ­£ç¡®å¤„ç†
                if isinstance(result_image, np.ndarray):
                    if result_image.dtype == np.float32 or result_image.dtype == np.float64:
                        result_image = (result_image * 255).astype(np.uint8)
                    result_image = Image.fromarray(result_image)
                
                buffered = io.BytesIO()
                result_image.save(buffered, format="JPEG", quality=95)
                img_str = base64.b64encode(buffered.getvalue()).decode()
                
                return jsonify({
                    'success': True,
                    'result': f"data:image/jpeg;base64,{img_str}",
                    'completed': True
                })
            else:
                return jsonify({
                    'success': False,
                    'error': result['error'],
                    'completed': True
                })
        else:
            return jsonify({'success': False, 'error': 'è½¬æ¢å°šæœªå®Œæˆ', 'completed': False})
    else:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

@app.route('/upload', methods=['POST'])
def upload_file():
    """å¤„ç†æ–‡ä»¶ä¸Šä¼ å’Œé£æ ¼è½¬æ¢"""
    try:
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'})
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'})
        
        # è¯»å–å›¾ç‰‡
        image = Image.open(file.stream)
        
        # ä¿å­˜åŸå›¾ç”¨äºæ˜¾ç¤º
        original_buffered = io.BytesIO()
        image.save(original_buffered, format="JPEG", quality=95)
        original_img_str = base64.b64encode(original_buffered.getvalue()).decode()
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(int(time.time() * 1000))
        
        # å¯åŠ¨å¼‚æ­¥è½¬æ¢
        thread = threading.Thread(target=convert_image_async, args=(task_id, image))
        thread.daemon = True
        thread.start()
        
        print(f"ğŸ¨ å¼€å§‹å¼‚æ­¥å®«å´éªé£æ ¼è½¬æ¢ï¼Œä»»åŠ¡ID: {task_id}")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'original': f"data:image/jpeg;base64,{original_img_str}",
            'message': 'è½¬æ¢ä»»åŠ¡å·²å¼€å§‹ï¼Œè¯·ç­‰å¾…å®Œæˆ'
        })
        
    except Exception as e:
        import traceback
        print(f"âŒ è½¬æ¢é”™è¯¯: {str(e)}")
        print(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5005)