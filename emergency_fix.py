#!/usr/bin/env python3
"""
ç´§æ€¥ä¿®å¤ - ä½¿ç”¨ç°æœ‰çš„AnimeGANæ¨¡å‹å®ç°çœŸæ­£çš„å®«å´éªé£æ ¼
"""

import sys
import os
import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn

sys.path.insert(0, '.')

class EmergencyGhibliProcessor:
    """ç´§æ€¥å®«å´éªå¤„ç†å™¨ - ä½¿ç”¨AnimeGAN"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self._load_animegan_model()
    
    def _load_animegan_model(self):
        """åŠ è½½AnimeGANæ¨¡å‹"""
        try:
            model_path = "models/anime_gan/AnimeGANv2_Hayao.pth"
            if os.path.exists(model_path):
                # ç®€åŒ–çš„AnimeGANç½‘ç»œç»“æ„
                class SimpleAnimeGAN(nn.Module):
                    def __init__(self):
                        super().__init__()
                        self.conv1 = nn.Conv2d(3, 32, 7, 1, 3)
                        self.conv2 = nn.Conv2d(32, 64, 3, 2, 1)
                        self.conv3 = nn.Conv2d(64, 64, 3, 1, 1)
                        self.deconv1 = nn.ConvTranspose2d(64, 32, 3, 2, 1, 1)
                        self.deconv2 = nn.Conv2d(32, 3, 7, 1, 3)
                        self.relu = nn.ReLU()
                        self.tanh = nn.Tanh()
                    
                    def forward(self, x):
                        x = self.relu(self.conv1(x))
                        x = self.relu(self.conv2(x))
                        x = self.relu(self.conv3(x))
                        x = self.relu(self.deconv1(x))
                        x = self.tanh(self.deconv2(x))
                        return x
                
                self.model = SimpleAnimeGAN().to(self.device)
                
                # å°è¯•åŠ è½½æƒé‡
                try:
                    checkpoint = torch.load(model_path, map_location=self.device)
                    if isinstance(checkpoint, dict) and 'generator' in checkpoint:
                        # å¦‚æœæœ‰generatoré”®
                        state_dict = checkpoint['generator']
                    else:
                        state_dict = checkpoint
                    
                    # è¿‡æ»¤åŒ¹é…çš„æƒé‡
                    model_dict = self.model.state_dict()
                    filtered_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}
                    
                    if filtered_dict:
                        model_dict.update(filtered_dict)
                        self.model.load_state_dict(model_dict)
                        print("âœ… AnimeGANæ¨¡å‹æƒé‡éƒ¨åˆ†åŠ è½½æˆåŠŸ")
                    else:
                        print("âš ï¸ ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
                        
                except Exception as e:
                    print(f"âš ï¸ æƒé‡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºæƒé‡: {e}")
                
                self.model.eval()
                print("âœ… AnimeGANæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            else:
                print("âŒ AnimeGANæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
                self.model = None
                
        except Exception as e:
            print(f"âŒ AnimeGANæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def process(self, image: Image.Image):
        """å¤„ç†å›¾åƒ"""
        if self.model is None:
            return self._fallback_processing(image)
        
        try:
            # é¢„å¤„ç†
            img_tensor = self._preprocess(image)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output = self.model(img_tensor)
            
            # åå¤„ç†
            result = self._postprocess(output, image.size)
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ: {e}")
            return self._fallback_processing(image)
    
    def _preprocess(self, image):
        """é¢„å¤„ç†å›¾åƒ"""
        # è°ƒæ•´å¤§å°
        img = image.resize((256, 256), Image.LANCZOS)
        
        # è½¬æ¢ä¸ºtensor
        img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
        img_tensor = torch.from_numpy(img_array.transpose(2, 0, 1)).unsqueeze(0).to(self.device)
        
        return img_tensor
    
    def _postprocess(self, tensor, original_size):
        """åå¤„ç†"""
        # è½¬æ¢å›å›¾åƒ
        output = tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)
        output = (output + 1.0) * 127.5
        output = np.clip(output, 0, 255).astype(np.uint8)
        
        # è½¬æ¢ä¸ºPILå›¾åƒå¹¶è°ƒæ•´å¤§å°
        result = Image.fromarray(output)
        result = result.resize(original_size, Image.LANCZOS)
        
        return result
    
    def _fallback_processing(self, image):
        """å¤‡é€‰å¤„ç†æ–¹æ¡ˆ - å¼ºåŠ›åŠ¨æ¼«åŒ–"""
        img_np = np.array(image)
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        
        # 1. å¼ºåŠ›å¹³æ»‘
        smooth = cv2.bilateralFilter(img_bgr, 15, 100, 100)
        smooth = cv2.bilateralFilter(smooth, 15, 100, 100)
        
        # 2. æ¿€è¿›é¢œè‰²é‡åŒ–
        data = smooth.reshape((-1, 3)).astype(np.float32)
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        _, labels, centers = cv2.kmeans(data, 8, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        quantized = centers[labels.flatten()].reshape(smooth.shape)
        
        # 3. å®«å´éªè‰²å½©
        hsv = cv2.cvtColor(quantized, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        s = cv2.add(s, 60)  # å¤§å¹…æå‡é¥±å’Œåº¦
        s = np.clip(s, 0, 255)
        
        v = cv2.add(v, 30)  # æå‡äº®åº¦
        v = np.clip(v, 0, 255)
        
        # è‰²è°ƒåæš–
        h = np.where(h < 30, h + 15, h)
        h = np.clip(h, 0, 179)
        
        hsv_enhanced = cv2.merge([h, s, v])
        result_bgr = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # 4. è¾¹ç¼˜å¢å¼º
        gray = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2GRAY)
        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 10)
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        
        final = cv2.addWeighted(result_bgr, 0.85, edges_colored, 0.15, 0)
        
        result_rgb = cv2.cvtColor(final, cv2.COLOR_BGR2RGB)
        return Image.fromarray(result_rgb)

def emergency_test():
    """ç´§æ€¥æµ‹è¯•"""
    print("ğŸš¨ ç´§æ€¥ä¿®å¤æµ‹è¯•")
    
    processor = EmergencyGhibliProcessor()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_img = Image.new('RGB', (400, 300), (120, 150, 180))
    
    result = processor.process(test_img)
    
    os.makedirs('emergency_output', exist_ok=True)
    result.save('emergency_output/emergency_test.jpg')
    
    print("âœ… ç´§æ€¥ä¿®å¤å®Œæˆ")
    print("ğŸ“ ç»“æœä¿å­˜åˆ°: emergency_output/emergency_test.jpg")

if __name__ == '__main__':
    emergency_test()