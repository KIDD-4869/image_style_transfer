#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å®«å´éªé£æ ¼ä¸“ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GhibliGAN)
ç«¯åˆ°ç«¯çš„åŠ¨æ¼«é£æ ¼è½¬æ¢ç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torchvision.models import vgg19
import torchvision.models as models
import numpy as np
from PIL import Image
import os
import json
import time

class ResidualBlock(nn.Module):
    """æ®‹å·®å— - ç”¨äºæ„å»ºæ·±åº¦ç”Ÿæˆå™¨ç½‘ç»œ"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu1 = nn.LeakyReLU(0.2, inplace=True)
        
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu2 = nn.LeakyReLU(0.2, inplace=True)
        
        # è·³è·ƒè¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        residual = self.shortcut(x)
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu1(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        out += residual
        out = self.relu2(out)
        
        return out

class SelfAttention(nn.Module):
    """è‡ªæ³¨æ„åŠ›æœºåˆ¶ - æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»"""
    
    def __init__(self, in_channels):
        super(SelfAttention, self).__init__()
        
        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.value = nn.Conv2d(in_channels, in_channels, 1)
        self.gamma = nn.Parameter(torch.zeros(1))
        
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        batch_size, channels, height, width = x.size()
        
        # è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼
        proj_query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)
        proj_key = self.key(x).view(batch_size, -1, width * height)
        proj_value = self.value(x).view(batch_size, -1, width * height)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, channels, height, width)
        
        # æ®‹å·®è¿æ¥
        out = self.gamma * out + x
        
        return out

class GhibliGenerator(nn.Module):
    """å®«å´éªé£æ ¼ç”Ÿæˆå™¨"""
    
    def __init__(self, input_channels=3, output_channels=3):
        super(GhibliGenerator, self).__init__()
        
        # ç¼–ç å™¨ - æå–ç‰¹å¾
        self.encoder = nn.Sequential(
            # åˆå§‹å·ç§¯å±‚
            nn.Conv2d(input_channels, 64, 7, 1, 3),
            nn.InstanceNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ä¸‹é‡‡æ ·å±‚1
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.InstanceNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ä¸‹é‡‡æ ·å±‚2
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.InstanceNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ä¸‹é‡‡æ ·å±‚3
            nn.Conv2d(256, 512, 4, 2, 1),
            nn.InstanceNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
        )
        
        # æ®‹å·®å— - æ·±åº¦ç‰¹å¾æå–
        self.residual_blocks = nn.Sequential(
            *[ResidualBlock(512, 512) for _ in range(9)]
        )
        
        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.attention = SelfAttention(512)
        
        # è§£ç å™¨ - é‡å»ºå›¾åƒ
        self.decoder = nn.Sequential(
            # ä¸Šé‡‡æ ·å±‚1
            nn.ConvTranspose2d(512, 256, 4, 2, 1),
            nn.InstanceNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ä¸Šé‡‡æ ·å±‚2
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.InstanceNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ä¸Šé‡‡æ ·å±‚3
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.InstanceNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            
            # è¾“å‡ºå±‚
            nn.Conv2d(64, output_channels, 7, 1, 3),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´ [-1, 1]
        )
        
        # å®«å´éªé£æ ¼ç‰¹å¾å¢å¼º
        self.style_enhancer = nn.Sequential(
            nn.Conv2d(512, 512, 3, 1, 1),
            nn.InstanceNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1),
            nn.InstanceNorm2d(512),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # ç¼–ç ç‰¹å¾
        features = self.encoder(x)
        
        # æ®‹å·®å¤„ç†
        residual_out = self.residual_blocks(features)
        
        # è‡ªæ³¨æ„åŠ›
        attention_out = self.attention(residual_out)
        
        # é£æ ¼å¢å¼º
        style_weights = self.style_enhancer(attention_out)
        enhanced_features = attention_out * style_weights
        
        # è§£ç ç”Ÿæˆ
        output = self.decoder(enhanced_features)
        
        return output

class GhibliDiscriminator(nn.Module):
    """å®«å´éªé£æ ¼åˆ¤åˆ«å™¨"""
    
    def __init__(self, input_channels=3):
        super(GhibliDiscriminator, self).__init__()
        
        def discriminator_block(in_channels, out_channels, stride=2, normalize=True):
            layers = [nn.Conv2d(in_channels, out_channels, 4, stride, 1)]
            if normalize:
                layers.append(nn.InstanceNorm2d(out_channels))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return nn.Sequential(*layers)
        
        self.model = nn.Sequential(
            discriminator_block(input_channels, 64, normalize=False),
            discriminator_block(64, 128),
            discriminator_block(128, 256),
            discriminator_block(256, 512),
            discriminator_block(512, 1024),
            
            # è¾“å‡ºå±‚
            nn.Conv2d(1024, 1, 4, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.model(x)

class PerceptualLoss(nn.Module):
    """æ„ŸçŸ¥æŸå¤± - åŸºäºVGG19ç‰¹å¾"""
    
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        
        # åŠ è½½é¢„è®­ç»ƒVGG19
        vgg = vgg19(weights=models.VGG19_Weights.DEFAULT).features
        
        # å†»ç»“å‚æ•°
        for param in vgg.parameters():
            param.requires_grad = False
        
        # é€‰æ‹©ç‰¹å¾å±‚
        self.features = nn.Sequential()
        for i, layer in enumerate(list(vgg)[:35]):  # åˆ°conv4_3
            self.features.add_module(str(i), layer)
        
        # MSEæŸå¤±
        self.mse_loss = nn.MSELoss()
    
    def forward(self, generated, target):
        # ç‰¹å¾æå–
        gen_features = self.features(generated)
        target_features = self.features(target)
        
        # è®¡ç®—æ„ŸçŸ¥æŸå¤±
        return self.mse_loss(gen_features, target_features)

class StyleLoss(nn.Module):
    """é£æ ¼æŸå¤± - GramçŸ©é˜µåŒ¹é…"""
    
    def __init__(self):
        super(StyleLoss, self).__init__()
        
        # åŠ è½½é¢„è®­ç»ƒVGG19
        vgg = vgg19(weights=models.VGG19_Weights.DEFAULT).features
        
        # å†»ç»“å‚æ•°
        for param in vgg.parameters():
            param.requires_grad = False
        
        # é€‰æ‹©å¤šä¸ªç‰¹å¾å±‚
        self.style_layers = {
            '3': 'conv1_2',
            '8': 'conv2_2', 
            '15': 'conv3_3',
            '22': 'conv4_3'
        }
        
        self.features = {}
        
        def hook_fn(module, input, output, layer_name):
            self.features[layer_name] = output
        
        # æ³¨å†Œé’©å­
        for name, module in vgg._modules.items():
            if name in self.style_layers:
                module.register_forward_hook(
                    lambda m, i, o, n=name: hook_fn(m, i, o, n)
                )
        
        self.vgg = vgg
        self.mse_loss = nn.MSELoss()
    
    def gram_matrix(self, tensor):
        """è®¡ç®—GramçŸ©é˜µ"""
        batch_size, channels, height, width = tensor.size()
        
        # é‡å¡‘ä¸º [batch_size, channels, height*width]
        tensor = tensor.view(batch_size, channels, height * width)
        
        # è®¡ç®—GramçŸ©é˜µ
        gram = torch.bmm(tensor, tensor.transpose(1, 2))
        
        # å½’ä¸€åŒ–
        gram = gram / (channels * height * width)
        
        return gram
    
    def forward(self, generated, target):
        # æå–ç‰¹å¾
        self.vgg(generated)
        gen_features = self.features.copy()
        
        self.features.clear()
        self.vgg(target)
        target_features = self.features.copy()
        
        # è®¡ç®—é£æ ¼æŸå¤±
        style_loss = 0
        for layer_name in gen_features:
            gen_gram = self.gram_matrix(gen_features[layer_name])
            target_gram = self.gram_matrix(target_features[layer_name])
            style_loss += self.mse_loss(gen_gram, target_gram)
        
        return style_loss / len(gen_features)

class GhibliGAN:
    """å®«å´éªé£æ ¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œè®­ç»ƒå’Œæ¨ç†"""
    
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.generator = GhibliGenerator().to(device)
        self.discriminator = GhibliDiscriminator().to(device)
        
        # æŸå¤±å‡½æ•°
        self.adversarial_loss = nn.BCELoss()
        self.perceptual_loss = PerceptualLoss().to(device)
        self.style_loss = StyleLoss().to(device)
        self.l1_loss = nn.L1Loss()
        
        # ä¼˜åŒ–å™¨
        self.g_optimizer = torch.optim.Adam(
            self.generator.parameters(), 
            lr=0.0002, 
            betas=(0.5, 0.999)
        )
        self.d_optimizer = torch.optim.Adam(
            self.discriminator.parameters(), 
            lr=0.0002, 
            betas=(0.5, 0.999)
        )
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'g_loss': [],
            'd_loss': [],
            'perceptual_loss': [],
            'style_loss': []
        }
    
    def train_step(self, real_photos, anime_styles):
        """å•æ­¥è®­ç»ƒ"""
        batch_size = real_photos.size(0)
        
        # çœŸå®å’Œè™šå‡æ ‡ç­¾
        real_label = torch.ones(batch_size, 1, 16, 16).to(self.device)
        fake_label = torch.zeros(batch_size, 1, 16, 16).to(self.device)
        
        # è®­ç»ƒåˆ¤åˆ«å™¨
        self.d_optimizer.zero_grad()
        
        # çœŸå®ç…§ç‰‡çš„åˆ¤åˆ«
        real_output = self.discriminator(real_photos)
        d_loss_real = self.adversarial_loss(real_output, real_label)
        
        # ç”ŸæˆåŠ¨æ¼«å›¾åƒçš„åˆ¤åˆ«
        fake_anime = self.generator(real_photos)
        fake_output = self.discriminator(fake_anime.detach())
        d_loss_fake = self.adversarial_loss(fake_output, fake_label)
        
        # åˆ¤åˆ«å™¨æ€»æŸå¤±
        d_loss = (d_loss_real + d_loss_fake) * 0.5
        d_loss.backward()
        self.d_optimizer.step()
        
        # è®­ç»ƒç”Ÿæˆå™¨
        self.g_optimizer.zero_grad()
        
        # é‡æ–°ç”Ÿæˆå¹¶åˆ¤åˆ«
        fake_anime = self.generator(real_photos)
        fake_output = self.discriminator(fake_anime)
        
        # ç”Ÿæˆå™¨æŸå¤±
        g_adv_loss = self.adversarial_loss(fake_output, real_label)
        g_perceptual_loss = self.perceptual_loss(fake_anime, anime_styles)
        g_style_loss = self.style_loss(fake_anime, anime_styles)
        g_l1_loss = self.l1_loss(fake_anime, anime_styles)
        
        # æ€»æŸå¤±ï¼ˆåŠ æƒç»„åˆï¼‰
        g_loss = (g_adv_loss * 1.0 + 
                 g_perceptual_loss * 10.0 + 
                 g_style_loss * 100.0 + 
                 g_l1_loss * 10.0)
        
        g_loss.backward()
        self.g_optimizer.step()
        
        # è®°å½•æŸå¤±
        self.training_history['g_loss'].append(g_loss.item())
        self.training_history['d_loss'].append(d_loss.item())
        self.training_history['perceptual_loss'].append(g_perceptual_loss.item())
        self.training_history['style_loss'].append(g_style_loss.item())
        
        return {
            'g_loss': g_loss.item(),
            'd_loss': d_loss.item(),
            'perceptual_loss': g_perceptual_loss.item(),
            'style_loss': g_style_loss.item()
        }
    
    def save_model(self, path, epoch=None):
        """ä¿å­˜æ¨¡å‹"""
        state = {
            'generator': self.generator.state_dict(),
            'discriminator': self.discriminator.state_dict(),
            'g_optimizer': self.g_optimizer.state_dict(),
            'd_optimizer': self.d_optimizer.state_dict(),
            'training_history': self.training_history
        }
        
        if epoch is not None:
            state['epoch'] = epoch
        
        torch.save(state, path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return False
        
        state = torch.load(path, map_location=self.device)
        
        self.generator.load_state_dict(state['generator'])
        self.discriminator.load_state_dict(state['discriminator'])
        
        if 'g_optimizer' in state:
            self.g_optimizer.load_state_dict(state['g_optimizer'])
        if 'd_optimizer' in state:
            self.d_optimizer.load_state_dict(state['d_optimizer'])
        if 'training_history' in state:
            self.training_history = state['training_history']
        
        print(f"âœ… æ¨¡å‹å·²ä» {path} åŠ è½½")
        return True
    
    def inference(self, image):
        """æ¨ç†è½¬æ¢"""
        self.generator.eval()
        
        with torch.no_grad():
            # é¢„å¤„ç†
            transform = transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])
            
            if isinstance(image, Image.Image):
                input_tensor = transform(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image)
                input_tensor = transform(pil_image).unsqueeze(0).to(self.device)
            else:
                raise ValueError("è¾“å…¥å¿…é¡»æ˜¯PILå›¾åƒæˆ–numpyæ•°ç»„")
            
            # ç”Ÿæˆ
            output_tensor = self.generator(input_tensor)
            
            # åå¤„ç†
            output_tensor = (output_tensor + 1) / 2  # åå½’ä¸€åŒ–åˆ°[0,1]
            output_tensor = torch.clamp(output_tensor, 0, 1)
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            output_image = transforms.ToPILImage()(output_tensor.squeeze(0))
            
            return output_image

# å…¨å±€å®ä¾‹
ghibli_gan = GhibliGAN()

def convert_with_ghibli_gan(image, model_path=None, progress_callback=None):
    """
    ä½¿ç”¨GhibliGANè¿›è¡Œé£æ ¼è½¬æ¢
    
    Args:
        image: PILå›¾åƒæˆ–numpyæ•°ç»„
        model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    
    Returns:
        PILå›¾åƒ
    """
    try:
        if progress_callback:
            progress_callback("åŠ è½½æ¨¡å‹...", 10)
        
        # åŠ è½½æ¨¡å‹
        if model_path and os.path.exists(model_path):
            ghibli_gan.load_model(model_path)
        elif model_path:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {model_path}")
        
        if progress_callback:
            progress_callback("é¢„å¤„ç†å›¾åƒ...", 30)
        
        # æ¨ç†è½¬æ¢
        if progress_callback:
            progress_callback("ç”Ÿæˆå®«å´éªé£æ ¼...", 60)
        
        result = ghibli_gan.inference(image)
        
        if progress_callback:
            progress_callback("å®Œæˆ!", 100)
        
        return result
        
    except Exception as e:
        print(f"âŒ GhibliGANè½¬æ¢å¤±è´¥: {e}")
        return None

def create_sample_training_data():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®é…ç½®"""
    config = {
        "dataset_config": {
            "photo_dir": "training_data/photos",
            "style_dir": "training_data/ghibli_styles",
            "batch_size": 4,
            "image_size": 512,
            "num_workers": 4
        },
        "training_config": {
            "epochs": 100,
            "save_interval": 10,
            "lr": 0.0002,
            "beta1": 0.5,
            "beta2": 0.999
        },
        "loss_weights": {
            "adversarial": 1.0,
            "perceptual": 10.0,
            "style": 100.0,
            "l1": 10.0
        }
    }
    
    # ä¿å­˜é…ç½®
    os.makedirs("models", exist_ok=True)
    with open("models/ghibli_gan_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print("âœ… GhibliGANè®­ç»ƒé…ç½®å·²åˆ›å»º")
    return config

if __name__ == "__main__":
    # åˆ›å»ºé…ç½®
    create_sample_training_data()
    
    # æµ‹è¯•æ¨¡å‹
    print("ğŸ¨ æµ‹è¯•GhibliGAN...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
    
    def test_progress(stage, progress):
        print(f"ğŸ“Š {stage}: {progress}%")
    
    result = convert_with_ghibli_gan(test_image, progress_callback=test_progress)
    
    if result:
        print("âœ… GhibliGANæµ‹è¯•æˆåŠŸ")
        result.save("test_ghibli_gan_output.jpg")
        print("ğŸ“ è¾“å‡ºä¿å­˜åˆ°: test_ghibli_gan_output.jpg")
    else:
        print("âŒ GhibliGANæµ‹è¯•å¤±è´¥")