#!/usr/bin/env python3
"""
çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - åŸºäºæ·±åº¦å­¦ä¹ å’Œé£æ ¼è¿ç§»
é›†æˆé¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
"""

import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import torch.optim as optim
from torch.autograd import Variable
import os
import io
import base64
import time
import threading

# å¯¼å…¥ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ¨¡å—
try:
    from .neural_style_transfer import NeuralStyleTransfer, style_transfer_manager
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
    NeuralStyleTransfer = None
    style_transfer_manager = None

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨è½¬æ¢è¿›åº¦
conversion_progress = {}
conversion_results = {}

class RealGhibliStyleTransfer:
    """çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - åŸºäºæ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰ä¼˜åŒ–
    é›†æˆé¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
    """
    
    def __init__(self, use_neural_network=True):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.vgg = self._load_vgg().to(self.device)
        self.style_layers = ['3', '8', '15', '22']  # VGGå±‚ç”¨äºé£æ ¼æå–
        self.content_layers = ['22']  # VGGå±‚ç”¨äºå†…å®¹æå–
        self.progress_callback = None
        self.task_id = None
        
        # ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ¨¡å‹
        self.use_neural_network = use_neural_network
        self.neural_model = None
        
        # è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.seg_model = None
        
        if use_neural_network and NeuralStyleTransfer is not None:
            try:
                self.neural_model = NeuralStyleTransfer(model_type='vgg19')
                print("âœ… ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ç¥ç»ç½‘ç»œæ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                self.use_neural_network = False
        
        # åˆå§‹åŒ–è‡ªä¸»å­¦ä¹ å™¨
        self.auto_learner = None  # æš‚æ—¶ç¦ç”¨è‡ªä¸»å­¦ä¹ åŠŸèƒ½
        
        # æ˜¯å¦å¯ç”¨è‡ªä¸»å­¦ä¹ 
        self.enable_auto_learning = False
        
    def _load_vgg(self):
        """åŠ è½½é¢„è®­ç»ƒçš„VGG19æ¨¡å‹"""
        try:
            # ä½¿ç”¨æ–°çš„APIåŠ è½½VGG19æ¨¡å‹
            vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features
        except AttributeError:
            # å›é€€åˆ°æ—§ç‰ˆæœ¬API
            vgg = models.vgg19(pretrained=True).features
        
        # å†»ç»“å‚æ•°
        for param in vgg.parameters():
            param.requires_grad = False
        return vgg
    
    def _extract_features(self, x, model, layers):
        """ä»VGGæ¨¡å‹ä¸­æå–ç‰¹å¾"""
        features = {}
        for name, layer in model._modules.items():
            x = layer(x)
            if name in layers:
                features[name] = x
        return features
    
    def _gram_matrix(self, x):
        """è®¡ç®—GramçŸ©é˜µï¼ˆé£æ ¼ç‰¹å¾ï¼‰"""
        batch_size, channels, height, width = x.size()
        features = x.view(batch_size * channels, height * width)
        gram = torch.mm(features, features.t())
        return gram.div(batch_size * channels * height * width)
    
    def _load_ghibli_style_images(self):
        """åŠ è½½å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡"""
        # ä¼˜å…ˆä½¿ç”¨é¡¹ç›®å†…çš„é£æ ¼å‚è€ƒåº“ï¼Œå…¶æ¬¡å›é€€åˆ° temp ç›®å½•
        candidate_folders = ['ghibli_images', 'temp']
        style_folder = None
        for folder in candidate_folders:
            if os.path.isdir(folder) and any([f.lower().endswith(('.jpg','.jpeg','.png','.bmp')) for f in os.listdir(folder)]):
                style_folder = folder
                break
        if style_folder is None:
            style_folder = 'ghibli_images'  # ä»ç„¶æŒ‰è¯¥è·¯å¾„ç»„åˆé€šé…ç¬¦ï¼Œä¾¿äºä¸‹é¢ glob å¤±è´¥åç»™å‡ºæç¤º
        style_images = []
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        
        for ext in image_extensions:
            import glob
            pattern = os.path.join(style_folder, ext)
            style_images.extend(glob.glob(pattern))
        
        if not style_images:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤é£æ ¼")
            return None
        
        print(f"ğŸ¨ åŠ è½½äº† {len(style_images)} å¼ å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡")
        return style_images
    
    def _create_ghibli_style_tensor(self, target_size=512):
        """åˆ›å»ºå®«å´éªé£æ ¼ç‰¹å¾å¼ é‡"""
        style_images = self._load_ghibli_style_images()
        
        if not style_images:
            # å¦‚æœæ²¡æœ‰å‚è€ƒå›¾ç‰‡ï¼Œåˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾
            return self._create_default_ghibli_style(target_size)
        
        # åŠ è½½å¹¶å¤„ç†é£æ ¼å›¾ç‰‡
        style_tensors = []
        transform = transforms.Compose([
            transforms.Resize(target_size),
            transforms.CenterCrop(target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        for style_path in style_images:
            try:
                style_img = Image.open(style_path).convert('RGB')
                style_tensor = transform(style_img).unsqueeze(0).to(self.device)
                style_tensors.append(style_tensor)
            except Exception as e:
                print(f"âŒ åŠ è½½é£æ ¼å›¾ç‰‡ {style_path} å¤±è´¥: {e}")
        
        if not style_tensors:
            return self._create_default_ghibli_style(target_size)
        
        # å¹³å‡æ‰€æœ‰é£æ ¼å›¾ç‰‡çš„ç‰¹å¾
        style_features = {}
        for style_tensor in style_tensors:
            features = self._extract_features(style_tensor, self.vgg, self.style_layers)
            for layer, feature in features.items():
                if layer not in style_features:
                    style_features[layer] = []
                style_features[layer].append(self._gram_matrix(feature))
        
        # è®¡ç®—å¹³å‡é£æ ¼ç‰¹å¾
        avg_style_features = {}
        for layer, gram_list in style_features.items():
            avg_gram = torch.stack(gram_list).mean(dim=0)
            avg_style_features[layer] = avg_gram
        
        return avg_style_features
    
    def _create_default_ghibli_style(self, target_size):
        """åˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾"""
        print("ğŸ¨ ä½¿ç”¨é»˜è®¤å®«å´éªé£æ ¼ç‰¹å¾")
        
        # åˆ›å»ºå…·æœ‰å®«å´éªé£æ ¼ç‰¹å¾çš„é»˜è®¤é£æ ¼
        # å®«å´éªé£æ ¼ç‰¹ç‚¹ï¼šæŸ”å’Œè‰²å½©ã€æ¢¦å¹»å…‰å½±ã€ç®€æ´çº¿æ¡
        default_style = {}
        
        # è¿™é‡Œåº”è¯¥åŸºäºå®«å´éªçš„è‰ºæœ¯ç‰¹ç‚¹åˆ›å»ºé£æ ¼ç‰¹å¾
        # ç”±äºæ—¶é—´å…³ç³»ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
        
        return default_style
    
    def _get_features(self, x):
        """è·å–VGGç‰¹å¾"""
        return self._extract_features(x, self.vgg, self.style_layers + self.content_layers)
    
    def _get_ghibli_style_features(self):
        """è·å–å®«å´éªé£æ ¼ç‰¹å¾"""
        return self._create_ghibli_style_tensor()
    
    def _is_result_poor(self, result_image):
        """æ£€æŸ¥ç»“æœæ˜¯å¦è´¨é‡å·®"""
        if result_image is None:
            return True
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦ä¸ºç°åº¦æˆ–è‰²å½©ä¸¢å¤±
        if isinstance(result_image, Image.Image):
            # è½¬æ¢ä¸ºnumpyæ•°ç»„æ£€æŸ¥
            img_array = np.array(result_image)
            if len(img_array.shape) == 2:  # ç°åº¦å›¾
                return True
            
            # æ£€æŸ¥è‰²å½©é¥±å’Œåº¦
            hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
            saturation = np.mean(hsv[:,:,1])
            if saturation < 30:  # é¥±å’Œåº¦å¤ªä½
                return True
        
        return False
    
    def _preprocess_image_preserve_size(self, image):
        """é¢„å¤„ç†å›¾åƒä½†ä¿æŒåŸå§‹å°ºå¯¸"""
        # è·å–åŸå§‹å°ºå¯¸
        original_size = image.size
        
        # é™åˆ¶æœ€å¤§å°ºå¯¸ä½†ä¿æŒå®½é«˜æ¯”
        max_size = 800
        if max(original_size) > max_size:
            scale = max_size / max(original_size)
            new_size = (int(original_size[0] * scale), int(original_size[1] * scale))
            image = image.resize(new_size, Image.LANCZOS)
        
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).unsqueeze(0)
    
    def _postprocess_image_preserve_size(self, tensor, original_size):
        """åå¤„ç†å¼ é‡ä¸ºå›¾åƒå¹¶æ¢å¤åŸå§‹å°ºå¯¸"""
        # åå½’ä¸€åŒ–
        tensor = tensor.squeeze(0).cpu()
        tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        tensor = torch.clamp(tensor, 0, 1)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        transform = transforms.ToPILImage()
        image = transform(tensor)
        
        # æ¢å¤åŸå§‹å°ºå¯¸
        image = image.resize(original_size, Image.LANCZOS)
        
        return image
    
    def _anime_style_filter(self, img_bgr):
        """åŠ¨æ¼«é£æ ¼æ»¤é•œ"""
        # 1. åŒè¾¹æ»¤æ³¢ - ä¿ç•™è¾¹ç¼˜çš„åŒæ—¶å¹³æ»‘å›¾åƒ
        filtered = cv2.bilateralFilter(img_bgr, d=9, sigmaColor=75, sigmaSpace=75)
        
        # 2. è¾¹ç¼˜æ£€æµ‹å’Œå¢å¼º
        gray = cv2.cvtColor(filtered, cv2.COLOR_BGR2GRAY)
        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                    cv2.THRESH_BINARY, 9, 2)
        
        # 3. é¢œè‰²é‡åŒ– - å‡å°‘é¢œè‰²æ•°é‡ï¼Œåˆ›é€ åŠ¨æ¼«æ•ˆæœ
        Z = filtered.reshape((-1, 3))
        Z = np.float32(Z)
        
        # å®šä¹‰K-meanså‚æ•°
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        K = 16  # é¢œè‰²æ•°é‡
        
        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        res = centers[labels.flatten()]
        cartoon = res.reshape((filtered.shape))
        
        return cartoon
    
    def _apply_ghibli_color_style(self, img_bgr):
        """åº”ç”¨å®«å´éªè‰²å½©é£æ ¼"""
        # å®«å´éªé£æ ¼è‰²å½©ç‰¹ç‚¹ï¼šæŸ”å’Œã€æ¸©æš–ã€é«˜é¥±å’Œåº¦
        
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç¡®çš„è°ƒæ•´
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦ï¼ˆå®«å´éªé£æ ¼è‰²å½©é²œè‰³ï¼‰
        s = cv2.add(s, 30)
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒ
        h = cv2.add(h, 5)  # è½»å¾®åå‘æ©™è‰²/é»„è‰²
        h = np.clip(h, 0, 179)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        v = cv2.add(v, 10)
        v = np.clip(v, 0, 255)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        
        # è½¬æ¢å›BGR
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # åº”ç”¨æŸ”å’Œæ»¤é•œ
        soft = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        # æ··åˆåŸå§‹å’ŒæŸ”å’Œç‰ˆæœ¬
        result = cv2.addWeighted(enhanced, 0.7, soft, 0.3, 0)
        
        return result
    
    def _add_dreamy_lighting(self, img_bgr):
        """æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ"""
        h, w = img_bgr.shape[:2]
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½© - ä¸­å¿ƒæ˜äº®ï¼Œè¾¹ç¼˜æŸ”å’Œ
        light_mask = 1.0 - (distance / max_distance) * 0.1
        light_mask = np.clip(light_mask, 0.9, 1.0)
        
        # åº”ç”¨å…‰ç…§æ•ˆæœ
        final = img_bgr.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        return final
    
    def apply_real_ghibli_style(self, content_image, num_steps=80, style_weight=300000, content_weight=1, use_neural=True):
        """åº”ç”¨çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - ä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            content_image: å†…å®¹å›¾åƒ
            num_steps: è¿­ä»£æ­¥æ•°
            style_weight: é£æ ¼æƒé‡
            content_weight: å†…å®¹æƒé‡
            use_neural: æ˜¯å¦ä½¿ç”¨ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»
        """
        print("ğŸ¨ å¼€å§‹åº”ç”¨å®«å´éªé£æ ¼...")
        
        try:
            # ä¼˜å…ˆä½¿ç”¨ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»
            if use_neural and self.use_neural_network and self.neural_model is not None:
                print("ğŸ§  ä½¿ç”¨ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»")
                
                # è®¾ç½®è¿›åº¦å›è°ƒ
                if self.progress_callback and self.task_id:
                    def neural_progress_callback(progress, current_step, total_steps, loss):
                        self.progress_callback(self.task_id, progress, current_step, total_steps, loss)
                    
                    self.neural_model.set_progress_callback(neural_progress_callback)
                
                # ä½¿ç”¨ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»
                # ä¸¤é˜¶æ®µç­–ç•¥ï¼šå°å›¾æ”¶æ•› + å¤§å›¾å¾®è°ƒ
                # é˜¶æ®µ1ï¼šè¾ƒå°åˆ†è¾¨ç‡ï¼Œåé£æ ¼
                result = self.neural_model.transfer_style(
                    content_image,
                    style_weight=int(style_weight*0.7),
                    content_weight=int(max(1, content_weight*0.8)),
                    num_steps=int(num_steps*0.7),
                    learning_rate=0.025,
                    tv_weight=1e-5
                )
                # é˜¶æ®µ2ï¼šå…¨å°ºå¯¸è½»å¾®è°ƒï¼ˆæ›´ä¿ç»“æ„ï¼‰
                result = self.neural_model.transfer_style(
                    result,
                    style_weight=int(style_weight*0.3),
                    content_weight=int(max(1, content_weight*2)),
                    num_steps=max(20, int(num_steps*0.3)),
                    learning_rate=0.02,
                    tv_weight=2e-5
                )
                
                if result is not None and not self._is_result_poor(result):
                    print("âœ… ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æˆåŠŸ")
                    return result
                else:
                    print("âš ï¸ ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ•ˆæœä¸ä½³ï¼Œå°è¯•ä¼ ç»Ÿæ–¹æ³•")
            
            # ä½¿ç”¨ä¼ ç»Ÿæ·±åº¦å­¦ä¹ é£æ ¼è¿ç§»
            print("ğŸ”§ ä½¿ç”¨ä¼ ç»Ÿæ·±åº¦å­¦ä¹ é£æ ¼è¿ç§»")
            result = self._apply_neural_style_transfer(content_image, num_steps, style_weight, content_weight)
            
            # å¦‚æœæ·±åº¦å­¦ä¹ æ•ˆæœä¸å¥½ï¼Œä½¿ç”¨è®¡ç®—æœºè§†è§‰ä¼˜åŒ–
            if result is None or self._is_result_poor(result):
                print("âš ï¸ æ·±åº¦å­¦ä¹ æ•ˆæœä¸ä½³ï¼Œä½¿ç”¨è®¡ç®—æœºè§†è§‰ä¼˜åŒ–")
                result = self._apply_cv_optimized_ghibli_style(content_image)
            
            return result
            
        except Exception as e:
            print(f"âŒ é£æ ¼è½¬æ¢å¤±è´¥: {e}")
            return self._apply_cv_optimized_ghibli_style(content_image)
    
    def _apply_neural_style_transfer(self, content_image, num_steps, style_weight, content_weight):
        """åº”ç”¨ç¥ç»é£æ ¼è¿ç§»"""
        # é¢„å¤„ç†å†…å®¹å›¾åƒ - ä¿æŒåŸå§‹å°ºå¯¸
        original_size = content_image.size
        content_tensor = self._preprocess_image_preserve_size(content_image).to(self.device)
        
        # ä½¿ç”¨å†…å®¹å›¾åƒä½œä¸ºåˆå§‹è¾“å…¥
        input_img = content_tensor.clone().requires_grad_(True)
        
        # è·å–å†…å®¹ç‰¹å¾
        content_features = self._get_features(content_tensor)
        
        # ä½¿ç”¨å®«å´éªé£æ ¼å‚è€ƒå›¾åƒ
        style_features = self._get_ghibli_style_features()
        
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæ›´ç¨³å®š
        optimizer = optim.Adam([input_img], lr=0.02)
        
        for step in range(num_steps):
            optimizer.zero_grad()
            
            # è·å–å½“å‰è¾“å…¥çš„ç‰¹å¾
            features = self._get_features(input_img)
            
            style_loss = 0
            content_loss = 0
            
            # é£æ ¼æŸå¤±
            for layer in self.style_layers:
                if layer in features and layer in style_features:
                    target_style = style_features[layer]
                    current_style = features[layer]
                    
                    # è®¡ç®—GramçŸ©é˜µ
                    target_gram = self._gram_matrix(target_style)
                    current_gram = self._gram_matrix(current_style)
                    
                    style_loss += F.mse_loss(current_gram, target_gram)
            
            # å†…å®¹æŸå¤±
            for layer in self.content_layers:
                if layer in features:
                    target_content = content_features[layer]
                    current_content = features[layer]
                    content_loss += F.mse_loss(current_content, target_content)
            
            # æ€»æŸå¤±
            total_loss = style_weight * style_loss + content_weight * content_loss
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºnanæˆ–inf
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"âš ï¸ æ­¥éª¤ {step+1}: æŸå¤±å€¼å¼‚å¸¸ (NaN/Inf)ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                continue
            
            total_loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºnanæˆ–inf
            if torch.isnan(input_img.grad).any() or torch.isinf(input_img.grad).any():
                print(f"âš ï¸ æ­¥éª¤ {step+1}: æ¢¯åº¦å¼‚å¸¸ (NaN/Inf)ï¼Œé‡ç½®æ¢¯åº¦")
                optimizer.zero_grad()
                continue
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_([input_img], max_norm=0.5)
            
            optimizer.step()
            
            # æ›´æ–°è¿›åº¦
            progress = int((step + 1) / num_steps * 100)
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, progress, step + 1, num_steps, total_loss.item())
            
            if (step + 1) % 30 == 0:
                print(f"æ­¥éª¤ {step+1}/{num_steps}, æ€»æŸå¤±: {total_loss.item():.4f}")
        
        # åå¤„ç†è¾“å‡ºå›¾åƒ - æ¢å¤åŸå§‹å°ºå¯¸
        output_tensor = input_img.data.clamp(0, 1)
        result_image = self._postprocess_image_preserve_size(output_tensor, original_size)
        
        return result_image
    
    def _apply_cv_optimized_ghibli_style(self, image):
        """åº”ç”¨è®¡ç®—æœºè§†è§‰ä¼˜åŒ–çš„å®«å´éªé£æ ¼ - å‰æ™¯/èƒŒæ™¯åˆ†æ”¯ + çº¿ç¨¿å¢å¼º"""
        print("ğŸ¨ ä½¿ç”¨è®¡ç®—æœºè§†è§‰ä¼˜åŒ–å®«å´éªé£æ ¼...")
        
        img_np = np.array(image)
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR) if img_np.ndim==3 and img_np.shape[2]==3 else cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        h, w = img_bgr.shape[:2]
        
        # å°ºå¯¸é™åˆ¶
        max_size = 2048
        if max(h,w) > max_size:
            scale = max_size / max(h,w)
            img_bgr = cv2.resize(img_bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_LANCZOS4)
            h, w = img_bgr.shape[:2]
            print(f"ğŸ“ è®¡ç®—æœºè§†è§‰å¤„ç†: å›¾ç‰‡å°ºå¯¸è¿‡å¤§ï¼Œè‡ªåŠ¨ç¼©æ”¾è‡³: {w}x{h}")
        else:
            print(f"ğŸ“ è®¡ç®—æœºè§†è§‰å¤„ç†: ä¿æŒåŸå§‹å°ºå¯¸: {w}x{h}")
        
        # å‰æ™¯åˆ†å‰²
        mask = self._get_person_mask(img_bgr)  # 0~255
        inv_mask = 255 - mask
        
        # èƒŒæ™¯å¼ºåŠ¨æ¼«åŒ–
        bg = self._advanced_anime_style_filter(img_bgr)
        bg = self._enhanced_ghibli_color_style(bg)
        bg = self._enhanced_dreamy_lighting(bg)
        
        # å‰æ™¯å¼±å¹³æ»‘ + çº¿ç¨¿å¢å¼º + è½»è°ƒè‰²
        fg = cv2.bilateralFilter(img_bgr, 7, 60, 60)
        gray = cv2.cvtColor(fg, cv2.COLOR_BGR2GRAY)
        edges = self._xdog_edges(gray)
        edges_col = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        # çº¿ç¨¿ä»¥ä½é€æ˜åº¦å åŠ 
        fg = cv2.addWeighted(fg, 1.0, edges_col, 0.08, 0)
        fg = self._enhanced_ghibli_color_style(fg)
        
        # èåˆ
        mask_s = cv2.GaussianBlur(mask, (11,11), 0)
        comp = self._alpha_blend(fg, bg, mask_s)
        
        # æœ€ç»ˆç»†èŠ‚
        comp = self._final_touch_optimization(comp)
        result_rgb = cv2.cvtColor(comp, cv2.COLOR_BGR2RGB)
        return Image.fromarray(result_rgb)
    
    def _advanced_anime_style_filter(self, img_bgr):
        """
        é«˜çº§åŠ¨æ¼«é£æ ¼æ»¤é•œ - æ›´æ¥è¿‘å®«å´éªé£æ ¼
        - å¤šæ¬¡è¾¹ç¼˜ä¿ç•™å¹³æ»‘ï¼Œç§»é™¤å†™å®çº¹ç†
        - é¢œè‰²å¤§å—åŒ–ï¼ˆKMeans + SLICè¶…åƒç´ å‡å€¼åŒ–ï¼‰
        - æŸ”å’Œç»†çº¿ç¨¿å åŠ 
        """
        # 1) è¾¹ç¼˜ä¿ç•™å¹³æ»‘ï¼ˆä¸¤æ¬¡åŒè¾¹æ»¤æ³¢ï¼‰
        guided = cv2.bilateralFilter(img_bgr, d=11, sigmaColor=85, sigmaSpace=85)
        guided = cv2.bilateralFilter(guided, d=9, sigmaColor=75, sigmaSpace=75)

        # 2) æ™ºèƒ½è¾¹ç¼˜æ£€æµ‹ + è½»å¾®è†¨èƒ€
        gray = cv2.cvtColor(guided, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 40, 120)
        kernel = np.ones((2, 2), np.uint8)
        edges = cv2.dilate(edges, kernel, iterations=1)

        # 3) é¢œè‰²é‡åŒ–ï¼ˆæ›´å°‘çš„é¢œè‰²ä»¥è·å¾—å¡é€šåˆ†åŒºï¼‰
        Z = guided.reshape((-1, 3)).astype(np.float32)
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 1.0)
        K = 12
        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        cartoon = centers[labels.flatten()].reshape(guided.shape)

        # 4) SLICè¶…åƒç´ å‡å€¼åŒ–ï¼Œè¿›ä¸€æ­¥å¤§ç‰‡åŒºæ‰å¹³ï¼ˆæ›´â€œåŠ¨æ¼«â€ï¼‰
        try:
            from skimage.segmentation import slic
            from skimage.color import label2rgb
            img_rgb = cv2.cvtColor(cartoon, cv2.COLOR_BGR2RGB)
            segments = slic(img_rgb, n_segments=600, compactness=20, sigma=0, start_label=1)
            flat_rgb = (label2rgb(segments, img_rgb, kind='avg') * 255).astype(np.uint8)
            cartoon = cv2.cvtColor(flat_rgb, cv2.COLOR_RGB2BGR)
        except Exception:
            pass

        # 5) è¾¹ç¼˜å åŠ ï¼ˆæŸ”å’Œçº¿ç¨¿ï¼‰
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR).astype(np.float32) / 255.0
        cartoon = cartoon.astype(np.float32) / 255.0
        result = cv2.addWeighted(cartoon, 0.85, edges_colored, 0.15, 0)
        result = (result * 255).astype(np.uint8)
        return result
    
    def _enhanced_ghibli_color_style(self, img_bgr):
        """å¢å¼ºçš„å®«å´éªè‰²å½©é£æ ¼"""
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç¡®çš„è°ƒæ•´
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦ï¼ˆå®«å´éªé£æ ¼è‰²å½©é²œè‰³ï¼‰
        s = cv2.add(s, 40)  # å¢åŠ é¥±å’Œåº¦
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒï¼ˆå®«å´éªé£æ ¼ç‰¹ç‚¹ï¼‰
        h = cv2.add(h, 8)  # è½»å¾®åå‘æ©™è‰²/é»„è‰²
        h = np.clip(h, 0, 179)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        v = cv2.add(v, 15)
        v = np.clip(v, 0, 255)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        
        # è½¬æ¢å›BGR
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # åº”ç”¨æŸ”å’Œæ»¤é•œ
        soft = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        # æ··åˆåŸå§‹å’ŒæŸ”å’Œç‰ˆæœ¬
        result = cv2.addWeighted(enhanced, 0.7, soft, 0.3, 0)
        
        return result
    
    def _enhanced_dreamy_lighting(self, img_bgr):
        """å¢å¼ºçš„æ¢¦å¹»å…‰å½±æ•ˆæœ"""
        h, w = img_bgr.shape[:2]
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½© - ä¸­å¿ƒæ˜äº®ï¼Œè¾¹ç¼˜æŸ”å’Œï¼ˆæ˜¾è‘—å‡å¼±ä»¥é¿å…æ˜æ˜¾åŒå¿ƒç¯å’Œå¸¦çŠ¶ï¼‰
        light_mask = 1.0 - (distance / max_distance) * 0.05
        light_mask = np.clip(light_mask, 0.95, 1.0)
        
        # åº”ç”¨æ›´æŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        final = img_bgr.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        return final
    
    def _final_touch_optimization(self, img_bgr):
        """æœ€ç»ˆç»†èŠ‚ä¼˜åŒ–"""
        # 1. è½»å¾®é”åŒ–å¢å¼ºç»†èŠ‚
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(img_bgr, -1, kernel)
        
        # 2. è½»å¾®é™å™ªï¼ˆé™ä½å¼ºåº¦ä»¥é¿å…è¿‡åº¦å¹³æ»‘ï¼‰
        denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 3, 3, 7, 21)
        
        # 3. æœ€ç»ˆè‰²å½©å¹³è¡¡
        lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºäº®åº¦
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_balanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        return final
    
    def set_progress_callback(self, callback, task_id):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
        self.task_id = task_id
    
    def _get_person_mask(self, img_bgr):
        """è·å–äººç‰©å‰æ™¯æ©è†œï¼ˆ0~255ï¼‰"""
        try:
            if self.seg_model is None:
                from torchvision import models
                self.seg_model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT).eval()
            import torchvision.transforms as T
            transform = T.Compose([
                T.ToPILImage(),
                T.Resize(520),
                T.ToTensor(),
                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            x = transform(rgb).unsqueeze(0)
            with torch.no_grad():
                out = self.seg_model(x)['out'][0]  # [21, H, W]
            person_class = 15  # COCO person
            mask = out.argmax(0).byte().cpu().numpy()
            mask = (mask == person_class).astype(np.uint8) * 255
            # è°ƒæ•´åˆ°åŸå›¾å°ºå¯¸
            mask = cv2.resize(mask, (img_bgr.shape[1], img_bgr.shape[0]), interpolation=cv2.INTER_NEAREST)
            # å¹³æ»‘è¾¹ç¼˜
            mask = cv2.GaussianBlur(mask, (9,9), 0)
            return mask
        except Exception as e:
            print(f"âš ï¸ å‰æ™¯åˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨å…¨å›¾: {e}")
            return np.ones(img_bgr.shape[:2], dtype=np.uint8) * 255
    
    def _xdog_edges(self, gray, k=4.5, sigma=0.9, epsilon=-0.1, phi=10):
        """XDoGé£æ ¼çº¿ç¨¿ï¼Œè¿”å›0~255"""
        g1 = cv2.GaussianBlur(gray, (0,0), sigma)
        g2 = cv2.GaussianBlur(gray, (0,0), sigma*k)
        D = g1 - g2
        D = D / (np.max(np.abs(D)) + 1e-8)
        E = np.ones_like(D)
        E[D < epsilon] = 1 + np.tanh(phi*(D[D < epsilon]-epsilon))
        E[D >= epsilon] = 1
        E = (E*255).astype(np.uint8)
        return 255 - E
    
    def _alpha_blend(self, fg, bg, mask):
        mask_f = (mask.astype(np.float32)/255.0)[:,:,None]
        return (fg*mask_f + bg*(1-mask_f)).astype(np.uint8)
    
    def _preprocess_image(self, image):
        """é¢„å¤„ç†å›¾åƒ"""
        transform = transforms.Compose([
            transforms.Resize(512),
            transforms.CenterCrop(512),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).unsqueeze(0)
    
    def _postprocess_image(self, tensor):
        """åå¤„ç†å¼ é‡ä¸ºå›¾åƒ"""
        # åå½’ä¸€åŒ–
        tensor = tensor.squeeze(0).cpu()
        tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        tensor = torch.clamp(tensor, 0, 1)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        transform = transforms.ToPILImage()
        image = transform(tensor)
        
        return image
    
    def _fallback_traditional_method(self, image):
        """å¤‡é€‰ä¼ ç»Ÿæ–¹æ³•"""
        print("âš ï¸ ä½¿ç”¨å¤‡é€‰ä¼ ç»Ÿæ–¹æ³•")
        
        # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
        img_np = np.array(image)
        
        # è½¬æ¢ä¸ºBGRæ ¼å¼
        if len(img_np.shape) == 3 and img_np.shape[2] == 3:
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        else:
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        
        # é«˜è´¨é‡çš„å®«å´éªé£æ ¼å¤„ç†
        
        # 1. ä¿æŒåŸå§‹åˆ†è¾¨ç‡
        h, w = img_bgr.shape[:2]
        max_size = 2000
        if max(h, w) > max_size:
            scale = max_size / max(h, w)
            new_w, new_h = int(w * scale), int(h * scale)
            img_bgr = cv2.resize(img_bgr, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
        
        # 2. æ™ºèƒ½è¾¹ç¼˜ä¿ç•™ï¼ˆé‡ç‚¹æ”¹è¿›äººç‰©åŒºåŸŸï¼‰
        # ä½¿ç”¨åŒè¾¹æ»¤æ³¢æ›¿ä»£å¯¼å‘æ»¤æ³¢
        guided = cv2.bilateralFilter(img_bgr, d=11, sigmaColor=80, sigmaSpace=80)
        
        # 3. å®«å´éªé£æ ¼è‰²å½©è°ƒæ•´
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç¡®çš„è‰²å½©è°ƒæ•´
        lab = cv2.cvtColor(guided, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºè‰²å½©é²œè‰³åº¦ï¼ˆå®«å´éªé£æ ¼ç‰¹ç‚¹ï¼‰
        a = cv2.addWeighted(a, 1.2, a, 0, 0)
        b = cv2.addWeighted(b, 1.2, b, 0, 0)
        
        # è°ƒæ•´äº®åº¦å’Œå¯¹æ¯”åº¦
        l = cv2.createCLAHE(clipLimit=2.0).apply(l)
        
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # 4. æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ
        h, w = enhanced.shape[:2]
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½©
        light_mask = 1.0 - (distance / max_distance) * 0.15
        light_mask = np.clip(light_mask, 0.85, 1.0)
        
        # åº”ç”¨å…‰ç…§æ•ˆæœ
        final = enhanced.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        # è½¬æ¢å›RGB
        result_rgb = cv2.cvtColor(final, cv2.COLOR_BGR2RGB)
        
        return result_rgb

# åˆ›å»ºçœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢æ¨¡å‹
real_ghibli_model = RealGhibliStyleTransfer()

def update_progress(task_id, progress, current_step, total_steps, loss):
    """æ›´æ–°è½¬æ¢è¿›åº¦"""
    # ç¡®ä¿è¿›åº¦å’Œæ­¥éª¤ä¿¡æ¯ä¸€è‡´
    if progress > 0 and current_step == 0:
        # å¦‚æœè¿›åº¦æœ‰å€¼ä½†æ­¥éª¤ä¸º0ï¼Œæ ¹æ®è¿›åº¦è®¡ç®—æ­¥éª¤
        current_step = max(1, int(progress / 100 * total_steps))
    elif current_step > 0 and progress == 0:
        # å¦‚æœæ­¥éª¤æœ‰å€¼ä½†è¿›åº¦ä¸º0ï¼Œæ ¹æ®æ­¥éª¤è®¡ç®—è¿›åº¦
        progress = min(99, int(current_step / total_steps * 100))
    
    conversion_progress[task_id] = {
        'progress': progress,
        'current_step': current_step,
        'total_steps': total_steps,
        'loss': loss,
        'timestamp': time.time()
    }
    print(f"ğŸ“Š ä»»åŠ¡ {task_id}: {progress}% (æ­¥éª¤ {current_step}/{total_steps}, æŸå¤±: {loss:.4f})")

def convert_image_async(task_id, image):
    """å¼‚æ­¥è½¬æ¢å›¾åƒ"""
    try:
        # è®¾ç½®è¿›åº¦å›è°ƒ
        real_ghibli_model.set_progress_callback(update_progress, task_id)
        
        # å¼€å§‹è½¬æ¢
        result_image = real_ghibli_model.apply_real_ghibli_style(image, num_steps=100)
        
        # ä¿å­˜ç»“æœ
        conversion_results[task_id] = {
            'success': True,
            'result_image': result_image,
            'completed': True
        }
        
        # æ›´æ–°è¿›åº¦ä¸ºå®Œæˆ
        update_progress(task_id, 100, 100, 100, 0)
        
    except Exception as e:
        conversion_results[task_id] = {
            'success': False,
            'error': str(e),
            'completed': True
        }
        print(f"âŒ ä»»åŠ¡ {task_id} è½¬æ¢å¤±è´¥: {e}")

