#!/usr/bin/env python3
"""
çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - åŸºäºæ·±åº¦å­¦ä¹ å’Œé£æ ¼è¿ç§»
é›†æˆé¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
"""

import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import torch.optim as optim
from torch.autograd import Variable
import os
import io
import base64
import time
import threading

# å¯¼å…¥ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ¨¡å—
try:
    from .neural_style_transfer import NeuralStyleTransfer, style_transfer_manager
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
    NeuralStyleTransfer = None
    style_transfer_manager = None

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨è½¬æ¢è¿›åº¦
conversion_progress = {}
conversion_results = {}

class RealGhibliStyleTransfer:
    """çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - åŸºäºæ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰ä¼˜åŒ–
    é›†æˆé¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
    """
    
    def __init__(self, use_neural_network=True):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.vgg = self._load_vgg().to(self.device)
        self.style_layers = ['3', '8', '15', '22']  # VGGå±‚ç”¨äºé£æ ¼æå–
        self.content_layers = ['22']  # VGGå±‚ç”¨äºå†…å®¹æå–
        self.progress_callback = None
        self.task_id = None
        
        # ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ¨¡å‹
        self.use_neural_network = use_neural_network
        self.neural_model = None
        
        # è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.seg_model = None
        
        if use_neural_network and NeuralStyleTransfer is not None:
            try:
                self.neural_model = NeuralStyleTransfer(model_type='vgg19')
                print("âœ… ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ç¥ç»ç½‘ç»œæ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                self.use_neural_network = False
        
        # åˆå§‹åŒ–è‡ªä¸»å­¦ä¹ å™¨
        self.auto_learner = None  # æš‚æ—¶ç¦ç”¨è‡ªä¸»å­¦ä¹ åŠŸèƒ½
        
        # æ˜¯å¦å¯ç”¨è‡ªä¸»å­¦ä¹ 
        self.enable_auto_learning = False
        
    def _load_vgg(self):
        """åŠ è½½é¢„è®­ç»ƒçš„VGG19æ¨¡å‹"""
        try:
            # ä½¿ç”¨æ–°çš„APIåŠ è½½VGG19æ¨¡å‹
            vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features
        except AttributeError:
            # å›é€€åˆ°æ—§ç‰ˆæœ¬API
            vgg = models.vgg19(pretrained=True).features
        
        # å†»ç»“å‚æ•°
        for param in vgg.parameters():
            param.requires_grad = False
        return vgg
    
    def _extract_features(self, x, model, layers):
        """ä»VGGæ¨¡å‹ä¸­æå–ç‰¹å¾"""
        features = {}
        for name, layer in model._modules.items():
            x = layer(x)
            if name in layers:
                features[name] = x
        return features
    
    def _gram_matrix(self, x):
        """è®¡ç®—GramçŸ©é˜µï¼ˆé£æ ¼ç‰¹å¾ï¼‰"""
        batch_size, channels, height, width = x.size()
        features = x.view(batch_size * channels, height * width)
        gram = torch.mm(features, features.t())
        return gram.div(batch_size * channels * height * width)
    
    def _load_ghibli_style_images(self):
        """åŠ è½½å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡"""
        # ä¼˜å…ˆä½¿ç”¨é¡¹ç›®å†…çš„é£æ ¼å‚è€ƒåº“ï¼Œå…¶æ¬¡å›é€€åˆ° temp ç›®å½•
        candidate_folders = ['ghibli_images', 'temp']
        style_folder = None
        for folder in candidate_folders:
            if os.path.isdir(folder) and any([f.lower().endswith(('.jpg','.jpeg','.png','.bmp')) for f in os.listdir(folder)]):
                style_folder = folder
                break
        if style_folder is None:
            style_folder = 'ghibli_images'  # ä»ç„¶æŒ‰è¯¥è·¯å¾„ç»„åˆé€šé…ç¬¦ï¼Œä¾¿äºä¸‹é¢ glob å¤±è´¥åç»™å‡ºæç¤º
        style_images = []
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        
        for ext in image_extensions:
            import glob
            pattern = os.path.join(style_folder, ext)
            style_images.extend(glob.glob(pattern))
        
        if not style_images:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤é£æ ¼")
            return None
        
        print(f"ğŸ¨ åŠ è½½äº† {len(style_images)} å¼ å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡")
        return style_images
    
    def _create_ghibli_style_tensor(self, target_size=512):
        """åˆ›å»ºå®«å´éªé£æ ¼ç‰¹å¾å¼ é‡"""
        style_images = self._load_ghibli_style_images()
        
        if not style_images:
            # å¦‚æœæ²¡æœ‰å‚è€ƒå›¾ç‰‡ï¼Œåˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾
            return self._create_default_ghibli_style(target_size)
        
        # åŠ è½½å¹¶å¤„ç†é£æ ¼å›¾ç‰‡
        style_tensors = []
        transform = transforms.Compose([
            transforms.Resize(target_size),
            transforms.CenterCrop(target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        for style_path in style_images:
            try:
                style_img = Image.open(style_path).convert('RGB')
                style_tensor = transform(style_img).unsqueeze(0).to(self.device)
                style_tensors.append(style_tensor)
            except Exception as e:
                print(f"âŒ åŠ è½½é£æ ¼å›¾ç‰‡ {style_path} å¤±è´¥: {e}")
        
        if not style_tensors:
            return self._create_default_ghibli_style(target_size)
        
        # å¹³å‡æ‰€æœ‰é£æ ¼å›¾ç‰‡çš„ç‰¹å¾
        style_features = {}
        for style_tensor in style_tensors:
            features = self._extract_features(style_tensor, self.vgg, self.style_layers)
            for layer, feature in features.items():
                if layer not in style_features:
                    style_features[layer] = []
                style_features[layer].append(self._gram_matrix(feature))
        
        # è®¡ç®—å¹³å‡é£æ ¼ç‰¹å¾
        avg_style_features = {}
        for layer, gram_list in style_features.items():
            avg_gram = torch.stack(gram_list).mean(dim=0)
            avg_style_features[layer] = avg_gram
        
        return avg_style_features
    
    def _create_default_ghibli_style(self, target_size):
        """åˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾"""
        print("ğŸ¨ ä½¿ç”¨é»˜è®¤å®«å´éªé£æ ¼ç‰¹å¾")
        
        # åˆ›å»ºå…·æœ‰å®«å´éªé£æ ¼ç‰¹å¾çš„é»˜è®¤é£æ ¼
        # å®«å´éªé£æ ¼ç‰¹ç‚¹ï¼šæŸ”å’Œè‰²å½©ã€æ¢¦å¹»å…‰å½±ã€ç®€æ´çº¿æ¡
        default_style = {}
        
        # è¿™é‡Œåº”è¯¥åŸºäºå®«å´éªçš„è‰ºæœ¯ç‰¹ç‚¹åˆ›å»ºé£æ ¼ç‰¹å¾
        # ç”±äºæ—¶é—´å…³ç³»ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
        
        return default_style
    
    def _get_features(self, x):
        """è·å–VGGç‰¹å¾"""
        return self._extract_features(x, self.vgg, self.style_layers + self.content_layers)
    
    def _get_ghibli_style_features(self):
        """è·å–å®«å´éªé£æ ¼ç‰¹å¾"""
        return self._create_ghibli_style_tensor()
    
    def _is_result_poor(self, result_image):
        """æ£€æŸ¥ç»“æœæ˜¯å¦è´¨é‡å·®"""
        if result_image is None:
            return True
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦ä¸ºç°åº¦æˆ–è‰²å½©ä¸¢å¤±
        if isinstance(result_image, Image.Image):
            # è½¬æ¢ä¸ºnumpyæ•°ç»„æ£€æŸ¥
            img_array = np.array(result_image)
            if len(img_array.shape) == 2:  # ç°åº¦å›¾
                return True
            
            # æ£€æŸ¥è‰²å½©é¥±å’Œåº¦
            hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
            saturation = np.mean(hsv[:,:,1])
            if saturation < 30:  # é¥±å’Œåº¦å¤ªä½
                return True
        
        return False
    
    def _preprocess_image_preserve_size(self, image):
        """é¢„å¤„ç†å›¾åƒä½†ä¿æŒåŸå§‹å°ºå¯¸"""
        # è·å–åŸå§‹å°ºå¯¸
        original_size = image.size
        
        # é™åˆ¶æœ€å¤§å°ºå¯¸ä½†ä¿æŒå®½é«˜æ¯”
        max_size = 800
        if max(original_size) > max_size:
            scale = max_size / max(original_size)
            new_size = (int(original_size[0] * scale), int(original_size[1] * scale))
            image = image.resize(new_size, Image.LANCZOS)
        
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).unsqueeze(0)
    
    def _postprocess_image_preserve_size(self, tensor, original_size):
        """åå¤„ç†å¼ é‡ä¸ºå›¾åƒå¹¶æ¢å¤åŸå§‹å°ºå¯¸"""
        # åå½’ä¸€åŒ–
        tensor = tensor.squeeze(0).cpu()
        tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        tensor = torch.clamp(tensor, 0, 1)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        transform = transforms.ToPILImage()
        image = transform(tensor)
        
        # æ¢å¤åŸå§‹å°ºå¯¸
        image = image.resize(original_size, Image.LANCZOS)
        
        return image
    
    def _anime_style_filter(self, img_bgr):
        """åŠ¨æ¼«é£æ ¼æ»¤é•œ"""
        # 1. åŒè¾¹æ»¤æ³¢ - ä¿ç•™è¾¹ç¼˜çš„åŒæ—¶å¹³æ»‘å›¾åƒ
        filtered = cv2.bilateralFilter(img_bgr, d=9, sigmaColor=75, sigmaSpace=75)
        
        # 2. è¾¹ç¼˜æ£€æµ‹å’Œå¢å¼º
        gray = cv2.cvtColor(filtered, cv2.COLOR_BGR2GRAY)
        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                    cv2.THRESH_BINARY, 9, 2)
        
        # 3. é¢œè‰²é‡åŒ– - å‡å°‘é¢œè‰²æ•°é‡ï¼Œåˆ›é€ åŠ¨æ¼«æ•ˆæœ
        Z = filtered.reshape((-1, 3))
        Z = np.float32(Z)
        
        # å®šä¹‰K-meanså‚æ•°
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        K = 16  # é¢œè‰²æ•°é‡
        
        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        res = centers[labels.flatten()]
        cartoon = res.reshape((filtered.shape))
        
        return cartoon
    
    def _apply_ghibli_color_style(self, img_bgr):
        """åº”ç”¨å®«å´éªè‰²å½©é£æ ¼"""
        # å®«å´éªé£æ ¼è‰²å½©ç‰¹ç‚¹ï¼šæŸ”å’Œã€æ¸©æš–ã€é«˜é¥±å’Œåº¦
        
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç¡®çš„è°ƒæ•´
        # ç¡®ä¿å›¾åƒæ˜¯3é€šé“çš„BGRæ ¼å¼
        if len(img_bgr.shape) == 2:
            img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)
        elif img_bgr.shape[2] == 1:
            img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)
        
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦ï¼ˆå®«å´éªé£æ ¼è‰²å½©é²œè‰³ï¼‰
        s = cv2.add(s, 30)
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒ
        h = cv2.add(h, 5)  # è½»å¾®åå‘æ©™è‰²/é»„è‰²
        h = np.clip(h, 0, 179)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        v = cv2.add(v, 10)
        v = np.clip(v, 0, 255)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        
        # è½¬æ¢å›BGR
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # åº”ç”¨æŸ”å’Œæ»¤é•œ
        soft = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        # æ··åˆåŸå§‹å’ŒæŸ”å’Œç‰ˆæœ¬
        result = cv2.addWeighted(enhanced, 0.7, soft, 0.3, 0)
        
        return result
    
    def _add_dreamy_lighting(self, img_bgr):
        """æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ"""
        h, w = img_bgr.shape[:2]
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½© - ä¸­å¿ƒæ˜äº®ï¼Œè¾¹ç¼˜æŸ”å’Œ
        light_mask = 1.0 - (distance / max_distance) * 0.1
        light_mask = np.clip(light_mask, 0.9, 1.0)
        
        # åº”ç”¨å…‰ç…§æ•ˆæœ
        final = img_bgr.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        return final
    
    def apply_real_ghibli_style(self, content_image, num_steps=80, style_weight=300000, content_weight=1, use_neural=True):
        """åº”ç”¨çœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢ - åŸºäºå®é™…å¯ç”¨çš„ä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            content_image: å†…å®¹å›¾åƒ
            num_steps: è¿­ä»£æ­¥æ•°
            style_weight: é£æ ¼æƒé‡
            content_weight: å†…å®¹æƒé‡
            use_neural: æ˜¯å¦ä½¿ç”¨ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»
        """
        print("ğŸ¨ å¼€å§‹åº”ç”¨å®«å´éªé£æ ¼è½¬æ¢...")
        
        try:
            # ä½¿ç”¨å®é™…å¯ç”¨çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•
            print("ğŸ”§ ä½¿ç”¨ä¼˜åŒ–çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•")
            result = self._apply_optimized_cv_anime_style(content_image)
            
            return result
            
        except Exception as e:
            print(f"âŒ é£æ ¼è½¬æ¢å¤±è´¥: {e}")
            return self._apply_cv_optimized_ghibli_style(content_image)
    
    def _apply_neural_style_transfer(self, content_image, num_steps, style_weight, content_weight):
        """åº”ç”¨ç¥ç»é£æ ¼è¿ç§»"""
        # é¢„å¤„ç†å†…å®¹å›¾åƒ - ä¿æŒåŸå§‹å°ºå¯¸
        original_size = content_image.size
        content_tensor = self._preprocess_image_preserve_size(content_image).to(self.device)
        
        # ä½¿ç”¨å†…å®¹å›¾åƒä½œä¸ºåˆå§‹è¾“å…¥
        input_img = content_tensor.clone().requires_grad_(True)
        
        # è·å–å†…å®¹ç‰¹å¾
        content_features = self._get_features(content_tensor)
        
        # ä½¿ç”¨å®«å´éªé£æ ¼å‚è€ƒå›¾åƒ
        style_features = self._get_ghibli_style_features()
        
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæ›´ç¨³å®š
        optimizer = optim.Adam([input_img], lr=0.02)
        
        # åˆå§‹è¿›åº¦æ›´æ–°
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 5, 0, num_steps, 0)
        
        for step in range(num_steps):
            optimizer.zero_grad()
            
            # è·å–å½“å‰è¾“å…¥çš„ç‰¹å¾
            features = self._get_features(input_img)
            
            style_loss = 0
            content_loss = 0
            
            # é£æ ¼æŸå¤±
            for layer in self.style_layers:
                if layer in features and layer in style_features:
                    target_style = style_features[layer]
                    current_style = features[layer]
                    
                    # è®¡ç®—GramçŸ©é˜µ
                    target_gram = self._gram_matrix(target_style)
                    current_gram = self._gram_matrix(current_style)
                    
                    style_loss += F.mse_loss(current_gram, target_gram)
            
            # å†…å®¹æŸå¤±
            for layer in self.content_layers:
                if layer in features:
                    target_content = content_features[layer]
                    current_content = features[layer]
                    content_loss += F.mse_loss(current_content, target_content)
            
            # æ€»æŸå¤±
            total_loss = style_weight * style_loss + content_weight * content_loss
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºnanæˆ–inf
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"âš ï¸ æ­¥éª¤ {step+1}: æŸå¤±å€¼å¼‚å¸¸ (NaN/Inf)ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                continue
            
            total_loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºnanæˆ–inf
            if torch.isnan(input_img.grad).any() or torch.isinf(input_img.grad).any():
                print(f"âš ï¸ æ­¥éª¤ {step+1}: æ¢¯åº¦å¼‚å¸¸ (NaN/Inf)ï¼Œé‡ç½®æ¢¯åº¦")
                optimizer.zero_grad()
                continue
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_([input_img], max_norm=0.5)
            
            optimizer.step()
            
            # æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°ï¼ˆæ¯5æ­¥æ›´æ–°ä¸€æ¬¡ï¼‰
            if (step + 1) % 5 == 0 or step == num_steps - 1:
                progress = int((step + 1) / num_steps * 100)
                if self.progress_callback and self.task_id:
                    self.progress_callback(self.task_id, progress, step + 1, num_steps, total_loss.item())
            
            if (step + 1) % 30 == 0:
                print(f"æ­¥éª¤ {step+1}/{num_steps}, æ€»æŸå¤±: {total_loss.item():.4f}")
        
        # åå¤„ç†è¾“å‡ºå›¾åƒ - æ¢å¤åŸå§‹å°ºå¯¸
        output_tensor = input_img.data.clamp(0, 1)
        result_image = self._postprocess_image_preserve_size(output_tensor, original_size)
        
        # æœ€ç»ˆè¿›åº¦æ›´æ–°
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 100, num_steps, num_steps, total_loss.item())
        
        return result_image
    
    def _apply_cv_optimized_ghibli_style(self, image):
        """åº”ç”¨è®¡ç®—æœºè§†è§‰ä¼˜åŒ–çš„å®«å´éªé£æ ¼ - å¤§å¹…æ”¹è¿›ç‰ˆæœ¬"""
        print("ğŸ¨ ä½¿ç”¨æ”¹è¿›çš„è®¡ç®—æœºè§†è§‰å®«å´éªé£æ ¼...")
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 10, 1, 10, 0)
        
        img_np = np.array(image)
        
        # æ­£ç¡®å¤„ç†å›¾åƒæ ¼å¼è½¬æ¢
        if img_np.ndim == 2:
            # ç°åº¦å›¾
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        elif img_np.ndim == 3:
            if img_np.shape[2] == 3:
                # RGBå›¾åƒ
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            elif img_np.shape[2] == 4:
                # RGBAå›¾åƒï¼Œè½¬æ¢ä¸ºRGB
                img_bgr = cv2.cvtColor(img_np[:,:,:3], cv2.COLOR_RGB2BGR)
            else:
                # å…¶ä»–é€šé“æ•°ï¼Œè½¬æ¢ä¸ºç°åº¦å†è½¬BGR
                img_gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)
                img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        h, w = img_bgr.shape[:2]
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 20, 2, 10, 0)
        
        # å°ºå¯¸é™åˆ¶
        max_size = 2048
        if max(h,w) > max_size:
            scale = max_size / max(h,w)
            img_bgr = cv2.resize(img_bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_LANCZOS4)
            h, w = img_bgr.shape[:2]
            print(f"ğŸ“ è®¡ç®—æœºè§†è§‰å¤„ç†: å›¾ç‰‡å°ºå¯¸è¿‡å¤§ï¼Œè‡ªåŠ¨ç¼©æ”¾è‡³: {w}x{h}")
        else:
            print(f"ğŸ“ è®¡ç®—æœºè§†è§‰å¤„ç†: ä¿æŒåŸå§‹å°ºå¯¸: {w}x{h}")
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 30, 3, 10, 0)
        
        # 1. å…ˆè½¬æ¢ä¸ºåŠ¨æ¼«é£æ ¼ - åˆ›é€ åŸºæœ¬çš„åŠ¨æ¼«æ•ˆæœ
        img_bgr = self._anime_style_conversion(img_bgr)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 50, 5, 10, 0)
        
        # 2. å†å åŠ å®«å´éªè‰²å½©é£æ ¼ - åœ¨åŠ¨æ¼«åŸºç¡€ä¸Šè°ƒæ•´è‰²å½©
        img_bgr = self._ghibli_color_style(img_bgr)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 70, 7, 10, 0)
        
        # 3. æœ€ç»ˆä¼˜åŒ–
        img_bgr = self._final_anime_optimization(img_bgr)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 100, 10, 10, 0)
        
        result_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        return Image.fromarray(result_rgb)
    
    def _advanced_anime_style_filter(self, img_bgr):
        """
        é«˜çº§åŠ¨æ¼«é£æ ¼æ»¤é•œ - æ›´æ¥è¿‘å®«å´éªé£æ ¼
        - å¤šæ¬¡è¾¹ç¼˜ä¿ç•™å¹³æ»‘ï¼Œç§»é™¤å†™å®çº¹ç†
        - é¢œè‰²å¤§å—åŒ–ï¼ˆKMeans + SLICè¶…åƒç´ å‡å€¼åŒ–ï¼‰
        - æŸ”å’Œç»†çº¿ç¨¿å åŠ 
        """
        # 1) è¾¹ç¼˜ä¿ç•™å¹³æ»‘ï¼ˆä¸¤æ¬¡åŒè¾¹æ»¤æ³¢ï¼‰
        guided = cv2.bilateralFilter(img_bgr, d=11, sigmaColor=85, sigmaSpace=85)
        guided = cv2.bilateralFilter(guided, d=9, sigmaColor=75, sigmaSpace=75)

        # 2) æ™ºèƒ½è¾¹ç¼˜æ£€æµ‹ + è½»å¾®è†¨èƒ€
        gray = cv2.cvtColor(guided, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 40, 120)
        kernel = np.ones((2, 2), np.uint8)
        edges = cv2.dilate(edges, kernel, iterations=1)

        # 3) é¢œè‰²é‡åŒ–ï¼ˆæ›´å°‘çš„é¢œè‰²ä»¥è·å¾—å¡é€šåˆ†åŒºï¼‰
        Z = guided.reshape((-1, 3)).astype(np.float32)
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 1.0)
        K = 12
        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        cartoon = centers[labels.flatten()].reshape(guided.shape)

        # 4) SLICè¶…åƒç´ å‡å€¼åŒ–ï¼Œè¿›ä¸€æ­¥å¤§ç‰‡åŒºæ‰å¹³ï¼ˆæ›´â€œåŠ¨æ¼«â€ï¼‰
        try:
            from skimage.segmentation import slic
            from skimage.color import label2rgb
            img_rgb = cv2.cvtColor(cartoon, cv2.COLOR_BGR2RGB)
            segments = slic(img_rgb, n_segments=600, compactness=20, sigma=0, start_label=1)
            flat_rgb = (label2rgb(segments, img_rgb, kind='avg') * 255).astype(np.uint8)
            cartoon = cv2.cvtColor(flat_rgb, cv2.COLOR_RGB2BGR)
        except Exception:
            pass

        # 5) è¾¹ç¼˜å åŠ ï¼ˆæŸ”å’Œçº¿ç¨¿ï¼‰
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR).astype(np.float32) / 255.0
        cartoon = cartoon.astype(np.float32) / 255.0
        result = cv2.addWeighted(cartoon, 0.85, edges_colored, 0.15, 0)
        result = (result * 255).astype(np.uint8)
        return result
    
    def _enhanced_ghibli_color_style(self, img_bgr):
        """å¢å¼ºçš„å®«å´éªè‰²å½©é£æ ¼"""
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç¡®çš„è°ƒæ•´
        # ç¡®ä¿å›¾åƒæ˜¯3é€šé“çš„BGRæ ¼å¼
        if len(img_bgr.shape) == 2:
            img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)
        elif img_bgr.shape[2] == 1:
            img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)
        
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦ï¼ˆå®«å´éªé£æ ¼è‰²å½©é²œè‰³ï¼‰
        s = cv2.add(s, 40)  # å¢åŠ é¥±å’Œåº¦
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒï¼ˆå®«å´éªé£æ ¼ç‰¹ç‚¹ï¼‰
        h = cv2.add(h, 8)  # è½»å¾®åå‘æ©™è‰²/é»„è‰²
        h = np.clip(h, 0, 179)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        v = cv2.add(v, 15)
        v = np.clip(v, 0, 255)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        
        # è½¬æ¢å›BGR
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # åº”ç”¨æŸ”å’Œæ»¤é•œ
        soft = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        # æ··åˆåŸå§‹å’ŒæŸ”å’Œç‰ˆæœ¬
        result = cv2.addWeighted(enhanced, 0.7, soft, 0.3, 0)
        
        return result
    
    def _enhanced_dreamy_lighting(self, img_bgr):
        """å¢å¼ºçš„æ¢¦å¹»å…‰å½±æ•ˆæœ"""
        h, w = img_bgr.shape[:2]
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½© - ä¸­å¿ƒæ˜äº®ï¼Œè¾¹ç¼˜æŸ”å’Œï¼ˆæ˜¾è‘—å‡å¼±ä»¥é¿å…æ˜æ˜¾åŒå¿ƒç¯å’Œå¸¦çŠ¶ï¼‰
        light_mask = 1.0 - (distance / max_distance) * 0.05
        light_mask = np.clip(light_mask, 0.95, 1.0)
        
        # åº”ç”¨æ›´æŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        final = img_bgr.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        return final
    
    def _enhanced_ghibli_color_palette(self, img_bgr):
        """åŸºäºç‰¹å¾åˆ†æçš„å®«å´éªè‰²å½©è°ƒè‰²æ¿ - ç²¾ç¡®åŒ¹é…å®«å´éªé£æ ¼"""
        # å®«å´éªé£æ ¼ç‰¹ç‚¹ï¼šé«˜é¥±å’Œåº¦ã€æ˜äº®ã€æ¸©æš–è‰²è°ƒã€æ¢¦å¹»æ„Ÿ
        
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œç²¾ç¡®è°ƒæ•´
        # ç¡®ä¿å›¾åƒæ˜¯3é€šé“çš„BGRæ ¼å¼
        if len(img_bgr.shape) == 2:
            img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)
        elif img_bgr.shape[2] == 1:
            img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)
        
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # 1. å¤§å¹…å¢å¼ºé¥±å’Œåº¦ - å®«å´éªé£æ ¼è‰²å½©é²œè‰³ï¼ˆè§£å†³é»‘ç™½ç°é—®é¢˜ï¼‰
        s = cv2.add(s, 80)  # å¤§å¹…å¢åŠ é¥±å’Œåº¦
        s = np.clip(s, 0, 240)  # æé«˜æœ€å¤§é¥±å’Œåº¦é™åˆ¶
        
        # 2. æ˜¾è‘—å¢å¼ºäº®åº¦ - å®«å´éªé£æ ¼æ˜äº®
        v = cv2.add(v, 30)
        v = np.clip(v, 0, 255)
        
        # 3. å¼ºçƒˆè°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒï¼ˆæ©™è‰²/é»„è‰²ï¼‰
        # å®«å´éªé£æ ¼æ¸©æš–è‰²è°ƒèŒƒå›´ï¼š10-40ï¼ˆæ©™è‰²åˆ°é»„è‰²ï¼‰
        h_warm = h.copy()
        warm_mask = (h > 10) & (h < 40)
        if np.any(warm_mask):
            h_warm[warm_mask] = np.clip(h_warm[warm_mask] + 10, 0, 179)  # å¼ºçƒˆåå‘æ›´æš–
            h = np.where(warm_mask, h_warm, h)
        
        # 4. å¢å¼ºè“è‰²å’Œç»¿è‰²ï¼ˆå®«å´éªé£æ ¼ä¸­çš„å¤©ç©ºå’Œè‡ªç„¶è‰²ï¼‰
        blue_green_mask = (h > 85) & (h < 150)
        if np.any(blue_green_mask):
            s_blue_green = s.copy()
            s_blue_green[blue_green_mask] = np.clip(s_blue_green[blue_green_mask] + 50, 0, 255)
            s = np.where(blue_green_mask, s_blue_green, s)
        
        h = np.clip(h, 0, 179)
        
        # 5. åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # 6. åº”ç”¨LABè‰²å½©ç©ºé—´è¿›ä¸€æ­¥ä¼˜åŒ–
        lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¤§å¹…å¢å¼ºè‰²å½©é²œè‰³åº¦
        a = cv2.add(a, 25)  # å¼ºçƒˆå¢å¼ºçº¢è‰²/ç»¿è‰²
        b = cv2.add(b, 30)  # å¼ºçƒˆå¢å¼ºè“è‰²/é»„è‰²
        a = np.clip(a, 0, 255)
        b = np.clip(b, 0, 255)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_balanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        # 7. åº”ç”¨æŸ”å’Œæ»¤é•œä¿æŒå®«å´éªçš„æŸ”å’Œæ„Ÿ
        soft = cv2.GaussianBlur(final, (3, 3), 0)
        result = cv2.addWeighted(final, 0.9, soft, 0.1, 0)  # å‡å°‘æŸ”å’Œåº¦ï¼Œä¿æŒæ¸…æ™°åº¦
        
        return result
    
    def _anime_style_conversion(self, img_bgr):
        """çœŸæ­£çš„åŠ¨æ¼«é£æ ¼è½¬æ¢ - å°†çœŸå®ç…§ç‰‡è½¬æ¢ä¸ºåŠ¨æ¼«é£æ ¼"""
        # åŠ¨æ¼«é£æ ¼æ ¸å¿ƒç‰¹ç‚¹ï¼š
        # 1. ç®€åŒ–çš„è‰²å—å’Œæ‰å¹³åŒ–æ•ˆæœ
        # 2. æ¸…æ™°çš„è½®å»“çº¿æ¡
        # 3. å‡å°‘å†™å®çº¹ç†ï¼Œå¢åŠ å¡é€šæ„Ÿ
        
        # ç¬¬ä¸€æ­¥ï¼šæ·±åº¦è¾¹ç¼˜ä¿ç•™å¹³æ»‘ - ç§»é™¤å†™å®çº¹ç†
        filtered = cv2.bilateralFilter(img_bgr, d=15, sigmaColor=100, sigmaSpace=100)
        filtered = cv2.bilateralFilter(filtered, d=13, sigmaColor=80, sigmaSpace=80)
        
        # ç¬¬äºŒæ­¥ï¼šå¼ºçƒˆçš„é¢œè‰²é‡åŒ– - åˆ›é€ åŠ¨æ¼«çš„æ‰å¹³è‰²å—
        Z = filtered.reshape((-1, 3))
        Z = np.float32(Z)
        
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 1.0)
        K = 8  # è¾ƒå°‘çš„é¢œè‰²æ•°é‡ï¼Œåˆ›é€ åŠ¨æ¼«æ‰å¹³åŒ–æ•ˆæœ
        
        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 20, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        cartoon = centers[labels.flatten()]
        cartoon = cartoon.reshape((filtered.shape))
        
        # ç¬¬ä¸‰æ­¥ï¼šè¶…åƒç´ åˆ†å‰² - åˆ›é€ è‡ªç„¶çš„è‰²å—è¾¹ç•Œ
        try:
            from skimage.segmentation import slic
            from skimage.color import label2rgb
            
            img_rgb = cv2.cvtColor(cartoon, cv2.COLOR_BGR2RGB)
            segments = slic(img_rgb, n_segments=200, compactness=25, sigma=1)
            flat_rgb = (label2rgb(segments, img_rgb, kind='avg') * 255).astype(np.uint8)
            cartoon = cv2.cvtColor(flat_rgb, cv2.COLOR_RGB2BGR)
        except Exception:
            pass
        
        # ç¬¬å››æ­¥ï¼šç”Ÿæˆæ¸…æ™°çš„åŠ¨æ¼«è½®å»“çº¿æ¡
        gray = cv2.cvtColor(cartoon, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨å¤šç§è¾¹ç¼˜æ£€æµ‹æ–¹æ³•ç»„åˆ
        edges_canny = cv2.Canny(gray, 30, 100)
        edges_adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                             cv2.THRESH_BINARY, 9, 2)
        
        # åˆå¹¶è¾¹ç¼˜æ£€æµ‹ç»“æœ
        edges_combined = cv2.bitwise_or(edges_canny, edges_adaptive)
        
        # æŸ”åŒ–çº¿æ¡ï¼Œåˆ›é€ åŠ¨æ¼«é£æ ¼çš„æŸ”å’Œè½®å»“
        edges_soft = cv2.GaussianBlur(edges_combined, (3, 3), 0.5)
        
        # ç¬¬äº”æ­¥ï¼šå°†çº¿æ¡å åŠ åˆ°è‰²å—ä¸Šï¼Œåˆ›é€ çœŸæ­£çš„åŠ¨æ¼«æ•ˆæœ
        edges_colored = cv2.cvtColor(edges_soft, cv2.COLOR_GRAY2BGR)
        
        # å¼ºçƒˆçš„çº¿æ¡å åŠ ï¼Œåˆ›é€ æ˜æ˜¾çš„åŠ¨æ¼«è½®å»“
        result = cv2.addWeighted(cartoon, 0.8, edges_colored, 0.2, 0)
        
        return result
    
    def _ghibli_color_style(self, img_bgr):
        """å®«å´éªè‰²å½©é£æ ¼ - åœ¨åŠ¨æ¼«åŸºç¡€ä¸Šå åŠ å®«å´éªç‰¹è‰²è‰²å½©"""
        # å®«å´éªè‰²å½©ç‰¹ç‚¹ï¼š
        # 1. æ¸©æš–æ˜äº®çš„è‰²è°ƒ
        # 2. é«˜é¥±å’Œåº¦ä½†ä¸åˆºçœ¼
        # 3. æ¢¦å¹»çš„å…‰å½±æ•ˆæœ
        
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œç²¾ç¡®è°ƒæ•´
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦ - å®«å´éªé£æ ¼è‰²å½©é²œè‰³
        s = cv2.add(s, 40)
        s = np.clip(s, 0, 220)
        
        # è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒï¼ˆæ©™è‰²/é»„è‰²ï¼‰
        h_warm = h.copy()
        warm_mask = (h > 10) & (h < 40)
        if np.any(warm_mask):
            h_warm[warm_mask] = np.clip(h_warm[warm_mask] + 8, 0, 179)
            h = np.where(warm_mask, h_warm, h)
        
        # å¢å¼ºäº®åº¦ - å®«å´éªé£æ ¼æ˜äº®
        v = cv2.add(v, 20)
        v = np.clip(v, 0, 255)
        
        # å¢å¼ºè“è‰²å’Œç»¿è‰²ï¼ˆå®«å´éªé£æ ¼ä¸­çš„å¤©ç©ºå’Œè‡ªç„¶è‰²ï¼‰
        blue_green_mask = (h > 85) & (h < 150)
        if np.any(blue_green_mask):
            s_blue_green = s.copy()
            s_blue_green[blue_green_mask] = np.clip(s_blue_green[blue_green_mask] + 30, 0, 255)
            s = np.where(blue_green_mask, s_blue_green, s)
        
        h = np.clip(h, 0, 179)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # åº”ç”¨LABè‰²å½©ç©ºé—´è¿›ä¸€æ­¥ä¼˜åŒ–
        lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºè‰²å½©é²œè‰³åº¦
        a = cv2.add(a, 15)
        b = cv2.add(b, 20)
        a = np.clip(a, 0, 255)
        b = np.clip(b, 0, 255)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_balanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        # æ·»åŠ å®«å´éªé£æ ¼çš„æ¢¦å¹»å…‰å½±æ•ˆæœ
        h, w = final.shape[:2]
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        light_mask = 1.0 - (distance / max_distance) * 0.08
        light_mask = np.clip(light_mask, 0.92, 1.0)
        
        result = final.astype(np.float32) * light_mask[:,:,np.newaxis]
        result = np.clip(result, 0, 255).astype(np.uint8)
        
        return result
    
    def _clear_line_enhancement(self, img_bgr):
        """åŸºäºç‰¹å¾åˆ†æçš„çº¿æ¡å¢å¼º - ç²¾ç¡®åŒ¹é…å®«å´éªé£æ ¼"""
        # å®«å´éªé£æ ¼ç‰¹ç‚¹ï¼šæ¸…æ™°ä½†ä¸ç”Ÿç¡¬çš„çº¿æ¡
        
        # 1. æå–ç°åº¦å›¾åƒ
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        # 2. ä½¿ç”¨æ”¹è¿›çš„è¾¹ç¼˜æ£€æµ‹ - å¢å¼ºçº¿æ¡æ¸…æ™°åº¦
        # ä½¿ç”¨Cannyè¾¹ç¼˜æ£€æµ‹ï¼Œè°ƒæ•´é˜ˆå€¼ä»¥è·å¾—æ›´æ¸…æ™°çš„çº¿æ¡
        edges_canny = cv2.Canny(gray, 50, 150)
        
        # 3. ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼è·å–æ›´ä¸°å¯Œçš„è¾¹ç¼˜ä¿¡æ¯
        edges_adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                             cv2.THRESH_BINARY, 9, 3)
        
        # 4. åˆå¹¶ä¸¤ç§è¾¹ç¼˜æ£€æµ‹ç»“æœ
        edges_combined = cv2.bitwise_or(edges_canny, edges_adaptive)
        
        # 5. åˆ›å»ºæ›´æ¸…æ™°çš„çº¿ç¨¿æ•ˆæœ
        edges_enhanced = cv2.GaussianBlur(edges_combined, (3, 3), 0.8)
        
        # 6. å¢å¼ºçº¿ç¨¿å¯¹æ¯”åº¦
        edges_enhanced = cv2.addWeighted(edges_enhanced, 1.5, edges_enhanced, 0, 0)
        
        # 7. è½¬æ¢ä¸ºå½©è‰²çº¿ç¨¿
        edges_colored = cv2.cvtColor(edges_enhanced, cv2.COLOR_GRAY2BGR)
        
        # 8. å¢å¼ºçº¿ç¨¿å¼ºåº¦ï¼Œæé«˜åŠ¨æ¼«é£æ ¼æ˜æ˜¾åº¦
        line_strength = 0.15  # å¢åŠ çº¿ç¨¿å¼ºåº¦
        result = cv2.addWeighted(img_bgr, 1.0 - line_strength, edges_colored, line_strength, 0)
        
        return result
    
    def _preserve_structure_enhancement(self, img_bgr, original_img):
        """ä¿æŒåŸå›¾ç»“æ„çš„å¢å¼º - é¿å…ä¸åŸå›¾å·®å¼‚è¿‡å¤§"""
        # 1. ä¸åŸå›¾è¿›è¡Œæ··åˆï¼Œä¿æŒåŸå§‹ç»“æ„
        # æ£€æŸ¥å›¾åƒç»´åº¦ï¼Œæ­£ç¡®å¤„ç†ç°åº¦å›¾å’Œå½©è‰²å›¾
        if len(original_img.shape) == 3 and original_img.shape[2] == 3:
            original_bgr = cv2.cvtColor(original_img, cv2.COLOR_RGB2BGR)
        else:
            # ç°åº¦å›¾æˆ–å•é€šé“å›¾
            original_bgr = cv2.cvtColor(original_img, cv2.COLOR_GRAY2BGR)
        
        # è°ƒæ•´å°ºå¯¸åŒ¹é…
        if original_bgr.shape != img_bgr.shape:
            original_bgr = cv2.resize(original_bgr, (img_bgr.shape[1], img_bgr.shape[0]))
        
        # 2. ä¸åŸå›¾è¿›è¡Œé€‚åº¦æ··åˆï¼ˆ70%åŠ¨æ¼«æ•ˆæœ + 30%åŸå›¾ï¼‰
        blended = cv2.addWeighted(img_bgr, 0.7, original_bgr, 0.3, 0)
        
        return blended
    
    def _final_ghibli_optimization(self, img_bgr):
        """æœ€ç»ˆçš„å®«å´éªé£æ ¼ä¼˜åŒ–"""
        # 1. è½»å¾®é”åŒ–å¢å¼ºç»†èŠ‚
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(img_bgr, -1, kernel)
        
        # 2. è½»å¾®é™å™ª
        denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 5, 5, 7, 21)
        
        # 3. æœ€ç»ˆè‰²å½©å¹³è¡¡
        # ç¡®ä¿å›¾åƒæ˜¯3é€šé“çš„BGRæ ¼å¼
        if len(denoised.shape) == 2:
            denoised = cv2.cvtColor(denoised, cv2.COLOR_GRAY2BGR)
        elif denoised.shape[2] == 1:
            denoised = cv2.cvtColor(denoised, cv2.COLOR_GRAY2BGR)
            
        lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºäº®åº¦
        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_balanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        # 4. æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ
        h, w = final.shape[:2]
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        light_mask = 1.0 - (distance / max_distance) * 0.08
        light_mask = np.clip(light_mask, 0.92, 1.0)
        
        result = final.astype(np.float32) * light_mask[:,:,np.newaxis]
        result = np.clip(result, 0, 255).astype(np.uint8)
        
        return result
    
    def set_progress_callback(self, callback, task_id):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
        self.task_id = task_id
    
    def _get_person_mask(self, img_bgr):
        """è·å–äººç‰©å‰æ™¯æ©è†œï¼ˆ0~255ï¼‰"""
        try:
            if self.seg_model is None:
                from torchvision import models
                self.seg_model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT).eval()
            import torchvision.transforms as T
            transform = T.Compose([
                T.ToPILImage(),
                T.Resize(520),
                T.ToTensor(),
                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            x = transform(rgb).unsqueeze(0)
            with torch.no_grad():
                out = self.seg_model(x)['out'][0]  # [21, H, W]
            person_class = 15  # COCO person
            mask = out.argmax(0).byte().cpu().numpy()
            mask = (mask == person_class).astype(np.uint8) * 255
            # è°ƒæ•´åˆ°åŸå›¾å°ºå¯¸
            mask = cv2.resize(mask, (img_bgr.shape[1], img_bgr.shape[0]), interpolation=cv2.INTER_NEAREST)
            # å¹³æ»‘è¾¹ç¼˜
            mask = cv2.GaussianBlur(mask, (9,9), 0)
            return mask
        except Exception as e:
            print(f"âš ï¸ å‰æ™¯åˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨å…¨å›¾: {e}")
            return np.ones(img_bgr.shape[:2], dtype=np.uint8) * 255
    
    def _xdog_edges(self, gray, k=4.5, sigma=0.9, epsilon=-0.1, phi=10):
        """XDoGé£æ ¼çº¿ç¨¿ï¼Œè¿”å›0~255"""
        g1 = cv2.GaussianBlur(gray, (0,0), sigma)
        g2 = cv2.GaussianBlur(gray, (0,0), sigma*k)
        D = g1 - g2
        D = D / (np.max(np.abs(D)) + 1e-8)
        E = np.ones_like(D)
        E[D < epsilon] = 1 + np.tanh(phi*(D[D < epsilon]-epsilon))
        E[D >= epsilon] = 1
        E = (E*255).astype(np.uint8)
        return 255 - E
    
    def _alpha_blend(self, fg, bg, mask):
        mask_f = (mask.astype(np.float32)/255.0)[:,:,None]
        return (fg*mask_f + bg*(1-mask_f)).astype(np.uint8)
    
    def _preprocess_image(self, image):
        """é¢„å¤„ç†å›¾åƒ"""
        transform = transforms.Compose([
            transforms.Resize(512),
            transforms.CenterCrop(512),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).unsqueeze(0)
    
    def _postprocess_image(self, tensor):
        """åå¤„ç†å¼ é‡ä¸ºå›¾åƒ"""
        # åå½’ä¸€åŒ–
        tensor = tensor.squeeze(0).cpu()
        tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        tensor = torch.clamp(tensor, 0, 1)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        transform = transforms.ToPILImage()
        image = transform(tensor)
        
        return image
    
    def _fallback_traditional_method(self, image):
        """å¤‡é€‰ä¼ ç»Ÿæ–¹æ³•"""
        print("âš ï¸ ä½¿ç”¨å¤‡é€‰ä¼ ç»Ÿæ–¹æ³•")
        
        # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
        img_np = np.array(image)
        
        # è½¬æ¢ä¸ºBGRæ ¼å¼
        if len(img_np.shape) == 3 and img_np.shape[2] == 3:
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        else:
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        
        # é«˜è´¨é‡çš„å®«å´éªé£æ ¼å¤„ç†
        
        # 1. ä¿æŒåŸå§‹åˆ†è¾¨ç‡
        h, w = img_bgr.shape[:2]
        max_size = 2000
        if max(h, w) > max_size:
            scale = max_size / max(h, w)
            new_w, new_h = int(w * scale), int(h * scale)
            img_bgr = cv2.resize(img_bgr, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
        
        # 2. æ™ºèƒ½è¾¹ç¼˜ä¿ç•™ï¼ˆé‡ç‚¹æ”¹è¿›äººç‰©åŒºåŸŸï¼‰
        # ä½¿ç”¨åŒè¾¹æ»¤æ³¢æ›¿ä»£å¯¼å‘æ»¤æ³¢
        guided = cv2.bilateralFilter(img_bgr, d=11, sigmaColor=80, sigmaSpace=80)
        
        # 3. å®«å´éªé£æ ¼è‰²å½©è°ƒæ•´
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç¡®çš„è‰²å½©è°ƒæ•´
        # ç¡®ä¿å›¾åƒæ˜¯3é€šé“çš„BGRæ ¼å¼
        if len(guided.shape) == 2:
            guided = cv2.cvtColor(guided, cv2.COLOR_GRAY2BGR)
        elif guided.shape[2] == 1:
            guided = cv2.cvtColor(guided, cv2.COLOR_GRAY2BGR)
            
        lab = cv2.cvtColor(guided, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºè‰²å½©é²œè‰³åº¦ï¼ˆå®«å´éªé£æ ¼ç‰¹ç‚¹ï¼‰
        a = cv2.addWeighted(a, 1.2, a, 0, 0)
        b = cv2.addWeighted(b, 1.2, b, 0, 0)
        
        # è°ƒæ•´äº®åº¦å’Œå¯¹æ¯”åº¦
        l = cv2.createCLAHE(clipLimit=2.0).apply(l)
        
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # 4. æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ
        h, w = enhanced.shape[:2]
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½©
        light_mask = 1.0 - (distance / max_distance) * 0.15
        light_mask = np.clip(light_mask, 0.85, 1.0)
        
        # åº”ç”¨å…‰ç…§æ•ˆæœ
        final = enhanced.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        # è½¬æ¢å›RGB
        result_rgb = cv2.cvtColor(final, cv2.COLOR_BGR2RGB)
        
        return result_rgb

    def _subtle_color_optimization(self, img_bgr):
        """è½»å¾®çš„è‰²å½©ä¼˜åŒ– - ä¿æŒåŸå›¾è‰²å½©ï¼Œåªåšè½»å¾®è°ƒæ•´"""
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œè½»å¾®è°ƒæ•´
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # è½»å¾®å¢å¼ºé¥±å’Œåº¦ï¼ˆ+10ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„+80ï¼‰
        s = cv2.add(s, 10)
        s = np.clip(s, 0, 255)
        
        # è½»å¾®å¢å¼ºäº®åº¦ï¼ˆ+5ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„+30ï¼‰
        v = cv2.add(v, 5)
        v = np.clip(v, 0, 255)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        return enhanced
    
    def _apply_optimized_cv_anime_style(self, content_image):
        """åº”ç”¨ä¸“é—¨é’ˆå¯¹å®«å´éªé£æ ¼ä¼˜åŒ–çš„è½¬æ¢ç®—æ³•"""
        print("ğŸ¨ ä½¿ç”¨å®«å´éªé£æ ¼ä¸“ç”¨è½¬æ¢ç®—æ³•...")
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 10, 1, 10, 0)
        
        img_np = np.array(content_image)
        
        # æ­£ç¡®å¤„ç†å›¾åƒæ ¼å¼è½¬æ¢
        if img_np.ndim == 2:
            # ç°åº¦å›¾
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        elif img_np.ndim == 3:
            if img_np.shape[2] == 3:
                # RGBå›¾åƒ
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            elif img_np.shape[2] == 4:
                # RGBAå›¾åƒï¼Œè½¬æ¢ä¸ºRGB
                img_bgr = cv2.cvtColor(img_np[:,:,:3], cv2.COLOR_RGB2BGR)
            else:
                # å…¶ä»–é€šé“æ•°ï¼Œè½¬æ¢ä¸ºç°åº¦å†è½¬BGR
                img_gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)
                img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        
        h, w = img_bgr.shape[:2]
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 20, 2, 10, 0)
        
        # å°ºå¯¸é™åˆ¶
        max_size = 1024
        if max(h,w) > max_size:
            scale = max_size / max(h,w)
            img_bgr = cv2.resize(img_bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_LANCZOS4)
            h, w = img_bgr.shape[:2]
            print(f"ğŸ“ ä¼˜åŒ–å¤„ç†: å›¾ç‰‡å°ºå¯¸è¿‡å¤§ï¼Œè‡ªåŠ¨ç¼©æ”¾è‡³: {w}x{h}")
        else:
            print(f"ğŸ“ ä¼˜åŒ–å¤„ç†: ä¿æŒåŸå§‹å°ºå¯¸: {w}x{h}")
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 30, 3, 10, 0)
        
        # 1. å®«å´éªé£æ ¼é¢„å¤„ç† - åŸºäºå‚è€ƒå›¾ç‰‡åˆ†æ
        # ä½¿ç”¨è½»å¾®çš„åŒè¾¹æ»¤æ³¢ï¼Œä¿ç•™ç»†èŠ‚ä½†åˆ›é€ æŸ”å’Œæ•ˆæœ
        filtered = cv2.bilateralFilter(img_bgr, d=9, sigmaColor=75, sigmaSpace=75)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 40, 4, 10, 0)
        
        # 2. å®«å´éªé£æ ¼çº¿æ¡ç”Ÿæˆ - æ¸…æ™°ä½†ä¸ç”Ÿç¡¬
        gray = cv2.cvtColor(filtered, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨å®«å´éªé£æ ¼çš„è¾¹ç¼˜æ£€æµ‹
        edges_canny = cv2.Canny(gray, 50, 150)  # å‚è€ƒå®«å´éªå›¾ç‰‡çš„è¾¹ç¼˜å¯†åº¦
        
        # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼å¢å¼ºé‡è¦è¾¹ç¼˜
        edges_adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                               cv2.THRESH_BINARY, 11, 2)
        
        # åˆå¹¶è¾¹ç¼˜æ£€æµ‹ç»“æœ
        edges_combined = cv2.bitwise_or(edges_canny, edges_adaptive)
        
        # å®«å´éªé£æ ¼çš„æŸ”å’Œçº¿æ¡
        edges_soft = cv2.GaussianBlur(edges_combined, (3, 3), 0.5)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 50, 5, 10, 0)
        
        # 3. å®«å´éªé£æ ¼é¢œè‰²å¤„ç† - åŸºäºå‚è€ƒå›¾ç‰‡åˆ†æ
        # ä½¿ç”¨é€‚åº¦çš„é¢œè‰²é‡åŒ–ï¼ˆå‚è€ƒå®«å´éªé£æ ¼çš„é¢œè‰²ç®€åŒ–ç¨‹åº¦ï¼‰
        Z = filtered.reshape((-1, 3))
        Z = np.float32(Z)
        
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        K = 24  # åŸºäºå®«å´éªé£æ ¼çš„é¢œè‰²æ•°é‡è°ƒæ•´
        
        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        cartoon = centers[labels.flatten()]
        cartoon = cartoon.reshape((filtered.shape))
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 60, 6, 10, 0)
        
        # 4. è¶…åƒç´ åˆ†å‰² - åˆ›é€ è‡ªç„¶çš„è‰²å—è¾¹ç•Œï¼ˆå®«å´éªé£æ ¼ç‰¹ç‚¹ï¼‰
        try:
            from skimage.segmentation import slic
            from skimage.color import label2rgb
            
            img_rgb = cv2.cvtColor(cartoon, cv2.COLOR_BGR2RGB)
            segments = slic(img_rgb, n_segments=200, compactness=20, sigma=1)
            flat_rgb = (label2rgb(segments, img_rgb, kind='avg') * 255).astype(np.uint8)
            cartoon = cv2.cvtColor(flat_rgb, cv2.COLOR_RGB2BGR)
            
        except Exception as e:
            print(f"âš ï¸ è¶…åƒç´ åˆ†å‰²å¤±è´¥: {e}")
            # ä½¿ç”¨å‡å€¼æ¼‚ç§»æ»¤æ³¢ä½œä¸ºå¤‡é€‰
            try:
                cartoon = cv2.pyrMeanShiftFiltering(cartoon, 15, 30)
            except Exception:
                pass
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 70, 7, 10, 0)
        
        # 5. å®«å´éªé£æ ¼çº¿æ¡å åŠ 
        edges_colored = cv2.cvtColor(edges_soft, cv2.COLOR_GRAY2BGR)
        
        # åŸºäºå®«å´éªé£æ ¼çš„çº¿æ¡å åŠ æ¯”ä¾‹
        result = cv2.addWeighted(cartoon, 0.85, edges_colored, 0.15, 0)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 80, 8, 10, 0)
        
        # 6. åº”ç”¨å®«å´éªè‰²å½©é£æ ¼ï¼ˆåŸºäºå‚è€ƒå›¾ç‰‡åˆ†æï¼‰
        result = self._apply_ghibli_style_based_on_reference(result)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 90, 9, 10, 0)
        
        # 7. æœ€ç»ˆå®«å´éªé£æ ¼ä¼˜åŒ–
        result = self._final_ghibli_style_optimization(result)
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 100, 10, 10, 0)
        
        result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
        return Image.fromarray(result_rgb)
    
    def _apply_ghibli_color_to_anime(self, img_bgr):
        """åœ¨åŠ¨æ¼«é£æ ¼åŸºç¡€ä¸Šåº”ç”¨å®«å´éªè‰²å½©"""
        # å®«å´éªè‰²å½©ç‰¹ç‚¹ï¼šæ¸©æš–ã€æ˜äº®ã€é«˜é¥±å’Œåº¦ä½†ä¸åˆºçœ¼
        
        # 1. è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œç²¾ç¡®è°ƒæ•´
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # 2. å¢å¼ºé¥±å’Œåº¦ï¼ˆå®«å´éªé£æ ¼è‰²å½©é²œè‰³ï¼‰
        s = cv2.add(s, 40)  # é€‚åº¦å¢å¼ºé¥±å’Œåº¦
        s = np.clip(s, 0, 220)
        
        # 3. è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒï¼ˆæ©™è‰²/é»„è‰²ï¼‰
        h_warm = h.copy()
        warm_mask = (h > 10) & (h < 40)
        if np.any(warm_mask):
            h_warm[warm_mask] = np.clip(h_warm[warm_mask] + 8, 0, 179)  # è½»å¾®åå‘æ¸©æš–
            h = np.where(warm_mask, h_warm, h)
        
        # 4. å¢å¼ºäº®åº¦ - å®«å´éªé£æ ¼æ˜äº®
        v = cv2.add(v, 20)
        v = np.clip(v, 0, 255)
        
        # 5. å¢å¼ºè“è‰²å’Œç»¿è‰²ï¼ˆå®«å´éªé£æ ¼ä¸­çš„å¤©ç©ºå’Œè‡ªç„¶è‰²ï¼‰
        blue_green_mask = (h > 85) & (h < 150)
        if np.any(blue_green_mask):
            s_blue_green = s.copy()
            s_blue_green[blue_green_mask] = np.clip(s_blue_green[blue_green_mask] + 30, 0, 255)
            s = np.where(blue_green_mask, s_blue_green, s)
        
        h = np.clip(h, 0, 179)
        
        # 6. åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # 7. åº”ç”¨LABè‰²å½©ç©ºé—´è¿›ä¸€æ­¥ä¼˜åŒ–
        lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºè‰²å½©é²œè‰³åº¦
        a = cv2.add(a, 15)
        b = cv2.add(b, 20)
        a = np.clip(a, 0, 255)
        b = np.clip(b, 0, 255)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_balanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        # 8. æ·»åŠ å®«å´éªé£æ ¼çš„æ¢¦å¹»å…‰å½±æ•ˆæœ
        h, w = final.shape[:2]
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºæŸ”å’Œçš„å…‰ç…§æ•ˆæœ
        light_mask = 1.0 - (distance / max_distance) * 0.08
        light_mask = np.clip(light_mask, 0.92, 1.0)
        
        result = final.astype(np.float32) * light_mask[:,:,np.newaxis]
        result = np.clip(result, 0, 255).astype(np.uint8)
        
        return result
    
    def _apply_improved_ghibli_color(self, img_bgr):
        """æ”¹è¿›çš„å®«å´éªè‰²å½©é£æ ¼ - æ›´è‡ªç„¶ï¼Œä¿ç•™æ›´å¤šç»†èŠ‚"""
        # å®«å´éªè‰²å½©ç‰¹ç‚¹ï¼šæ¸©æš–ã€æ˜äº®ã€è‡ªç„¶
        
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œç²¾ç¡®è°ƒæ•´
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # é€‚åº¦å¢å¼ºé¥±å’Œåº¦ - ä¸è¿‡åº¦
        s = cv2.add(s, 20)  # å‡å°‘é¥±å’Œåº¦å¢å¼ºå¹…åº¦
        s = np.clip(s, 0, 200)
        
        # è½»å¾®è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒ
        h_warm = h.copy()
        warm_mask = (h > 10) & (h < 40)
        if np.any(warm_mask):
            h_warm[warm_mask] = np.clip(h_warm[warm_mask] + 5, 0, 179)  # å‡å°‘è‰²è°ƒè°ƒæ•´å¹…åº¦
            h = np.where(warm_mask, h_warm, h)
        
        # é€‚åº¦å¢å¼ºäº®åº¦
        v = cv2.add(v, 10)
        v = np.clip(v, 0, 255)
        
        h = np.clip(h, 0, 179)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # åº”ç”¨LABè‰²å½©ç©ºé—´è¿›ä¸€æ­¥ä¼˜åŒ–
        lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # é€‚åº¦å¢å¼ºè‰²å½©é²œè‰³åº¦
        a = cv2.add(a, 10)
        b = cv2.add(b, 15)
        a = np.clip(a, 0, 255)
        b = np.clip(b, 0, 255)
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_balanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        return final
    
    def _improved_final_optimization(self, img_bgr):
        """æ”¹è¿›çš„æœ€ç»ˆä¼˜åŒ– - ä¿ç•™æ›´å¤šç»†èŠ‚"""
        # è½»å¾®é”åŒ–å¢å¼ºç»†èŠ‚
        kernel = np.array([[-0.5,-0.5,-0.5], [-0.5,5,-0.5], [-0.5,-0.5,-0.5]])
        sharpened = cv2.filter2D(img_bgr, -1, kernel)
        
        # è½»å¾®é™å™ªï¼Œä¿ç•™ç»†èŠ‚
        denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 2, 2, 3, 10)
        
        return denoised
    
    def _apply_ghibli_style_based_on_reference(self, img_bgr):
        """åŸºäºå®«å´éªå‚è€ƒå›¾ç‰‡çš„è‰²å½©é£æ ¼åº”ç”¨"""
        # å®«å´éªé£æ ¼ç‰¹ç‚¹ï¼ˆåŸºäºåˆ†æï¼‰ï¼š
        # - ä¸­ç­‰é¥±å’Œåº¦ï¼ˆçº¦160-170ï¼‰
        # - è¾ƒé«˜çš„äº®åº¦
        # - æ¸©æš–æŸ”å’Œçš„è‰²è°ƒ
        
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´è¿›è¡Œç²¾ç¡®è°ƒæ•´
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å®«å´éªé£æ ¼é¥±å’Œåº¦è°ƒæ•´
        target_saturation = 165  # åŸºäºå‚è€ƒå›¾ç‰‡çš„å¹³å‡é¥±å’Œåº¦
        current_saturation = np.mean(s)
        saturation_boost = max(0, target_saturation - current_saturation)
        s = cv2.add(s, int(saturation_boost))
        s = np.clip(s, 0, 220)
        
        # å®«å´éªé£æ ¼äº®åº¦è°ƒæ•´
        target_brightness = 180  # åŸºäºå‚è€ƒå›¾ç‰‡çš„å¹³å‡äº®åº¦
        current_brightness = np.mean(v)
        brightness_boost = max(0, target_brightness - current_brightness)
        v = cv2.add(v, int(brightness_boost))
        v = np.clip(v, 0, 255)
        
        # å®«å´éªé£æ ¼è‰²è°ƒè°ƒæ•´ - åå‘æ¸©æš–è‰²è°ƒ
        h_warm = h.copy()
        warm_mask = (h > 10) & (h < 40)  # æ©™è‰²åˆ°é»„è‰²èŒƒå›´
        if np.any(warm_mask):
            h_warm[warm_mask] = np.clip(h_warm[warm_mask] + 8, 0, 179)
            h = np.where(warm_mask, h_warm, h)
        
        h = np.clip(h, 0, 179)
        
        # åˆå¹¶HSVé€šé“
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # åº”ç”¨LABè‰²å½©ç©ºé—´è¿›ä¸€æ­¥ä¼˜åŒ–
        lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å®«å´éªé£æ ¼è‰²å½©é²œè‰³åº¦
        a = cv2.add(a, 20)  # å¢å¼ºçº¢è‰²/ç»¿è‰²
        b = cv2.add(b, 25)  # å¢å¼ºè“è‰²/é»„è‰²
        a = np.clip(a, 0, 255)
        b = np.clip(b, 0, 255)
        
        # å®«å´éªé£æ ¼äº®åº¦å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_balanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        return final
    
    def _final_ghibli_style_optimization(self, img_bgr):
        """æœ€ç»ˆçš„å®«å´éªé£æ ¼ä¼˜åŒ–"""
        # å®«å´éªé£æ ¼è½»å¾®é”åŒ–
        kernel = np.array([[-0.5,-0.5,-0.5], [-0.5,5,-0.5], [-0.5,-0.5,-0.5]])
        sharpened = cv2.filter2D(img_bgr, -1, kernel)
        
        # å®«å´éªé£æ ¼é™å™ªï¼ˆä¿ç•™ç»†èŠ‚ï¼‰
        denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 2, 2, 3, 10)
        
        # æ·»åŠ å®«å´éªé£æ ¼çš„æ¢¦å¹»å…‰å½±æ•ˆæœ
        h, w = denoised.shape[:2]
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # å®«å´éªé£æ ¼çš„å…‰ç…§æ•ˆæœ
        light_mask = 1.0 - (distance / max_distance) * 0.1
        light_mask = np.clip(light_mask, 0.9, 1.0)
        
        result = denoised.astype(np.float32) * light_mask[:,:,np.newaxis]
        result = np.clip(result, 0, 255).astype(np.uint8)
        
        return result
    
    def _blend_with_original(self, anime_img, original_img):
        """ä¸åŸå›¾æ··åˆï¼Œä¿ç•™æ›´å¤šå®ç‰©å†…å®¹"""
        # è°ƒæ•´å°ºå¯¸åŒ¹é…
        if original_img.shape != anime_img.shape:
            original_img = cv2.resize(original_img, (anime_img.shape[1], anime_img.shape[0]))
        
        # ä¸åŸå›¾è¿›è¡Œé€‚åº¦æ··åˆï¼ˆ80%åŠ¨æ¼«æ•ˆæœ + 20%åŸå›¾ç»†èŠ‚ï¼‰
        blended = cv2.addWeighted(anime_img, 0.8, original_img, 0.2, 0)
        
        return blended
    
    def _final_anime_optimization(self, img_bgr):
        """æœ€ç»ˆçš„åŠ¨æ¼«åŒ–ä¼˜åŒ– - ä¿æŒåŠ¨æ¼«é£æ ¼çš„åŒæ—¶è½»å¾®ä¼˜åŒ–"""
        # è½»å¾®é”åŒ–å¢å¼ºç»†èŠ‚
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(img_bgr, -1, kernel)
        
        # è½»å¾®é™å™ª
        denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 3, 3, 5, 15)
        
        return denoised

# åˆ›å»ºçœŸæ­£çš„å®«å´éªé£æ ¼è½¬æ¢æ¨¡å‹
real_ghibli_model = RealGhibliStyleTransfer()

def update_progress(task_id, progress, current_step, total_steps, loss):
    """æ›´æ–°è½¬æ¢è¿›åº¦"""
    # ç¡®ä¿è¿›åº¦å’Œæ­¥éª¤ä¿¡æ¯ä¸€è‡´
    if progress > 0 and current_step == 0:
        # å¦‚æœè¿›åº¦æœ‰å€¼ä½†æ­¥éª¤ä¸º0ï¼Œæ ¹æ®è¿›åº¦è®¡ç®—æ­¥éª¤
        current_step = max(1, int(progress / 100 * total_steps))
    elif current_step > 0 and progress == 0:
        # å¦‚æœæ­¥éª¤æœ‰å€¼ä½†è¿›åº¦ä¸º0ï¼Œæ ¹æ®æ­¥éª¤è®¡ç®—è¿›åº¦
        progress = min(99, int(current_step / total_steps * 100))
    
    conversion_progress[task_id] = {
        'progress': progress,
        'current_step': current_step,
        'total_steps': total_steps,
        'loss': loss,
        'timestamp': time.time()
    }
    print(f"ğŸ“Š ä»»åŠ¡ {task_id}: {progress}% (æ­¥éª¤ {current_step}/{total_steps}, æŸå¤±: {loss:.4f})")

def convert_image_async(task_id, image):
    """å¼‚æ­¥è½¬æ¢å›¾åƒ"""
    try:
        # è®¾ç½®è¿›åº¦å›è°ƒ
        real_ghibli_model.set_progress_callback(update_progress, task_id)
        
        # å¼€å§‹è½¬æ¢
        result_image = real_ghibli_model.apply_real_ghibli_style(image, num_steps=100)
        
        # ä¿å­˜ç»“æœ
        conversion_results[task_id] = {
            'success': True,
            'result_image': result_image,
            'completed': True
        }
        
        # æ›´æ–°è¿›åº¦ä¸ºå®Œæˆ
        update_progress(task_id, 100, 100, 100, 0)
        
    except Exception as e:
        conversion_results[task_id] = {
            'success': False,
            'error': str(e),
            'completed': True
        }
        print(f"âŒ ä»»åŠ¡ {task_id} è½¬æ¢å¤±è´¥: {e}")

