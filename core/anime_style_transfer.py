#!/usr/bin/env python3
"""
çœŸæ­£çš„åŠ¨æ¼«é£æ ¼è½¬æ¢æ¨¡å— - åŸºäºæ·±åº¦å­¦ä¹ å’ŒGANs
å®ç°çœŸæ­£çš„ç…§ç‰‡è½¬åŠ¨æ¼«é£æŠ€æœ¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import torch.optim as optim
import numpy as np
from PIL import Image
import cv2
import os

class AnimeStyleGAN(nn.Module):
    """åŠ¨æ¼«é£æ ¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ"""
    
    def __init__(self):
        super(AnimeStyleGAN, self).__init__()
        
        # ç¼–ç å™¨ - æå–çœŸå®ç…§ç‰‡ç‰¹å¾
        self.encoder = nn.Sequential(
            # è¾“å…¥: 3x256x256
            nn.Conv2d(3, 64, 4, 2, 1),  # 64x128x128
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1),  # 128x64x64
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, 2, 1),  # 256x32x32
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
            nn.Conv2d(256, 512, 4, 2, 1),  # 512x16x16
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),
            nn.Conv2d(512, 1024, 4, 2, 1),  # 1024x8x8
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.2),
        )
        
        # è§£ç å™¨ - ç”ŸæˆåŠ¨æ¼«é£æ ¼å›¾åƒ
        self.decoder = nn.Sequential(
            # è¾“å…¥: 1024x8x8
            nn.ConvTranspose2d(1024, 512, 4, 2, 1),  # 512x16x16
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # 256x32x32
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 128x64x64
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 64x128x128
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, 4, 2, 1),  # 3x256x256
            nn.Tanh(),
        )
        
        # åˆ¤åˆ«å™¨ - åˆ¤æ–­æ˜¯å¦ä¸ºåŠ¨æ¼«é£æ ¼
        self.discriminator = nn.Sequential(
            # è¾“å…¥: 3x256x256
            nn.Conv2d(3, 64, 4, 2, 1),  # 64x128x128
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1),  # 128x64x64
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, 2, 1),  # 256x32x32
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
            nn.Conv2d(256, 512, 4, 2, 1),  # 512x16x16
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),
            nn.Conv2d(512, 1, 4, 2, 1),  # 1x8x8
            nn.Sigmoid(),
        )
    
    def forward(self, x):
        # ç¼–ç å™¨æå–ç‰¹å¾
        encoded = self.encoder(x)
        # è§£ç å™¨ç”ŸæˆåŠ¨æ¼«é£æ ¼
        decoded = self.decoder(encoded)
        return decoded
    
    def discriminate(self, x):
        return self.discriminator(x)

class AnimeStyleTransfer:
    """åŸºäºæ·±åº¦å­¦ä¹ çš„åŠ¨æ¼«é£æ ¼è½¬æ¢"""
    
    def __init__(self, use_pretrained=True):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self._load_model(use_pretrained)
        
        # åŠ¨æ¼«é£æ ¼ç‰¹å¾æå–å™¨
        self.style_extractor = self._load_style_extractor()
        
        # é¢éƒ¨ç‰¹å¾æ£€æµ‹å™¨
        self.face_detector = self._load_face_detector()
        
        # åŠ¨æ¼«é£æ ¼ç‰¹å¾åº“
        self.anime_features = self._load_anime_features()
        
    def _load_model(self, use_pretrained):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æˆ–åˆ›å»ºæ–°æ¨¡å‹"""
        if use_pretrained:
            # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            model_path = "models/anime_style_gan.pth"
            if os.path.exists(model_path):
                try:
                    model = AnimeStyleGAN()
                    model.load_state_dict(torch.load(model_path, map_location=self.device))
                    model.to(self.device)
                    model.eval()
                    print("âœ… åŠ è½½é¢„è®­ç»ƒåŠ¨æ¼«é£æ ¼æ¨¡å‹æˆåŠŸ")
                    return model
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
        
        # åˆ›å»ºæ–°æ¨¡å‹
        model = AnimeStyleGAN().to(self.device)
        print("ğŸ†• åˆ›å»ºæ–°çš„åŠ¨æ¼«é£æ ¼æ¨¡å‹")
        return model
    
    def _load_style_extractor(self):
        """åŠ è½½é£æ ¼ç‰¹å¾æå–å™¨"""
        try:
            # ä½¿ç”¨VGG19æå–é£æ ¼ç‰¹å¾
            vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features
            for param in vgg.parameters():
                param.requires_grad = False
            return vgg.to(self.device)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é£æ ¼æå–å™¨å¤±è´¥: {e}")
            return None
    
    def _load_face_detector(self):
        """åŠ è½½é¢éƒ¨ç‰¹å¾æ£€æµ‹å™¨"""
        try:
            # ä½¿ç”¨OpenCVçš„äººè„¸æ£€æµ‹å™¨
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            return face_cascade
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é¢éƒ¨æ£€æµ‹å™¨å¤±è´¥: {e}")
            return None
    
    def _load_anime_features(self):
        """åŠ è½½åŠ¨æ¼«é£æ ¼ç‰¹å¾åº“"""
        # åŠ¨æ¼«é£æ ¼ç‰¹å¾ï¼šå¤§çœ¼ã€å°é¼»ã€é²œè‰³è‰²å½©ã€æ¸…æ™°çº¿æ¡
        anime_features = {
            'eye_ratio': 0.15,  # çœ¼ç›å é¢éƒ¨æ¯”ä¾‹ï¼ˆåŠ¨æ¼«é€šå¸¸æ›´å¤§ï¼‰
            'nose_ratio': 0.05,  # é¼»å­å é¢éƒ¨æ¯”ä¾‹ï¼ˆåŠ¨æ¼«é€šå¸¸æ›´å°ï¼‰
            'saturation_boost': 0.3,  # é¥±å’Œåº¦å¢å¼º
            'contrast_boost': 0.2,  # å¯¹æ¯”åº¦å¢å¼º
            'edge_strength': 0.15,  # è¾¹ç¼˜å¼ºåº¦
        }
        return anime_features
    
    def _preprocess_image(self, image, target_size=256):
        """é¢„å¤„ç†å›¾åƒ"""
        transform = transforms.Compose([
            transforms.Resize(target_size),
            transforms.CenterCrop(target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
        return transform(image).unsqueeze(0).to(self.device)
    
    def _postprocess_image(self, tensor):
        """åå¤„ç†å¼ é‡ä¸ºå›¾åƒ"""
        tensor = tensor.squeeze(0).cpu()
        tensor = tensor * 0.5 + 0.5  # åå½’ä¸€åŒ–
        tensor = torch.clamp(tensor, 0, 1)
        
        transform = transforms.ToPILImage()
        return transform(tensor)
    
    def _detect_faces(self, image):
        """æ£€æµ‹é¢éƒ¨ç‰¹å¾"""
        if self.face_detector is None:
            return []
        
        # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œäººè„¸æ£€æµ‹
        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)
        faces = self.face_detector.detectMultiScale(gray, 1.1, 4)
        
        face_features = []
        for (x, y, w, h) in faces:
            # è®¡ç®—é¢éƒ¨ç‰¹å¾æ¯”ä¾‹
            features = {
                'bbox': (x, y, w, h),
                'eye_region': (x + w//4, y + h//3, w//2, h//3),  # çœ¼ç›åŒºåŸŸ
                'nose_region': (x + w//3, y + h//2, w//3, h//4),  # é¼»å­åŒºåŸŸ
                'face_ratio': w / h  # é¢éƒ¨å®½é«˜æ¯”
            }
            face_features.append(features)
        
        return face_features
    
    def _apply_anime_face_features(self, image, face_features):
        """åº”ç”¨åŠ¨æ¼«é¢éƒ¨ç‰¹å¾"""
        img_array = np.array(image)
        
        for features in face_features:
            x, y, w, h = features['bbox']
            
            # 1. çœ¼ç›æ”¾å¤§ï¼ˆåŠ¨æ¼«ç‰¹å¾ï¼‰
            eye_x, eye_y, eye_w, eye_h = features['eye_region']
            eyes = img_array[eye_y:eye_y+eye_h, eye_x:eye_x+eye_w]
            
            if eyes.size > 0:
                # æ”¾å¤§çœ¼ç›åŒºåŸŸ
                new_eye_h = int(eye_h * 1.3)  # æ”¾å¤§30%
                new_eye_w = int(eye_w * 1.2)
                eyes_resized = cv2.resize(eyes, (new_eye_w, new_eye_h))
                
                # è®¡ç®—æ–°çš„çœ¼ç›ä½ç½®ï¼ˆå±…ä¸­ï¼‰
                new_eye_x = eye_x - (new_eye_w - eye_w) // 2
                new_eye_y = eye_y - (new_eye_h - eye_h) // 3
                
                # ç¡®ä¿ä¸è¶Šç•Œ
                new_eye_x = max(0, new_eye_x)
                new_eye_y = max(0, new_eye_y)
                
                # æ›¿æ¢çœ¼ç›åŒºåŸŸ
                end_y = min(new_eye_y + new_eye_h, img_array.shape[0])
                end_x = min(new_eye_x + new_eye_w, img_array.shape[1])
                
                actual_h = end_y - new_eye_y
                actual_w = end_x - new_eye_x
                
                if actual_h > 0 and actual_w > 0:
                    img_array[new_eye_y:end_y, new_eye_x:end_x] = eyes_resized[:actual_h, :actual_w]
            
            # 2. é¼»å­ç¼©å°ï¼ˆåŠ¨æ¼«ç‰¹å¾ï¼‰
            nose_x, nose_y, nose_w, nose_h = features['nose_region']
            if nose_h > 10 and nose_w > 10:  # ç¡®ä¿é¼»å­åŒºåŸŸè¶³å¤Ÿå¤§
                nose = img_array[nose_y:nose_y+nose_h, nose_x:nose_x+nose_w]
                
                # ç¼©å°é¼»å­
                new_nose_h = max(5, int(nose_h * 0.7))  # ç¼©å°30%
                new_nose_w = max(5, int(nose_w * 0.7))
                nose_resized = cv2.resize(nose, (new_nose_w, new_nose_h))
                
                # å±…ä¸­æ”¾ç½®
                new_nose_x = nose_x + (nose_w - new_nose_w) // 2
                new_nose_y = nose_y + (nose_h - new_nose_h) // 2
                
                img_array[new_nose_y:new_nose_y+new_nose_h, new_nose_x:new_nose_x+new_nose_w] = nose_resized
        
        return Image.fromarray(img_array)
    
    def _enhance_anime_features(self, image):
        """å¢å¼ºåŠ¨æ¼«ç‰¹å¾"""
        img_array = np.array(image)
        
        # 1. å¢å¼ºé¥±å’Œåº¦ï¼ˆåŠ¨æ¼«è‰²å½©é²œè‰³ï¼‰
        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
        h, s, v = cv2.split(hsv)
        s = cv2.add(s, int(255 * self.anime_features['saturation_boost']))
        s = np.clip(s, 0, 255)
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2RGB)
        
        # 2. å¢å¼ºå¯¹æ¯”åº¦
        lab = cv2.cvtColor(enhanced, cv2.COLOR_RGB2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2RGB)
        
        # 3. å¢å¼ºè¾¹ç¼˜ï¼ˆåŠ¨æ¼«çº¿æ¡æ¸…æ™°ï¼‰
        gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
        
        # å åŠ è¾¹ç¼˜
        result = cv2.addWeighted(enhanced, 0.85, edges_colored, 0.15, 0)
        
        return Image.fromarray(result)
    
    def transfer_to_anime(self, content_image, enhance_features=True):
        """å°†çœŸå®ç…§ç‰‡è½¬æ¢ä¸ºåŠ¨æ¼«é£æ ¼"""
        print("ğŸ¨ å¼€å§‹çœŸæ­£çš„åŠ¨æ¼«é£æ ¼è½¬æ¢...")
        
        try:
            # 1. æ£€æµ‹é¢éƒ¨ç‰¹å¾
            face_features = self._detect_faces(content_image)
            print(f"ğŸ‘¤ æ£€æµ‹åˆ° {len(face_features)} ä¸ªé¢éƒ¨")
            
            # 2. ä½¿ç”¨GANæ¨¡å‹è¿›è¡Œé£æ ¼è½¬æ¢
            content_tensor = self._preprocess_image(content_image)
            
            with torch.no_grad():
                anime_tensor = self.model(content_tensor)
            
            anime_image = self._postprocess_image(anime_tensor)
            
            # 3. åº”ç”¨åŠ¨æ¼«é¢éƒ¨ç‰¹å¾
            if face_features and enhance_features:
                anime_image = self._apply_anime_face_features(anime_image, face_features)
            
            # 4. å¢å¼ºåŠ¨æ¼«ç‰¹å¾
            if enhance_features:
                anime_image = self._enhance_anime_features(anime_image)
            
            print("âœ… åŠ¨æ¼«é£æ ¼è½¬æ¢å®Œæˆ")
            return anime_image
            
        except Exception as e:
            print(f"âŒ åŠ¨æ¼«é£æ ¼è½¬æ¢å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            return self._fallback_traditional_method(content_image)
    
    def _fallback_traditional_method(self, image):
        """å¤‡é€‰ä¼ ç»Ÿæ–¹æ³•"""
        print("âš ï¸ ä½¿ç”¨å¤‡é€‰ä¼ ç»ŸåŠ¨æ¼«é£æ ¼è½¬æ¢")
        
        img_array = np.array(image)
        
        # ä½¿ç”¨æ”¹è¿›çš„åŠ¨æ¼«é£æ ¼æ»¤é•œ
        # 1. æ·±åº¦è¾¹ç¼˜ä¿ç•™å¹³æ»‘
        filtered = cv2.bilateralFilter(img_array, d=15, sigmaColor=100, sigmaSpace=100)
        
        # 2. å¼ºçƒˆçš„é¢œè‰²é‡åŒ–
        Z = filtered.reshape((-1, 3))
        Z = np.float32(Z)
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        K = 8
        _, labels, centers = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        centers = np.uint8(centers)
        cartoon = centers[labels.flatten()].reshape(filtered.shape)
        
        # 3. æ¸…æ™°çš„è¾¹ç¼˜æ£€æµ‹
        gray = cv2.cvtColor(cartoon, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, 30, 100)
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
        
        # 4. å åŠ è¾¹ç¼˜
        result = cv2.addWeighted(cartoon, 0.8, edges_colored, 0.2, 0)
        
        return Image.fromarray(result)

# åˆ›å»ºå…¨å±€åŠ¨æ¼«é£æ ¼è½¬æ¢å™¨
anime_style_transfer = AnimeStyleTransfer()