#!/usr/bin/env python3
"""
AnimeGANå¤„ç†å™¨æ¨¡å—
é›†æˆé¢„è®­ç»ƒçš„AnimeGANæ¨¡å‹ï¼Œå®ç°ç«¯åˆ°ç«¯åŠ¨æ¼«é£æ ¼è½¬æ¢
ä¸ºå®«å´éªé£æ ¼ä¸“ç”¨GANå¥ å®šåŸºç¡€
"""

import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import torchvision.utils as vutils
import os
import time
import requests
from urllib.parse import urlparse
import hashlib

class AnimeGANProcessor:
    """AnimeGANå¤„ç†å™¨ - ç«¯åˆ°ç«¯åŠ¨æ¼«é£æ ¼è½¬æ¢"""
    
    def __init__(self, model_type='v2'):
        """
        åˆå§‹åŒ–AnimeGANå¤„ç†å™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('v1', 'v2', 'v3', 'hayao', 'shinkai', 'paprika')
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_type = model_type
        self.model = None
        self.transform = None
        
        # æ¨¡å‹é…ç½® - ä½¿ç”¨æ›´è½»é‡çº§çš„æœ¬åœ°æ¨¡å‹
        self.model_configs = {
            'hayao': {
                'name': 'AnimeGANv2_Hayao',
                'style': 'hayao_ghibli',
                'url': 'https://github.com/TachibanaYoshino/AnimeGANv2/releases/download/1.0/Hayao.tar.gz',
                'size': 512,
                'filename': 'generator_Hayao.pth',  # tar.gzä¸­çš„æ¨¡å‹æ–‡ä»¶å
                'use_local': True  # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è½»é‡çº§æ¨¡å‹
            },
            'shinkai': {
                'name': 'AnimeGANv2_Shinkai',
                'style': 'shinkai_makoto',
                'url': 'https://github.com/TachibanaYoshino/AnimeGANv2/releases/download/1.0/Shinkai.tar.gz',
                'size': 512,
                'filename': 'generator_Shinkai.pth',
                'use_local': True
            },
            'paprika': {
                'name': 'AnimeGANv2_Paprika',
                'style': 'paprika_satoshi',
                'url': 'https://github.com/TachibanaYoshino/AnimeGANv2/releases/download/1.0/Paprika.tar.gz',
                'size': 512,
                'filename': 'generator_Paprika.pth',
                'use_local': True
            }
        }
        
        # åˆå§‹åŒ–å˜æ¢
        self._initialize_transform()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        print(f"ğŸ¨ AnimeGANå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_configs[model_type]['name']}")
    
    def _initialize_transform(self):
        """åˆå§‹åŒ–å›¾åƒé¢„å¤„ç†å˜æ¢"""
        self.transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    
    def _load_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„AnimeGANæ¨¡å‹"""
        try:
            # é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹
            model_path = self._get_local_model_path()
            
            if model_path and os.path.exists(model_path):
                print(f"ğŸ“¦ åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
                self.model = self._create_generator()
                
                # å¦‚æœæ˜¯é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ï¼ŒåŠ è½½å®ƒ
                if os.path.getsize(model_path) > 1000:  # è‡³å°‘1KB
                    try:
                        checkpoint = torch.load(model_path, map_location=self.device)
                        # å¤„ç†ä¸åŒçš„æ¨¡å‹æ ¼å¼
                        if 'generator' in checkpoint:
                            self.model.load_state_dict(checkpoint['generator'])
                        elif 'net_G' in checkpoint:
                            self.model.load_state_dict(checkpoint['net_G'])
                        else:
                            self.model.load_state_dict(checkpoint)
                        print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
                    except Exception as weight_error:
                        print(f"âš ï¸ é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {weight_error}")
                        # ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹
                else:
                    print("âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                
                self.model.to(self.device)
                self.model.eval()
                print("âœ… AnimeGANæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            else:
                # å°è¯•ä¸‹è½½æ¨¡å‹
                print("ğŸ“¥ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹è½½...")
                model_path = self._download_model()
                
                if model_path and os.path.exists(model_path):
                    print(f"ğŸ“¦ åŠ è½½ä¸‹è½½æ¨¡å‹: {model_path}")
                    self.model = self._create_generator()
                    checkpoint = torch.load(model_path, map_location=self.device)
                    
                    # å¤„ç†ä¸åŒçš„æ¨¡å‹æ ¼å¼
                    if 'generator' in checkpoint:
                        self.model.load_state_dict(checkpoint['generator'])
                    elif 'net_G' in checkpoint:
                        self.model.load_state_dict(checkpoint['net_G'])
                    else:
                        self.model.load_state_dict(checkpoint)
                    
                    self.model.to(self.device)
                    self.model.eval()
                    print("âœ… AnimeGANæ¨¡å‹åŠ è½½æˆåŠŸ")
                else:
                    print("âŒ æ— æ³•ä¸‹è½½AnimeGANæ¨¡å‹ï¼Œå°†ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                    self.model = None
                
        except Exception as e:
            print(f"âŒ AnimeGANæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def _get_local_model_path(self):
        """è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„"""
        config = self.model_configs.get(self.model_type)
        if not config:
            return None
        
        model_dir = "models/anime_gan"
        os.makedirs(model_dir, exist_ok=True)
        
        # ä½¿ç”¨å›ºå®šçš„æ–‡ä»¶å
        model_filename = f"{config['name']}.pth"
        model_path = os.path.join(model_dir, model_filename)
        
        # å¦‚æœæ¨¡å‹å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if os.path.exists(model_path):
            return model_path
        
        # åˆ›å»ºä¸€ä¸ªè½»é‡çº§çš„æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼ˆå ä½ç¬¦ï¼‰
        try:
            print(f"ğŸ”§ åˆ›å»ºæœ¬åœ°æ¨¡å‹å ä½ç¬¦: {model_path}")
            
            # ä¸´æ—¶åˆ›å»ºæ¨¡å‹æ¥ä¿å­˜å ä½ç¬¦
            temp_model = self._create_generator()
            
            # ä¿å­˜æ¨¡å‹ç»“æ„ï¼ˆéšæœºåˆå§‹åŒ–çš„æƒé‡ï¼‰
            torch.save(temp_model.state_dict(), model_path)
            print(f"âœ… æœ¬åœ°æ¨¡å‹å ä½ç¬¦åˆ›å»ºå®Œæˆ: {model_path}")
            return model_path
            
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºæœ¬åœ°æ¨¡å‹å ä½ç¬¦å¤±è´¥: {e}")
            return None
    
    def _download_model(self):
        """ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹"""
        config = self.model_configs.get(self.model_type)
        if not config:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
            return None
        
        model_dir = "models/anime_gan"
        os.makedirs(model_dir, exist_ok=True)
        
        # ä½¿ç”¨å›ºå®šçš„æ–‡ä»¶åï¼Œä¸ä½¿ç”¨URLå“ˆå¸Œ
        model_filename = f"{config['name']}.pth"
        model_path = os.path.join(model_dir, model_filename)
        
        # å¦‚æœæ¨¡å‹å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if os.path.exists(model_path):
            print(f"ğŸ“¦ æ¨¡å‹å·²å­˜åœ¨: {model_path}")
            return model_path
        
        print(f"â¬‡ï¸  å¼€å§‹ä¸‹è½½æ¨¡å‹: {config['name']}")
        print(f"ğŸ“¥ ä¸‹è½½åœ°å€: {config['url']}")
        
        try:
            import tarfile
            import tempfile
            
            # ä¸‹è½½tar.gzæ–‡ä»¶
            response = requests.get(config['url'], stream=True, timeout=60)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            # ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.tar.gz', delete=False) as temp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        temp_file.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"\râ¬‡ï¸  ä¸‹è½½è¿›åº¦: {progress:.1f}%", end='')
                
                temp_path = temp_file.name
            
            print(f"\nğŸ“¦ è§£å‹æ¨¡å‹æ–‡ä»¶...")
            
            # è§£å‹tar.gzæ–‡ä»¶
            with tarfile.open(temp_path, 'r:gz') as tar:
                # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
                all_files = tar.getnames()
                print(f"ğŸ“‹ å‹ç¼©åŒ…å†…å®¹: {all_files}")
                
                # æŸ¥æ‰¾.pthæ–‡ä»¶ - æ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶
                pth_files = [member for member in tar.getmembers() if member.name.endswith('.pth')]
                
                if pth_files:
                    # ä¼˜å…ˆæŸ¥æ‰¾åŒ…å«generatorçš„æ–‡ä»¶
                    generator_files = [member for member in pth_files if 'generator' in member.name.lower()]
                    
                    if generator_files:
                        selected_file = generator_files[0]
                        print(f"âœ… æ‰¾åˆ°generatoræ–‡ä»¶: {selected_file.name}")
                    else:
                        selected_file = pth_files[0]
                        print(f"âœ… æ‰¾åˆ°.pthæ–‡ä»¶: {selected_file.name}")
                    
                    # æå–é€‰å®šçš„.pthæ–‡ä»¶
                    extracted_file = tar.extractfile(selected_file)
                    if extracted_file:
                        with open(model_path, 'wb') as f:
                            f.write(extracted_file.read())
                        print(f"âœ… æ¨¡å‹æå–å®Œæˆ: {model_path}")
                    else:
                        print(f"âŒ æ— æ³•æå–æ–‡ä»¶: {selected_file.name}")
                        return None
                else:
                    print(f"âŒ å‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°ä»»ä½•.pthæ–‡ä»¶")
                    print(f"ğŸ“‹ å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶: {all_files}")
                    return None
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)
            
            if os.path.exists(model_path):
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(model_path)
                if file_size > 1000:  # è‡³å°‘1KB
                    print(f"âœ… æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œå¤§å°: {file_size / (1024*1024):.1f}MB")
                    return model_path
                else:
                    print(f"âŒ æ¨¡å‹æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æŸå: {file_size} bytes")
                    os.remove(model_path)
                    return None
            else:
                print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¿å­˜å¤±è´¥")
                return None
            
        except Exception as e:
            print(f"\nâŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            # åˆ é™¤éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
            if os.path.exists(model_path):
                os.remove(model_path)
            return None
    
    def _create_generator(self):
        """åˆ›å»ºAnimeGANç”Ÿæˆå™¨ç½‘ç»œ"""
        class AnimeGANGenerator(nn.Module):
            def __init__(self):
                super(AnimeGANGenerator, self).__init__()
                
                # ç¼–ç å™¨éƒ¨åˆ†
                self.encoder = nn.Sequential(
                    # è¾“å…¥: 3 x 512 x 512
                    nn.Conv2d(3, 64, 7, 1, 3),
                    nn.InstanceNorm2d(64),
                    nn.ReLU(inplace=True),
                    
                    # ä¸‹é‡‡æ ·å±‚
                    nn.Conv2d(64, 128, 4, 2, 1),
                    nn.InstanceNorm2d(128),
                    nn.ReLU(inplace=True),
                    
                    nn.Conv2d(128, 256, 4, 2, 1),
                    nn.InstanceNorm2d(256),
                    nn.ReLU(inplace=True),
                    
                    nn.Conv2d(256, 512, 4, 2, 1),
                    nn.InstanceNorm2d(512),
                    nn.ReLU(inplace=True),
                    
                    nn.Conv2d(512, 512, 4, 2, 1),
                    nn.InstanceNorm2d(512),
                    nn.ReLU(inplace=True),
                )
                
                # æ®‹å·®å—
                self.residual_blocks = nn.Sequential(*[
                    self._make_residual_block(512) for _ in range(6)
                ])
                
                # è§£ç å™¨éƒ¨åˆ†
                self.decoder = nn.Sequential(
                    # ä¸Šé‡‡æ ·å±‚
                    nn.ConvTranspose2d(512, 512, 4, 2, 1),
                    nn.InstanceNorm2d(512),
                    nn.ReLU(inplace=True),
                    
                    nn.ConvTranspose2d(512, 256, 4, 2, 1),
                    nn.InstanceNorm2d(256),
                    nn.ReLU(inplace=True),
                    
                    nn.ConvTranspose2d(256, 128, 4, 2, 1),
                    nn.InstanceNorm2d(128),
                    nn.ReLU(inplace=True),
                    
                    nn.ConvTranspose2d(128, 64, 4, 2, 1),
                    nn.InstanceNorm2d(64),
                    nn.ReLU(inplace=True),
                    
                    # è¾“å‡ºå±‚
                    nn.Conv2d(64, 3, 7, 1, 3),
                    nn.Tanh()
                )
            
            def _make_residual_block(self, dim):
                return nn.Sequential(
                    nn.Conv2d(dim, dim, 3, 1, 1),
                    nn.InstanceNorm2d(dim),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(dim, dim, 3, 1, 1),
                    nn.InstanceNorm2d(dim)
                )
            
            def forward(self, x):
                x = self.encoder(x)
                x = self.residual_blocks(x)
                x = self.decoder(x)
                return x
        
        return AnimeGANGenerator()
    
    def preprocess_image(self, image):
        """
        é¢„å¤„ç†è¾“å…¥å›¾åƒ
        
        Args:
            image: PILå›¾åƒæˆ–numpyæ•°ç»„
            
        Returns:
            tensor: é¢„å¤„ç†åçš„tensor
        """
        if isinstance(image, np.ndarray):
            # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºPILå›¾åƒ
            if len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        
        # åº”ç”¨å˜æ¢
        tensor = self.transform(image)
        return tensor.unsqueeze(0)
    
    def postprocess_output(self, output_tensor, original_size=None):
        """
        åå¤„ç†è¾“å‡ºtensor
        
        Args:
            output_tensor: æ¨¡å‹è¾“å‡ºtensor
            original_size: åŸå§‹å›¾åƒå°ºå¯¸ (width, height)
            
        Returns:
            PILå›¾åƒ
        """
        # ç§»é™¤batchç»´åº¦ï¼Œè½¬æ¢åˆ°CPU
        output = output_tensor.squeeze(0).cpu()
        
        # åå½’ä¸€åŒ–
        output = (output + 1) / 2
        output = torch.clamp(output, 0, 1)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        transform = transforms.ToPILImage()
        image = transform(output)
        
        # æ¢å¤åŸå§‹å°ºå¯¸
        if original_size:
            image = image.resize(original_size, Image.LANCZOS)
        
        return image
    
    def convert_to_anime(self, image, progress_callback=None):
        """
        å°†çœŸå®ç…§ç‰‡è½¬æ¢ä¸ºåŠ¨æ¼«é£æ ¼
        
        Args:
            image: è¾“å…¥å›¾åƒ (PILæˆ–numpy)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            è½¬æ¢åçš„PILå›¾åƒ
        """
        if self.model is None:
            print("âš ï¸ AnimeGANæ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
            return self._fallback_conversion(image)
        
        try:
            print("ğŸ¨ ä½¿ç”¨AnimeGANè¿›è¡ŒåŠ¨æ¼«é£æ ¼è½¬æ¢...")
            
            if progress_callback:
                progress_callback("preprocessing", 10)
            
            # è·å–åŸå§‹å°ºå¯¸
            if isinstance(image, Image.Image):
                original_size = image.size
            else:
                original_size = (image.shape[1], image.shape[0])
            
            # é¢„å¤„ç†
            input_tensor = self.preprocess_image(image).to(self.device)
            
            if progress_callback:
                progress_callback("processing", 30)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output_tensor = self.model(input_tensor)
            
            if progress_callback:
                progress_callback("postprocessing", 80)
            
            # åå¤„ç†
            result_image = self.postprocess_output(output_tensor, original_size)
            
            if progress_callback:
                progress_callback("complete", 100)
            
            print("âœ… AnimeGANè½¬æ¢å®Œæˆ")
            return result_image
            
        except Exception as e:
            print(f"âŒ AnimeGANè½¬æ¢å¤±è´¥: {e}")
            return self._fallback_conversion(image)
    
    def _fallback_conversion(self, image):
        """
        å›é€€è½¬æ¢æ–¹æ¡ˆ - ä½¿ç”¨ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³•
        
        Args:
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            è½¬æ¢åçš„å›¾åƒ
        """
        print("ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆè¿›è¡ŒåŠ¨æ¼«é£æ ¼è½¬æ¢")
        
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(image, Image.Image):
                img_array = np.array(image)
                img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
            else:
                img_bgr = image.copy()
            
            # åº”ç”¨åŸºç¡€çš„åŠ¨æ¼«åŒ–å¤„ç†
            # 1. åŒè¾¹æ»¤æ³¢
            bilateral = cv2.bilateralFilter(img_bgr, 15, 80, 80)
            
            # 2. è¾¹ç¼˜æ£€æµ‹
            gray = cv2.cvtColor(bilateral, cv2.COLOR_BGR2GRAY)
            edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 9)
            edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
            
            # 3. é¢œè‰²é‡åŒ–
            data = bilateral.reshape((-1, 3))
            data = np.float32(data)
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
            _, labels, centers = cv2.kmeans(data, 16, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
            centers = np.uint8(centers)
            quantized = centers[labels.flatten()].reshape(bilateral.shape)
            
            # 4. ç»„åˆç»“æœ
            result = cv2.addWeighted(quantized, 0.8, edges_colored, 0.2, 0)
            
            # 5. è‰²å½©è°ƒæ•´
            hsv = cv2.cvtColor(result, cv2.COLOR_BGR2HSV)
            hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.3, 0, 255)
            hsv[:, :, 2] = np.clip(hsv[:, :, 2] * 1.1, 0, 255)
            result = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
            return Image.fromarray(result_rgb)
            
        except Exception as e:
            print(f"âŒ å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
            # è¿”å›åŸå›¾
            if isinstance(image, Image.Image):
                return image
            else:
                return Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    def get_style_info(self):
        """è·å–å½“å‰æ¨¡å‹é£æ ¼ä¿¡æ¯"""
        config = self.model_configs.get(self.model_type, {})
        return {
            'name': config.get('name', 'Unknown'),
            'style': config.get('style', 'Unknown'),
            'size': config.get('size', 512),
            'available': self.model is not None
        }
    
    def switch_model(self, model_type):
        """åˆ‡æ¢æ¨¡å‹ç±»å‹"""
        if model_type in self.model_configs:
            self.model_type = model_type
            self.model = None
            self._load_model()
            print(f"ğŸ”„ åˆ‡æ¢åˆ°æ¨¡å‹: {self.model_configs[model_type]['name']}")
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

# åˆ›å»ºå…¨å±€AnimeGANå¤„ç†å™¨å®ä¾‹
anime_gan_processor = AnimeGANProcessor(model_type='hayao')  # é»˜è®¤ä½¿ç”¨å®«å´éªé£æ ¼

def convert_with_anime_gan(image, model_type='hayao', progress_callback=None):
    """
    ä½¿ç”¨AnimeGANè½¬æ¢å›¾åƒçš„ä¾¿æ·å‡½æ•°
    
    Args:
        image: è¾“å…¥å›¾åƒ
        model_type: æ¨¡å‹ç±»å‹
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        è½¬æ¢åçš„PILå›¾åƒ
    """
    global anime_gan_processor
    
    # å¦‚æœéœ€è¦åˆ‡æ¢æ¨¡å‹
    if anime_gan_processor.model_type != model_type:
        anime_gan_processor.switch_model(model_type)
    
    return anime_gan_processor.convert_to_anime(image, progress_callback)