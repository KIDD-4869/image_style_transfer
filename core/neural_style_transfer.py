#!/usr/bin/env python3
"""
ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»æ¨¡å— - åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„å…ˆè¿›é£æ ¼è¿ç§»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import torch.optim as optim
import numpy as np
from PIL import Image
import cv2
import os

class NeuralStyleTransfer:
    """åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»"""
    
    def __init__(self, model_type='vgg19'):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_type = model_type
        self.model = self._load_pretrained_model(model_type)
        
        # å®šä¹‰é£æ ¼å’Œå†…å®¹å±‚
        if model_type == 'vgg19':
            self.style_layers = ['0', '5', '10', '19', '28']  # æ›´ä¸°å¯Œçš„é£æ ¼ç‰¹å¾
            self.content_layers = ['21']  # æ›´æ·±å±‚çš„å†…å®¹ç‰¹å¾
        elif model_type == 'resnet50':
            self.style_layers = ['layer1', 'layer2', 'layer3', 'layer4']
            self.content_layers = ['layer4']
        
        # é¢„å®šä¹‰çš„å®«å´éªé£æ ¼ç‰¹å¾
        self.ghibli_style_features = None
        
    def _load_pretrained_model(self, model_type):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸ¯ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_type}")
        
        if model_type == 'vgg19':
            try:
                model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features
            except AttributeError:
                model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features
        elif model_type == 'resnet50':
            try:
                model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
            except AttributeError:
                model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # å†»ç»“æ¨¡å‹å‚æ•°
        for param in model.parameters():
            param.requires_grad = False
        
        return model.to(self.device)
    
    def _extract_features(self, x, layers):
        """ä»æ¨¡å‹ä¸­æå–ç‰¹å¾"""
        features = {}
        
        if self.model_type == 'vgg19':
            for name, layer in self.model._modules.items():
                x = layer(x)
                if name in layers:
                    features[name] = x
        elif self.model_type == 'resnet50':
            # ResNetç‰¹å¾æå–
            x = self.model.conv1(x)
            x = self.model.bn1(x)
            x = self.model.relu(x)
            x = self.model.maxpool(x)
            
            x = self.model.layer1(x)
            if 'layer1' in layers:
                features['layer1'] = x
            
            x = self.model.layer2(x)
            if 'layer2' in layers:
                features['layer2'] = x
            
            x = self.model.layer3(x)
            if 'layer3' in layers:
                features['layer3'] = x
            
            x = self.model.layer4(x)
            if 'layer4' in layers:
                features['layer4'] = x
        
        return features
    
    def _gram_matrix(self, x):
        """è®¡ç®—GramçŸ©é˜µï¼ˆé£æ ¼ç‰¹å¾ï¼‰"""
        batch_size, channels, height, width = x.size()
        features = x.view(batch_size * channels, height * width)
        gram = torch.mm(features, features.t())
        return gram.div(batch_size * channels * height * width)
    
    def _preprocess_image(self, image, target_size=512):
        """é¢„å¤„ç†å›¾åƒ - æ™ºèƒ½å¤„ç†å¤§å°ºå¯¸å›¾ç‰‡"""
        # è·å–åŸå§‹å°ºå¯¸
        original_size = image.size
        
        # æ™ºèƒ½è°ƒæ•´ç›®æ ‡å°ºå¯¸ï¼Œé¿å…å†…å­˜æº¢å‡º
        max_allowed_size = 1024  # æœ€å¤§å¤„ç†å°ºå¯¸
        
        # å¦‚æœå›¾ç‰‡å°ºå¯¸è¿‡å¤§ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾
        if max(original_size) > max_allowed_size:
            scale = max_allowed_size / max(original_size)
            target_size = (int(original_size[0] * scale), int(original_size[1] * scale))
            print(f"ğŸ“ å›¾ç‰‡å°ºå¯¸è¿‡å¤§ï¼Œè‡ªåŠ¨ç¼©æ”¾è‡³: {target_size[0]}x{target_size[1]}")
        else:
            # ä¿æŒåŸå§‹å°ºå¯¸æˆ–ä½¿ç”¨é»˜è®¤å°ºå¯¸
            target_size = min(max_allowed_size, max(original_size))
        
        transform = transforms.Compose([
            transforms.Resize(target_size),
            transforms.CenterCrop(target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).unsqueeze(0).to(self.device)
    
    def _postprocess_image(self, tensor):
        """åå¤„ç†å¼ é‡ä¸ºå›¾åƒ"""
        tensor = tensor.squeeze(0).cpu()
        tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        tensor = torch.clamp(tensor, 0, 1)
        
        transform = transforms.ToPILImage()
        return transform(tensor)
    
    def _load_ghibli_style_reference(self):
        """åŠ è½½å®«å´éªé£æ ¼å‚è€ƒå›¾åƒ"""
        style_folder = 'ghibli_images'
        style_images = []
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        
        for ext in image_extensions:
            import glob
            pattern = os.path.join(style_folder, ext)
            style_images.extend(glob.glob(pattern))
        
        if not style_images:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤é£æ ¼")
            return None
        
        print(f"ğŸ¨ åŠ è½½äº† {len(style_images)} å¼ å®«å´éªé£æ ¼å‚è€ƒå›¾ç‰‡")
        return style_images
    
    def _extract_ghibli_style_features(self, target_size=512):
        """æå–å®«å´éªé£æ ¼ç‰¹å¾"""
        if self.ghibli_style_features is not None:
            return self.ghibli_style_features
        
        style_images = self._load_ghibli_style_reference()
        
        if not style_images:
            # å¦‚æœæ²¡æœ‰å‚è€ƒå›¾ç‰‡ï¼Œåˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾
            return self._create_default_ghibli_style()
        
        # å¤„ç†æ‰€æœ‰é£æ ¼å›¾ç‰‡å¹¶æå–ç‰¹å¾
        style_features_list = []
        
        for style_path in style_images:
            try:
                style_img = Image.open(style_path).convert('RGB')
                style_tensor = self._preprocess_image(style_img, target_size)
                
                # æå–é£æ ¼ç‰¹å¾
                features = self._extract_features(style_tensor, self.style_layers)
                style_features = {}
                
                for layer, feature in features.items():
                    style_features[layer] = self._gram_matrix(feature)
                
                style_features_list.append(style_features)
                
            except Exception as e:
                print(f"âŒ å¤„ç†é£æ ¼å›¾ç‰‡ {style_path} å¤±è´¥: {e}")
        
        if not style_features_list:
            return self._create_default_ghibli_style()
        
        # å¹³å‡æ‰€æœ‰é£æ ¼å›¾ç‰‡çš„ç‰¹å¾
        avg_style_features = {}
        for layer in self.style_layers:
            layer_features = []
            for style_features in style_features_list:
                if layer in style_features:
                    layer_features.append(style_features[layer])
            
            if layer_features:
                avg_style_features[layer] = torch.stack(layer_features).mean(dim=0)
        
        self.ghibli_style_features = avg_style_features
        return avg_style_features
    
    def _create_default_ghibli_style(self):
        """åˆ›å»ºé»˜è®¤çš„å®«å´éªé£æ ¼ç‰¹å¾"""
        print("ğŸ¨ ä½¿ç”¨é»˜è®¤å®«å´éªé£æ ¼ç‰¹å¾")
        
        # åˆ›å»ºå…·æœ‰å®«å´éªé£æ ¼ç‰¹å¾çš„é»˜è®¤é£æ ¼
        # å®«å´éªé£æ ¼ç‰¹ç‚¹ï¼šæŸ”å’Œè‰²å½©ã€æ¢¦å¹»å…‰å½±ã€ç®€æ´çº¿æ¡
        default_style = {}
        
        # è¿™é‡Œå¯ä»¥åŸºäºå®«å´éªçš„è‰ºæœ¯ç‰¹ç‚¹åˆ›å»ºé»˜è®¤é£æ ¼ç‰¹å¾
        # ç”±äºæ—¶é—´å…³ç³»ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
        
        return default_style
    
    def transfer_style(self, content_image, style_weight=1000000, content_weight=1, 
                      num_steps=300, learning_rate=0.02, tv_weight=1e-5):
        """æ‰§è¡Œé£æ ¼è¿ç§»"""
        print(f"ğŸ¯ å¼€å§‹ç¥ç»ç½‘ç»œé£æ ¼è¿ç§» (æ¨¡å‹: {self.model_type}, æ­¥éª¤: {num_steps})")
        
        # é¢„å¤„ç†å†…å®¹å›¾åƒ
        content_tensor = self._preprocess_image(content_image)
        
        # ä½¿ç”¨å†…å®¹å›¾åƒä½œä¸ºåˆå§‹è¾“å…¥
        input_img = content_tensor.clone().requires_grad_(True)
        
        # æå–å†…å®¹ç‰¹å¾
        content_features = self._extract_features(content_tensor, self.content_layers)
        
        # æå–å®«å´éªé£æ ¼ç‰¹å¾
        style_features = self._extract_ghibli_style_features()
        
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨
        optimizer = optim.Adam([input_img], lr=learning_rate)
        
        # è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆæ¥è‡ª RealGhibliStyleTransfer æ³¨å…¥ï¼‰
        progress_callback = self.progress_callback
        if progress_callback:
            # å‘å‡ºåˆå§‹è¿›åº¦ï¼Œé¿å…å‰ç«¯é•¿æ—¶é—´æ˜¾ç¤º0%
            try:
                progress_callback(1, 0, num_steps, 0.0)
            except Exception:
                pass
        
        for step in range(num_steps):
            optimizer.zero_grad()
            
            # è·å–å½“å‰è¾“å…¥çš„ç‰¹å¾
            features = self._extract_features(input_img, self.style_layers + self.content_layers)
            
            style_loss = 0
            content_loss = 0
            
            # è®¡ç®—é£æ ¼æŸå¤±
            for layer in self.style_layers:
                if layer in features and layer in style_features:
                    target_style = style_features[layer]
                    current_style = features[layer]
                    
                    target_gram = self._gram_matrix(target_style)
                    current_gram = self._gram_matrix(current_style)
                    
                    style_loss += F.mse_loss(current_gram, target_gram)
            
            # è®¡ç®—å†…å®¹æŸå¤±
            for layer in self.content_layers:
                if layer in features:
                    target_content = content_features[layer]
                    current_content = features[layer]
                    content_loss += F.mse_loss(current_content, target_content)
            
            # æ€»æŸå¤±ï¼ˆåŠ å…¥TVçº¦æŸï¼‰
            total_loss = style_weight * style_loss + content_weight * content_loss + tv_weight * self._tv_loss(input_img)
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºnanæˆ–inf
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"âš ï¸ æ­¥éª¤ {step+1}: æŸå¤±å€¼å¼‚å¸¸ (NaN/Inf)ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                continue
            
            total_loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸ºnanæˆ–inf
            if torch.isnan(input_img.grad).any() or torch.isinf(input_img.grad).any():
                print(f"âš ï¸ æ­¥éª¤ {step+1}: æ¢¯åº¦å¼‚å¸¸ (NaN/Inf)ï¼Œé‡ç½®æ¢¯åº¦")
                optimizer.zero_grad()
                continue
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_([input_img], max_norm=0.5)
            
            optimizer.step()
            
            # é™åˆ¶åƒç´ å€¼èŒƒå›´
            with torch.no_grad():
                input_img.data.clamp_(0, 1)
            
            # æ›´æ–°è¿›åº¦
            if progress_callback:
                progress = int((step + 1) / num_steps * 100)
                progress_callback(progress, step + 1, num_steps, total_loss.item())
            
            if (step + 1) % 50 == 0:
                print(f"æ­¥éª¤ {step+1}/{num_steps}, æ€»æŸå¤±: {total_loss.item():.4f}, "
                      f"é£æ ¼æŸå¤±: {style_loss.item():.4f}, å†…å®¹æŸå¤±: {content_loss.item():.4f}")
        
        # åå¤„ç†è¾“å‡ºå›¾åƒ
        output_tensor = input_img.data.clamp(0, 1)
        # æ¢å¤åˆ°åŸå§‹å°ºå¯¸ï¼Œå‡å°‘æ¨¡ç³Š
        original_size = content_image.size
        result_image = self._postprocess_image(output_tensor, original_size)
        
        print("âœ… ç¥ç»ç½‘ç»œé£æ ¼è¿ç§»å®Œæˆ")
        return result_image
    
    def fast_style_transfer(self, content_image, style_weight=500000, num_steps=100):
        """å¿«é€Ÿé£æ ¼è¿ç§» - ä¼˜åŒ–ç‰ˆæœ¬"""
        print("âš¡ æ‰§è¡Œå¿«é€Ÿé£æ ¼è¿ç§»")
        
        # ä½¿ç”¨æ›´å°‘çš„æ­¥éª¤å’Œæ›´é«˜çš„å­¦ä¹ ç‡
        return self.transfer_style(
            content_image, 
            style_weight=style_weight, 
            content_weight=1,
            num_steps=num_steps, 
            learning_rate=0.03
        )
    
    def set_progress_callback(self, callback):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback

# é¢„è®­ç»ƒæ¨¡å‹ç®¡ç†å™¨
class StyleTransferManager:
    """é£æ ¼è¿ç§»æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.models = {}
        self.current_model = None
    
    def load_model(self, model_type='vgg19'):
        """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        if model_type in self.models:
            self.current_model = self.models[model_type]
            return self.current_model
        
        try:
            model = NeuralStyleTransfer(model_type)
            self.models[model_type] = model
            self.current_model = model
            return model
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹ {model_type} å¤±è´¥: {e}")
            return None
    
    def get_available_models(self):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return ['vgg19', 'resnet50']
    
    def get_current_model(self):
        """è·å–å½“å‰æ¨¡å‹"""
        return self.current_model

# åˆ›å»ºå…¨å±€æ¨¡å‹ç®¡ç†å™¨
style_transfer_manager = StyleTransferManager()