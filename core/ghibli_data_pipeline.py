#!/usr/bin/env python3
"""
å®«å´éªé£æ ¼ä¸“ç”¨æ•°æ®ç®¡é“
ä¸ºè®­ç»ƒå®«å´éªé£æ ¼GANæ¨¡å‹å‡†å¤‡é«˜è´¨é‡æ•°æ®é›†
åŒ…å«æ•°æ®æ”¶é›†ã€é¢„å¤„ç†ã€å¢å¼ºã€åŠ è½½ç­‰åŠŸèƒ½
"""

import os
import cv2
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import json
import time
import glob
import random
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

class GhibliDataPipeline:
    """å®«å´éªé£æ ¼ä¸“ç”¨æ•°æ®ç®¡é“"""
    
    def __init__(self, config_path=None):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        
        # æ•°æ®ç›®å½•
        self.photo_dir = self.config.get('photo_dir', 'training_data/photos')
        self.ghibli_dir = self.config.get('ghibli_dir', 'ghibli_images')
        self.output_dir = self.config.get('output_dir', 'training_data/processed')
        
        # å¤„ç†å‚æ•°
        self.image_size = self.config.get('image_size', 512)
        self.quality_threshold = self.config.get('quality_threshold', 0.3)
        self.augmentation_enabled = self.config.get('augmentation_enabled', True)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.photo_dir, exist_ok=True)
        os.makedirs(self.ghibli_dir, exist_ok=True)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'photos_processed': 0,
            'ghibli_processed': 0,
            'pairs_created': 0,
            'errors': 0
        }
        
        print("ğŸ¨ å®«å´éªé£æ ¼æ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'photo_dir': 'training_data/photos',
            'ghibli_dir': 'ghibli_images', 
            'output_dir': 'training_data/processed',
            'image_size': 512,
            'quality_threshold': 0.3,
            'augmentation_enabled': True,
            'augmentation_params': {
                'rotation_range': 10,
                'brightness_range': 0.1,
                'contrast_range': 0.1,
                'saturation_range': 0.1
            },
            'preprocessing': {
                'face_detection': True,
                'blur_detection': True,
                'noise_detection': True
            }
        }
        
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                # åˆå¹¶é…ç½®
                for key, value in user_config.items():
                    if key in default_config and isinstance(default_config[key], dict) and isinstance(value, dict):
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
                print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def collect_photo_data(self, source_dirs=None):
        """
        æ”¶é›†çœŸå®ç…§ç‰‡æ•°æ®
        
        Args:
            source_dirs: æºç›®å½•åˆ—è¡¨
        """
        print("ğŸ“¸ å¼€å§‹æ”¶é›†çœŸå®ç…§ç‰‡æ•°æ®...")
        
        if source_dirs is None:
            source_dirs = ['temp', '.', 'downloads']  # é»˜è®¤æœç´¢ç›®å½•
        
        collected_count = 0
        
        for source_dir in source_dirs:
            if not os.path.exists(source_dir):
                print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {source_dir}")
                continue
            
            print(f"ğŸ” æœç´¢ç›®å½•: {source_dir}")
            
            # æœç´¢å›¾ç‰‡æ–‡ä»¶
            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
            found_images = []
            
            for ext in image_extensions:
                pattern = os.path.join(source_dir, ext)
                found_images.extend(glob.glob(pattern))
                pattern = os.path.join(source_dir, ext.upper())
                found_images.extend(glob.glob(pattern))
            
            # å»é‡
            found_images = list(set(found_images))
            
            print(f"   ğŸ“Š æ‰¾åˆ° {len(found_images)} å¼ å›¾ç‰‡")
            
            # å¤„ç†æ¯å¼ å›¾ç‰‡
            for img_path in found_images:
                try:
                    if self._process_photo_image(img_path):
                        collected_count += 1
                        if collected_count % 100 == 0:
                            print(f"   âœ… å·²å¤„ç† {collected_count} å¼ å›¾ç‰‡")
                except Exception as e:
                    print(f"   âŒ å¤„ç†å¤±è´¥ {img_path}: {e}")
                    self.stats['errors'] += 1
        
        print(f"âœ… ç…§ç‰‡æ•°æ®æ”¶é›†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {collected_count} å¼ å›¾ç‰‡")
        return collected_count
    
    def collect_ghibli_data(self, extract_frames=True):
        """
        æ”¶é›†å®«å´éªé£æ ¼æ•°æ®
        
        Args:
            extract_frames: æ˜¯å¦ä»è§†é¢‘ä¸­æå–å¸§
        """
        print("ğŸ¬ å¼€å§‹æ”¶é›†å®«å´éªé£æ ¼æ•°æ®...")
        
        collected_count = 0
        
        # æ”¶é›†ç°æœ‰çš„å®«å´éªå›¾ç‰‡
        print("ğŸ” æœç´¢ç°æœ‰å®«å´éªå›¾ç‰‡...")
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        
        for ext in image_extensions:
            pattern = os.path.join(self.ghibli_dir, ext)
            found_images = glob.glob(pattern)
            
            for img_path in found_images:
                try:
                    if self._process_ghibli_image(img_path):
                        collected_count += 1
                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥ {img_path}: {e}")
                    self.stats['errors'] += 1
        
        # ä»è§†é¢‘æå–å¸§ï¼ˆå¦‚æœæœ‰ï¼‰
        if extract_frames:
            video_extensions = ['*.mp4', '*.avi', '*.mkv', '*.mov']
            
            for ext in video_extensions:
                pattern = os.path.join(self.ghibli_dir, ext)
                found_videos = glob.glob(pattern)
                
                for video_path in found_videos:
                    try:
                        frames_count = self._extract_frames_from_video(video_path)
                        collected_count += frames_count
                    except Exception as e:
                        print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥ {video_path}: {e}")
                        self.stats['errors'] += 1
        
        print(f"âœ… å®«å´éªæ•°æ®æ”¶é›†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {collected_count} å¼ å›¾ç‰‡")
        return collected_count
    
    def _process_photo_image(self, img_path):
        """å¤„ç†å•å¼ çœŸå®ç…§ç‰‡"""
        try:
            # è¯»å–å›¾ç‰‡
            img = cv2.imread(img_path)
            if img is None:
                return False
            
            # è´¨é‡æ£€æŸ¥
            if not self._check_image_quality(img):
                return False
            
            # é¢„å¤„ç†
            processed_img = self._preprocess_image(img)
            
            # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
            filename = os.path.basename(img_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(self.photo_dir, f"{name}_processed.jpg")
            cv2.imwrite(output_path, processed_img, [cv2.IMWRITE_JPEG_QUALITY, 90])
            
            self.stats['photos_processed'] += 1
            return True
            
        except Exception as e:
            return False
    
    def _process_ghibli_image(self, img_path):
        """å¤„ç†å•å¼ å®«å´éªé£æ ¼å›¾ç‰‡"""
        try:
            # è¯»å–å›¾ç‰‡
            img = cv2.imread(img_path)
            if img is None:
                return False
            
            # é¢„å¤„ç†
            processed_img = self._preprocess_ghibli_image(img)
            
            # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
            filename = os.path.basename(img_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(self.ghibli_dir, f"{name}_processed.jpg")
            cv2.imwrite(output_path, processed_img, [cv2.IMWRITE_JPEG_QUALITY, 90])
            
            self.stats['ghibli_processed'] += 1
            return True
            
        except Exception as e:
            return False
    
    def _extract_frames_from_video(self, video_path):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return 0
        
        frame_count = 0
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # æ¯ç§’æå–ä¸€å¸§
        frame_interval = int(fps)
        
        filename = os.path.splitext(os.path.basename(video_path))[0]
        
        for i in range(0, total_frames, frame_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            
            if ret:
                # é¢„å¤„ç†å¸§
                processed_frame = self._preprocess_ghibli_image(frame)
                
                # ä¿å­˜å¸§
                output_path = os.path.join(self.ghibli_dir, f"{filename}_frame_{frame_count:06d}.jpg")
                cv2.imwrite(output_path, processed_frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
                
                frame_count += 1
        
        cap.release()
        return frame_count
    
    def _check_image_quality(self, img):
        """æ£€æŸ¥å›¾åƒè´¨é‡"""
        # æ¨¡ç³Šæ£€æµ‹
        if self.config['preprocessing']['blur_detection']:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            if blur_score < 100:  # æ¨¡ç³Šé˜ˆå€¼
                return False
        
        # å™ªå£°æ£€æµ‹
        if self.config['preprocessing']['noise_detection']:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            noise_score = np.var(gray)
            
            # å™ªå£°è¿‡é«˜æˆ–è¿‡ä½éƒ½æ’é™¤
            if noise_score < 50 or noise_score > 2000:
                return False
        
        # å°ºå¯¸æ£€æŸ¥
        h, w = img.shape[:2]
        if h < 256 or w < 256:
            return False
        
        return True
    
    def _preprocess_image(self, img):
        """é¢„å¤„ç†çœŸå®ç…§ç‰‡"""
        # è°ƒæ•´å°ºå¯¸
        h, w = img.shape[:2]
        if max(h, w) != self.image_size:
            scale = self.image_size / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        # ä¸­å¿ƒè£å‰ªåˆ°ç›®æ ‡å°ºå¯¸
        h, w = img.shape[:2]
        start_y = (h - self.image_size) // 2
        start_x = (w - self.image_size) // 2
        img = img[start_y:start_y+self.image_size, start_x:start_x+self.image_size]
        
        # äººè„¸æ£€æµ‹ï¼ˆå¯é€‰ï¼‰
        if self.config['preprocessing']['face_detection']:
            try:
                # ä½¿ç”¨OpenCVçš„äººè„¸æ£€æµ‹
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                
                # å¦‚æœæ£€æµ‹åˆ°äººè„¸ï¼Œä¼˜å…ˆä¿ç•™åŒ…å«äººè„¸çš„å›¾ç‰‡
                if len(faces) > 0:
                    pass  # ä¿ç•™å›¾ç‰‡
                # å¯ä»¥æ·»åŠ å…¶ä»–é€»è¾‘...
            except:
                pass
        
        return img
    
    def _preprocess_ghibli_image(self, img):
        """é¢„å¤„ç†å®«å´éªé£æ ¼å›¾ç‰‡"""
        # è°ƒæ•´å°ºå¯¸
        h, w = img.shape[:2]
        if max(h, w) != self.image_size:
            scale = self.image_size / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        # ä¸­å¿ƒè£å‰ªåˆ°ç›®æ ‡å°ºå¯¸
        h, w = img.shape[:2]
        start_y = (h - self.image_size) // 2
        start_x = (w - self.image_size) // 2
        img = img[start_y:start_y+self.image_size, start_x:start_x+self.image_size]
        
        # å®«å´éªé£æ ¼ç‰¹æœ‰çš„é¢„å¤„ç†
        # å¢å¼ºé¥±å’Œåº¦
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.1, 0, 255)
        img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        return img
    
    def create_training_pairs(self):
        """åˆ›å»ºè®­ç»ƒé…å¯¹æ•°æ®"""
        print("ğŸ”— å¼€å§‹åˆ›å»ºè®­ç»ƒé…å¯¹...")
        
        # è·å–å¤„ç†åçš„å›¾ç‰‡åˆ—è¡¨
        photo_files = glob.glob(os.path.join(self.photo_dir, "*_processed.jpg"))
        ghibli_files = glob.glob(os.path.join(self.ghibli_dir, "*_processed.jpg"))
        
        print(f"ğŸ“Š ç…§ç‰‡æ•°é‡: {len(photo_files)}")
        print(f"ğŸ“Š å®«å´éªå›¾ç‰‡æ•°é‡: {len(ghibli_files)}")
        
        if len(photo_files) == 0 or len(ghibli_files) == 0:
            print("âŒ ç¼ºå°‘è®­ç»ƒæ•°æ®")
            return 0
        
        # åˆ›å»ºé…å¯¹
        pairs_created = 0
        
        # ä¸ºæ¯å¼ ç…§ç‰‡åˆ›å»ºé…å¯¹
        for photo_path in photo_files:
            try:
                # éšæœºé€‰æ‹©å®«å´éªé£æ ¼å›¾ç‰‡
                ghibli_path = random.choice(ghibli_files)
                
                # åˆ›å»ºé…å¯¹ç›®å½•
                pair_id = f"pair_{pairs_created:06d}"
                pair_dir = os.path.join(self.output_dir, pair_id)
                os.makedirs(pair_dir, exist_ok=True)
                
                # å¤åˆ¶æ–‡ä»¶
                photo_name = os.path.basename(photo_path)
                ghibli_name = os.path.basename(ghibli_path)
                
                os.system(f"cp '{photo_path}' '{os.path.join(pair_dir, 'photo.jpg')}'")
                os.system(f"cp '{ghibli_path}' '{os.path.join(pair_dir, 'ghibli.jpg')}'")
                
                # åˆ›å»ºé…å¯¹ä¿¡æ¯æ–‡ä»¶
                pair_info = {
                    'pair_id': pair_id,
                    'photo_file': photo_name,
                    'ghibli_file': ghibli_name,
                    'created_time': time.time()
                }
                
                with open(os.path.join(pair_dir, 'info.json'), 'w') as f:
                    json.dump(pair_info, f, indent=2)
                
                pairs_created += 1
                
                if pairs_created % 100 == 0:
                    print(f"   âœ… å·²åˆ›å»º {pairs_created} ä¸ªé…å¯¹")
                
            except Exception as e:
                print(f"âŒ åˆ›å»ºé…å¯¹å¤±è´¥: {e}")
                self.stats['errors'] += 1
        
        self.stats['pairs_created'] = pairs_created
        print(f"âœ… è®­ç»ƒé…å¯¹åˆ›å»ºå®Œæˆï¼Œå…±åˆ›å»º {pairs_created} ä¸ªé…å¯¹")
        return pairs_created
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'photos_processed': self.stats['photos_processed'],
            'ghibli_processed': self.stats['ghibli_processed'], 
            'pairs_created': self.stats['pairs_created'],
            'errors': self.stats['errors'],
            'photo_dir_size': len(glob.glob(os.path.join(self.photo_dir, "*.jpg"))),
            'ghibli_dir_size': len(glob.glob(os.path.join(self.ghibli_dir, "*.jpg"))),
            'output_dir_size': len(glob.glob(os.path.join(self.output_dir, "pair_*")))
        }

class GhibliDataset(Dataset):
    """å®«å´éªé£æ ¼è®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_dir, transform=None, augmentation=True):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            transform: å›¾åƒå˜æ¢
            augmentation: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º
        """
        self.data_dir = data_dir
        self.transform = transform
        self.augmentation = augmentation
        
        # åŠ è½½é…å¯¹æ•°æ®
        self.pairs = []
        pair_dirs = glob.glob(os.path.join(data_dir, "pair_*"))
        
        for pair_dir in pair_dirs:
            photo_path = os.path.join(pair_dir, 'photo.jpg')
            ghibli_path = os.path.join(pair_dir, 'ghibli.jpg')
            
            if os.path.exists(photo_path) and os.path.exists(ghibli_path):
                self.pairs.append((photo_path, ghibli_path))
        
        print(f"ğŸ“Š åŠ è½½äº† {len(self.pairs)} ä¸ªè®­ç»ƒé…å¯¹")
        
        # æ•°æ®å¢å¼ºå˜æ¢
        self.aug_transform = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=5),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
        ])
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        photo_path, ghibli_path = self.pairs[idx]
        
        # è¯»å–å›¾åƒ
        photo = Image.open(photo_path).convert('RGB')
        ghibli = Image.open(ghibli_path).convert('RGB')
        
        # æ•°æ®å¢å¼º
        if self.augmentation and random.random() > 0.5:
            # åŒæ ·çš„éšæœºç§å­ç¡®ä¿ä¸¤ä¸ªå›¾åƒåº”ç”¨ç›¸åŒçš„å˜æ¢
            seed = random.randint(0, 2**32)
            random.seed(seed)
            torch.manual_seed(seed)
            photo = self.aug_transform(photo)
            
            random.seed(seed)
            torch.manual_seed(seed)
            ghibli = self.aug_transform(ghibli)
        
        # åº”ç”¨åŸºç¡€å˜æ¢
        if self.transform:
            photo = self.transform(photo)
            ghibli = self.transform(ghibli)
        
        return {
            'photo': photo,
            'ghibli': ghibli,
            'photo_path': photo_path,
            'ghibli_path': ghibli_path
        }

def create_dataloaders(data_dir, batch_size=4, num_workers=2, train_split=0.8):
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        train_split: è®­ç»ƒé›†æ¯”ä¾‹
        
    Returns:
        train_loader, val_loader: è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    """
    # å®šä¹‰å˜æ¢
    transform = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    full_dataset = GhibliDataset(data_dir, transform=transform, augmentation=True)
    
    # åˆ†å‰²æ•°æ®é›†
    train_size = int(train_split * len(full_dataset))
    val_size = len(full_dataset) - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size]
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
    
    return train_loader, val_loader

    def collect_training_data(self, photo_dir, style_dir):
        """
        æ”¶é›†è®­ç»ƒæ•°æ®
        
        Args:
            photo_dir: ç…§ç‰‡ç›®å½•
            style_dir: é£æ ¼ç›®å½•
        
        Returns:
            tuple: (ç…§ç‰‡åˆ—è¡¨, é£æ ¼åˆ—è¡¨)
        """
        photos = []
        styles = []
        
        # æ”¶é›†ç…§ç‰‡
        if os.path.exists(photo_dir):
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                photos.extend(glob.glob(os.path.join(photo_dir, ext)))
                photos.extend(glob.glob(os.path.join(photo_dir, ext.upper())))
        
        # æ”¶é›†é£æ ¼å›¾åƒ
        if os.path.exists(style_dir):
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                styles.extend(glob.glob(os.path.join(style_dir, ext)))
                styles.extend(glob.glob(os.path.join(style_dir, ext.upper())))
        
        return photos, styles

# åˆ›å»ºå…¨å±€æ•°æ®ç®¡é“å®ä¾‹
ghibli_pipeline = GhibliDataPipeline()