#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢ç³»ç»Ÿ
é›†æˆäººè„¸æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€èƒŒæ™¯å¤„ç†ç­‰ä¸“ä¸šåŠŸèƒ½
"""

import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import os
import time
import threading
from typing import Optional
from .image_processor_interface import ImageProcessorInterface, ProcessingResult, ProcessingStyle

# å¯¼å…¥é«˜çº§å¤„ç†å™¨å’ŒAnimeGAN
from .advanced_anime_processor import advanced_processor
from .anime_gan_processor import convert_with_anime_gan

class GhibliEnhancedTransfer(ImageProcessorInterface):
    """å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢ç³»ç»Ÿ
    é›†æˆäººè„¸æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€èƒŒæ™¯å¤„ç†ç­‰ä¸“ä¸šåŠŸèƒ½
    """
    
    def __init__(self, use_face_detection=True, use_background_separation=True):
        super().__init__(ProcessingStyle.GHIBLI_ENHANCED)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.use_face_detection = use_face_detection
        self.use_background_separation = use_background_separation
        
        # åŠ è½½æ¨¡å‹
        self.face_detector = None
        self.seg_model = None
        self.face_landmark_model = None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()
        
        # è¿›åº¦å›è°ƒ
        self.progress_callback = None
        self.task_id = None
    
    def process(self, image: Image.Image, **kwargs) -> ProcessingResult:
        """
        å¤„ç†å›¾åƒï¼Œåº”ç”¨å¢å¼ºç‰ˆå®«å´éªé£æ ¼
        
        Args:
            image: è¾“å…¥å›¾åƒ
            **kwargs: å…¶ä»–å¤„ç†å‚æ•°
                - use_face_enhancement: æ˜¯å¦ä½¿ç”¨äººè„¸å¢å¼º
                - use_bg_separation: æ˜¯å¦ä½¿ç”¨èƒŒæ™¯åˆ†ç¦»
            
        Returns:
            ProcessingResult: å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        try:
            use_face_enhancement = kwargs.get('use_face_enhancement', True)
            use_bg_separation = kwargs.get('use_bg_separation', True)
            
            result_image = self.apply_enhanced_ghibli_style(
                image,
                use_face_enhancement=use_face_enhancement,
                use_bg_separation=use_bg_separation
            )
            
            processing_time = time.time() - start_time
            
            return ProcessingResult(
                success=True,
                image=result_image,
                processing_time=processing_time
            )
        except Exception as e:
            processing_time = time.time() - start_time
            return ProcessingResult(
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    def get_processing_info(self) -> dict:
        """
        è·å–å¤„ç†å™¨ä¿¡æ¯
        
        Returns:
            dict: å¤„ç†å™¨ä¿¡æ¯
        """
        return {
            "processor_type": "GhibliEnhancedTransfer",
            "style_type": self.style_type.value,
            "use_face_detection": self.use_face_detection,
            "use_background_separation": self.use_background_separation,
            "device": str(self.device)
        }
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
        print("ğŸ”§ åˆå§‹åŒ–å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢æ¨¡å‹...")
        
        # 1. äººè„¸æ£€æµ‹æ¨¡å‹
        if self.use_face_detection:
            self._initialize_face_detector()
        
        # 2. è¯­ä¹‰åˆ†å‰²æ¨¡å‹
        if self.use_background_separation:
            self._initialize_segmentation_model()
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_face_detector(self):
        """åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨"""
        try:
            # ä½¿ç”¨OpenCVçš„DNNæ¨¡å—åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹
            model_path = "models/face_detector/opencv_face_detector_uint8.pb"
            config_path = "models/face_detector/opencv_face_detector.pbtxt"
            
            # å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…ç½®çš„Haarçº§è”åˆ†ç±»å™¨
            if os.path.exists(model_path) and os.path.exists(config_path):
                self.face_detector = cv2.dnn.readNetFromTensorflow(model_path, config_path)
                print("âœ… åŠ è½½DNNäººè„¸æ£€æµ‹æ¨¡å‹æˆåŠŸ")
            else:
                # ä½¿ç”¨OpenCVå†…ç½®çš„Haarçº§è”åˆ†ç±»å™¨
                cascade_path = cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
                self.face_detector = cv2.CascadeClassifier(cascade_path)
                print("âœ… åŠ è½½Haarçº§è”äººè„¸æ£€æµ‹å™¨æˆåŠŸ")
                
        except Exception as e:
            print(f"âš ï¸ äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.face_detector = None
    
    def _initialize_segmentation_model(self):
        """åˆå§‹åŒ–è¯­ä¹‰åˆ†å‰²æ¨¡å‹"""
        try:
            # ä½¿ç”¨é¢„è®­ç»ƒçš„DeepLabV3æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²
            self.seg_model = models.segmentation.deeplabv3_resnet50(
                weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT
            ).eval().to(self.device)
            print("âœ… åŠ è½½è¯­ä¹‰åˆ†å‰²æ¨¡å‹æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰åˆ†å‰²æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.seg_model = None
    
    def detect_faces(self, image):
        """æ£€æµ‹å›¾åƒä¸­çš„äººè„¸"""
        if not self.use_face_detection or self.face_detector is None:
            return []
        
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                if len(img_np.shape) == 3:
                    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
                else:
                    gray = img_np
            else:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # æ£€æµ‹äººè„¸
            if isinstance(self.face_detector, cv2.dnn_Net):
                # DNNæ¨¡å‹æ£€æµ‹
                blob = cv2.dnn.blobFromImage(gray, 1.0, (300, 300), [104, 117, 123])
                self.face_detector.setInput(blob)
                detections = self.face_detector.forward()
                
                faces = []
                for i in range(detections.shape[2]):
                    confidence = detections[0, 0, i, 2]
                    if confidence > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                        x1 = int(detections[0, 0, i, 3] * gray.shape[1])
                        y1 = int(detections[0, 0, i, 4] * gray.shape[0])
                        x2 = int(detections[0, 0, i, 5] * gray.shape[1])
                        y2 = int(detections[0, 0, i, 6] * gray.shape[0])
                        faces.append((x1, y1, x2-x1, y2-y1))
            else:
                # Haarçº§è”æ£€æµ‹
                faces = self.face_detector.detectMultiScale(
                    gray, 
                    scaleFactor=1.1, 
                    minNeighbors=5, 
                    minSize=(30, 30)
                )
            
            print(f"ğŸ‘¤ æ£€æµ‹åˆ° {len(faces)} å¼ äººè„¸")
            return faces
            
        except Exception as e:
            print(f"âŒ äººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def segment_person(self, image):
        """è¯­ä¹‰åˆ†å‰²ï¼Œæå–äººç‰©å‰æ™¯"""
        if not self.use_background_separation or self.seg_model is None:
            # å¦‚æœæ²¡æœ‰åˆ†å‰²æ¨¡å‹ï¼Œè¿”å›å…¨å›¾æ©ç 
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                mask = np.ones(img_np.shape[:2], dtype=np.uint8) * 255
            else:
                mask = np.ones(image.shape[:2], dtype=np.uint8) * 255
            return mask
        
        try:
            # é¢„å¤„ç†å›¾åƒ
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                rgb = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            else:
                rgb = image
            
            # è°ƒæ•´å°ºå¯¸
            h, w = rgb.shape[:2]
            max_size = 520
            if max(h, w) > max_size:
                scale = max_size / max(h, w)
                new_w, new_h = int(w * scale), int(h * scale)
                rgb_resized = cv2.resize(rgb, (new_w, new_h))
            else:
                rgb_resized = rgb
            
            # è½¬æ¢ä¸ºtensor
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(rgb_resized).unsqueeze(0).to(self.device)
            
            # æ¨ç†
            with torch.no_grad():
                output = self.seg_model(input_tensor)['out'][0]
            
            # è·å–äººç‰©ç±»åˆ«ï¼ˆCOCOæ•°æ®é›†ä¸­äººç‰©ç±»åˆ«ä¸º15ï¼‰
            person_class = 15
            mask = (output.argmax(0) == person_class).cpu().numpy().astype(np.uint8) * 255
            
            # è°ƒæ•´åˆ°åŸå›¾å°ºå¯¸
            mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # å¹³æ»‘è¾¹ç¼˜
            mask = cv2.GaussianBlur(mask, (9, 9), 0)
            
            print("ğŸ¯ è¯­ä¹‰åˆ†å‰²å®Œæˆ")
            return mask
            
        except Exception as e:
            print(f"âŒ è¯­ä¹‰åˆ†å‰²å¤±è´¥: {e}")
            # è¿”å›å…¨å›¾æ©ç ä½œä¸ºå›é€€
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                mask = np.ones(img_np.shape[:2], dtype=np.uint8) * 255
            else:
                mask = np.ones(image.shape[:2], dtype=np.uint8) * 255
            return mask
    
    def enhance_faces(self, image, face_regions):
        """å¢å¼ºäººè„¸åŒºåŸŸ - å®«å´éªé£æ ¼ç¾åŒ–"""
        if not face_regions:
            return image
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(image, Image.Image):
            img_np = np.array(image)
            if len(img_np.shape) == 3:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            else:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        else:
            img_bgr = image.copy()
        
        enhanced_image = img_bgr.copy()
        
        for (x, y, w, h) in face_regions:
            # æå–äººè„¸åŒºåŸŸ
            face_roi = img_bgr[y:y+h, x:x+w]
            
            if face_roi.size == 0:
                continue
            
            # åº”ç”¨å®«å´éªé£æ ¼çš„äººè„¸ç¾åŒ–
            enhanced_face = self._ghibli_face_enhancement(face_roi)
            
            # å°†å¢å¼ºåçš„äººè„¸æ”¾å›åŸå›¾
            enhanced_image[y:y+h, x:x+w] = enhanced_face
        
        return enhanced_image
    
    def _ghibli_face_enhancement(self, face_region):
        """å®«å´éªé£æ ¼çš„äººè„¸ç¾åŒ–"""
        # 1. çš®è‚¤å¹³æ»‘
        smoothed = cv2.bilateralFilter(face_region, 9, 75, 75)
        
        # 2. çœ¼ç›å¢å¼º
        enhanced_eyes = self._enhance_eyes(smoothed)
        
        # 3. å˜´å”‡å¢å¼º
        enhanced_lips = self._enhance_lips(enhanced_eyes)
        
        # 4. è‰²å½©è°ƒæ•´
        final_face = self._adjust_face_colors(enhanced_lips)
        
        return final_face
    
    def _enhance_eyes(self, face_region):
        """å¢å¼ºçœ¼ç›åŒºåŸŸ"""
        h, w = face_region.shape[:2]
        
        # å®šä¹‰çœ¼ç›åŒºåŸŸï¼ˆç›¸å¯¹ä½ç½®ï¼‰
        eye_top = int(h * 0.25)
        eye_bottom = int(h * 0.45)
        eye_left = int(w * 0.25)
        eye_right = int(w * 0.75)
        
        # å¢å¼ºçœ¼ç›åŒºåŸŸäº®åº¦
        eye_region = face_region[eye_top:eye_bottom, eye_left:eye_right]
        
        if eye_region.size > 0:
            # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´
            lab = cv2.cvtColor(eye_region, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            
            # å¢å¼ºäº®åº¦
            l = cv2.add(l, 20)
            l = np.clip(l, 0, 255)
            
            # åˆå¹¶é€šé“
            lab_enhanced = cv2.merge([l, a, b])
            enhanced_eyes = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
            
            # å°†å¢å¼ºåçš„çœ¼ç›åŒºåŸŸæ”¾å›
            face_region[eye_top:eye_bottom, eye_left:eye_right] = enhanced_eyes
        
        return face_region
    
    def _enhance_lips(self, face_region):
        """å¢å¼ºå˜´å”‡åŒºåŸŸ"""
        h, w = face_region.shape[:2]
        
        # å®šä¹‰å˜´å”‡åŒºåŸŸï¼ˆç›¸å¯¹ä½ç½®ï¼‰
        lip_top = int(h * 0.6)
        lip_bottom = int(h * 0.75)
        lip_left = int(w * 0.35)
        lip_right = int(w * 0.65)
        
        # å¢å¼ºå˜´å”‡åŒºåŸŸé¥±å’Œåº¦
        lip_region = face_region[lip_top:lip_bottom, lip_left:lip_right]
        
        if lip_region.size > 0:
            # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´
            hsv = cv2.cvtColor(lip_region, cv2.COLOR_BGR2HSV)
            h, s, v = cv2.split(hsv)
            
            # å¢å¼ºé¥±å’Œåº¦
            s = cv2.add(s, 30)
            s = np.clip(s, 0, 255)
            
            # åˆå¹¶é€šé“
            hsv_enhanced = cv2.merge([h, s, v])
            enhanced_lips = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
            
            # å°†å¢å¼ºåçš„å˜´å”‡åŒºåŸŸæ”¾å›
            face_region[lip_top:lip_bottom, lip_left:lip_right] = enhanced_lips
        
        return face_region
    
    def _adjust_face_colors(self, face_region):
        """è°ƒæ•´é¢éƒ¨è‰²å½© - å®«å´éªé£æ ¼"""
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´è¿›è¡Œç²¾ç¡®è°ƒæ•´
        lab = cv2.cvtColor(face_region, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # è°ƒæ•´äº®åº¦ï¼ˆå®«å´éªé£æ ¼çš®è‚¤æ˜äº®ï¼‰
        l = cv2.add(l, 10)
        l = np.clip(l, 0, 255)
        
        # è°ƒæ•´è‰²å½©å¹³è¡¡ï¼ˆåå‘æ¸©æš–è‰²è°ƒï¼‰
        a = cv2.add(a, 5)  # åå‘çº¢è‰²
        b = cv2.add(b, 3)  # åå‘é»„è‰²
        
        # åˆå¹¶é€šé“
        lab_balanced = cv2.merge([l, a, b])
        final_face = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        return final_face
    
    def process_background(self, image, person_mask, style_template="classic"):
        """å¤„ç†èƒŒæ™¯ - å®«å´éªé£æ ¼èƒŒæ™¯ä¼˜åŒ–"""
        if not self.use_background_separation:
            return image
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(image, Image.Image):
            img_np = np.array(image)
            if len(img_np.shape) == 3:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            else:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        else:
            img_bgr = image.copy()
        
        # åˆ†ç¦»å‰æ™¯å’ŒèƒŒæ™¯
        foreground = img_bgr.copy()
        background = img_bgr.copy()
        
        # åˆ›å»ºèƒŒæ™¯æ©ç 
        bg_mask = cv2.bitwise_not(person_mask)
        
        # å¯¹èƒŒæ™¯åº”ç”¨å®«å´éªé£æ ¼å¤„ç†
        if style_template == "fantasy":
            bg_processed = self._fantasy_ghibli_style(background, intensity=0.6)
        elif style_template == "nostalgic":
            bg_processed = self._nostalgic_ghibli_style(background, intensity=0.7)
        elif style_template == "vibrant":
            bg_processed = self._vibrant_ghibli_style(background, intensity=0.8)
        else:
            bg_processed = self._classic_ghibli_style(background, intensity=0.5)
        
        # åˆå¹¶å‰æ™¯å’ŒèƒŒæ™¯
        result = np.zeros_like(img_bgr)
        
        # ä½¿ç”¨æ©ç æ··åˆå‰æ™¯å’ŒèƒŒæ™¯
        for i in range(3):  # å¯¹æ¯ä¸ªé€šé“è¿›è¡Œå¤„ç†
            result[:,:,i] = (
                foreground[:,:,i] * (person_mask / 255.0) + 
                bg_processed[:,:,i] * (bg_mask / 255.0)
            ).astype(np.uint8)
        
        return result
    
    def _classic_ghibli_style(self, image, intensity=0.5):
        """ç»å…¸å®«å´éªé£æ ¼èƒŒæ™¯"""
        # æŸ”å’Œè‰²å½©ï¼Œæ¢¦å¹»å…‰å½±
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦
        s = cv2.add(s, int(30 * intensity))
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒåå‘æ¸©æš–
        h = cv2.add(h, int(5 * intensity))
        h = np.clip(h, 0, 179)
        
        # åº”ç”¨æŸ”å’Œæ»¤é•œ
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # æ·»åŠ æ¢¦å¹»å…‰å½±
        final = self._add_dreamy_lighting(enhanced, intensity)
        
        return final
    
    def _fantasy_ghibli_style(self, image, intensity=0.6):
        """æ¢¦å¹»å®«å´éªé£æ ¼èƒŒæ™¯"""
        # æ›´å¼ºçƒˆçš„è‰²å½©å’Œå…‰å½±æ•ˆæœ
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºè‰²å½©é²œè‰³åº¦
        a = cv2.add(a, int(40 * intensity))
        b = cv2.add(b, int(30 * intensity))
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # æ›´å¼ºçš„æ¢¦å¹»å…‰å½±
        final = self._add_dreamy_lighting(enhanced, intensity * 1.2)
        
        return final
    
    def _nostalgic_ghibli_style(self, image, intensity=0.7):
        """æ€€æ—§å®«å´éªé£æ ¼èƒŒæ™¯"""
        # æŸ”å’Œçš„æ€€æ—§è‰²è°ƒ
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # é™ä½é¥±å’Œåº¦ï¼Œåˆ›é€ æ€€æ—§æ„Ÿ
        s = cv2.subtract(s, int(20 * intensity))
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒåå‘æš–é»„
        h = cv2.add(h, int(10 * intensity))
        h = np.clip(h, 0, 179)
        
        hsv_nostalgic = cv2.merge([h, s, v])
        nostalgic = cv2.cvtColor(hsv_nostalgic, cv2.COLOR_HSV2BGR)
        
        # æ·»åŠ è½»å¾®çš„èƒ¶ç‰‡é¢—ç²’æ•ˆæœ
        noise = np.random.normal(0, 3 * intensity, nostalgic.shape).astype(np.uint8)
        final = cv2.add(nostalgic, noise)
        
        return final
    
    def _vibrant_ghibli_style(self, image, intensity=0.8):
        """é²œè‰³å®«å´éªé£æ ¼èƒŒæ™¯"""
        # é«˜é¥±å’Œåº¦é²œè‰³è‰²å½©
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¤§å¹…å¢å¼ºé¥±å’Œåº¦
        s = cv2.add(s, int(50 * intensity))
        s = np.clip(s, 0, 255)
        
        # å¢å¼ºäº®åº¦
        v = cv2.add(v, int(20 * intensity))
        v = np.clip(v, 0, 255)
        
        hsv_vibrant = cv2.merge([h, s, v])
        vibrant = cv2.cvtColor(hsv_vibrant, cv2.COLOR_HSV2BGR)
        
        # é”åŒ–å¢å¼ºç»†èŠ‚
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        final = cv2.filter2D(vibrant, -1, kernel)
        
        return final
    
    def _add_dreamy_lighting(self, image, intensity=1.0):
        """æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ"""
        h, w = image.shape[:2]
        
        # åˆ›å»ºä¸­å¿ƒæ˜äº®çš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½©
        light_mask = 1.0 - (distance / max_distance) * 0.1 * intensity
        light_mask = np.clip(light_mask, 0.9, 1.0)
        
        # åº”ç”¨å…‰ç…§æ•ˆæœ
        final = image.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        return final
    
    def apply_enhanced_ghibli_style(self, image, use_face_enhancement=True, use_bg_separation=True):
        """åº”ç”¨å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢"""
        print("ğŸ¨ å¼€å§‹åº”ç”¨å¢å¼ºç‰ˆå®«å´éªé£æ ¼...")
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 10, 1, 10, 0)
        
        try:
            # 1. äººè„¸æ£€æµ‹
            faces = []
            if use_face_enhancement:
                faces = self.detect_faces(image)
                print(f"ğŸ‘¤ æ£€æµ‹åˆ° {len(faces)} å¼ äººè„¸")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 30, 2, 10, 0)
            
            # 2. è¯­ä¹‰åˆ†å‰²
            person_mask = None
            if use_bg_separation:
                person_mask = self.segment_person(image)
                print("ğŸ¯ è¯­ä¹‰åˆ†å‰²å®Œæˆ")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 50, 3, 10, 0)
            
            # 3. è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                if len(img_np.shape) == 3:
                    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
                else:
                    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
            else:
                img_bgr = image.copy()
            
            # 4. é¦–å…ˆå°è¯•ä½¿ç”¨AnimeGANè¿›è¡Œç«¯åˆ°ç«¯è½¬æ¢
            print("ğŸ¨ å°è¯•ä½¿ç”¨AnimeGANè¿›è¡Œç«¯åˆ°ç«¯è½¬æ¢...")
            
            try:
                # è½¬æ¢ä¸ºPILæ ¼å¼
                img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
                img_pil = Image.fromarray(img_rgb)
                
                # ä½¿ç”¨AnimeGANè½¬æ¢ï¼ˆå®«å´éªé£æ ¼ï¼‰
                def anime_gan_progress(stage, progress):
                    if self.progress_callback and self.task_id:
                        self.progress_callback(self.task_id, 20 + int(progress * 0.3), 3, 10, 0)
                
                anime_result = convert_with_anime_gan(
                    img_pil, 
                    model_type='hayao',  # ä½¿ç”¨å®«å´éªé£æ ¼
                    progress_callback=anime_gan_progress
                )
                
                # è½¬æ¢å›BGRæ ¼å¼
                img_array = np.array(anime_result)
                img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
                
                print("âœ… AnimeGANè½¬æ¢æˆåŠŸ")
                
            except Exception as e:
                print(f"âš ï¸ AnimeGANè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é«˜çº§å¤„ç†å™¨å›é€€: {e}")
                
                # ä½¿ç”¨é«˜çº§å¤„ç†å™¨ä½œä¸ºå›é€€æ–¹æ¡ˆ
                img_bgr = advanced_processor.process_anime_style(
                    img_bgr, 
                    use_slic=True, 
                    use_xdog=True, 
                    use_multiscale=True, 
                    use_color_mapping=False  # å…ˆä¸åº”ç”¨è‰²å½©æ˜ å°„ï¼Œåé¢å•ç‹¬å¤„ç†
                )
            
            # ä¿å­˜åŸå§‹ç»“æ„ç”¨äºåç»­æ··åˆ
            original_structure = img_bgr.copy()
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 50, 3, 10, 0)
            
            # 5. äººè„¸ç¾åŒ–
            if faces and use_face_enhancement:
                img_bgr = self.enhance_faces(img_bgr, faces)
                print("ğŸ’„ äººè„¸ç¾åŒ–å®Œæˆ")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 60, 4, 10, 0)
            
            # 6. èƒŒæ™¯å¤„ç†
            if person_mask is not None and use_bg_separation:
                img_bgr = self.process_background(img_bgr, person_mask)
                print("ğŸŒ… èƒŒæ™¯å¤„ç†å®Œæˆ")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 70, 5, 10, 0)
            
            # 7. åº”ç”¨æ™ºèƒ½å®«å´éªè‰²å½©æ˜ å°„
            print("ğŸ¨ åº”ç”¨æ™ºèƒ½å®«å´éªè‰²å½©æ˜ å°„...")
            img_bgr = advanced_processor.intelligent_color_mapping(img_bgr)
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 80, 6, 10, 0)
            
            # 8. ç»“æ„ä¿æŒæ··åˆ - ä¿æŒç‰©ä½“å’Œäººç‰©è¯†åˆ«æ€§
            print("ğŸ”§ ä¿æŒç»“æ„æ¸…æ™°åº¦...")
            img_bgr = self._preserve_structure_mixing(img_bgr, original_structure)
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 90, 7, 10, 0)
            
            # 9. æœ€ç»ˆå…‰ç…§ä¼˜åŒ–
            img_bgr = apply_final_lighting(img_bgr)
            
            # è½¬æ¢å›PILæ ¼å¼
            result_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            result = Image.fromarray(result_rgb)
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 100, 10, 10, 0)
            
            print("âœ… å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢å®Œæˆ")
            return result
            
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆé£æ ¼è½¬æ¢å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
            from .real_ghibli_transfer import RealGhibliStyleTransfer
            ghibli_model = RealGhibliStyleTransfer()
            return ghibli_model.apply_real_ghibli_style(image, use_neural=False)
    
    def _preserve_structure_mixing(self, styled_img, original_structure):
        """ä¿æŒç»“æ„æ··åˆ - ç¡®ä¿ç‰©ä½“å’Œäººç‰©æ¸…æ™°å¯è¯†åˆ«"""
        # 1. æ£€æµ‹ç»“æ„ä¿¡æ¯
        gray_original = cv2.cvtColor(original_structure, cv2.COLOR_BGR2GRAY)
        gray_styled = cv2.cvtColor(styled_img, cv2.COLOR_BGR2GRAY)
        
        # 2. è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦
        # ä½¿ç”¨Sobelç®—å­æ£€æµ‹è¾¹ç¼˜
        edges_original = cv2.Sobel(gray_original, cv2.CV_64F, 1, 1, ksize=3)
        edges_styled = cv2.Sobel(gray_styled, cv2.CV_64F, 1, 1, ksize=3)
        
        # å½’ä¸€åŒ–è¾¹ç¼˜ä¿¡æ¯
        edges_original = np.abs(edges_original)
        edges_styled = np.abs(edges_styled)
        
        # 3. åˆ›å»ºç»“æ„ä¿æŒæ©ç 
        # åœ¨ç»“æ„ä¿¡æ¯å¼ºçš„åŒºåŸŸï¼Œæ›´å¤šåœ°ä¿ç•™åŸå§‹ç»“æ„
        structure_mask = edges_original / (edges_original.max() + 1e-8)
        structure_mask = np.clip(structure_mask * 2, 0, 1)  # å¢å¼ºç»“æ„åŒºåŸŸ
        
        # 4. åˆ†å±‚æ··åˆ
        result = np.zeros_like(styled_img)
        
        # åœ¨ç»“æ„å¼ºçš„åŒºåŸŸï¼Œæ›´å¤šåœ°ä¿ç•™åŸå§‹ç»“æ„
        for i in range(3):
            result[:, :, i] = (
                styled_img[:, :, i] * (1 - structure_mask * 0.3) + 
                original_structure[:, :, i] * structure_mask * 0.3
            )
        
        # 5. å±€éƒ¨å¯¹æ¯”åº¦å¢å¼º
        lab = cv2.cvtColor(result.astype(np.uint8), cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºå±€éƒ¨å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_enhanced = cv2.merge([l, a, b])
        final = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        return final
    
    def set_progress_callback(self, callback, task_id):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
        self.task_id = task_id

# åˆ›å»ºå¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢æ¨¡å‹
ghibli_enhanced_model = GhibliEnhancedTransfer()

def update_enhanced_progress(task_id, progress, current_step, total_steps, loss):
    """æ›´æ–°å¢å¼ºç‰ˆè½¬æ¢è¿›åº¦"""
    print(f"ğŸ“Š å¢å¼ºç‰ˆä»»åŠ¡ {task_id}: {progress}% (æ­¥éª¤ {current_step}/{total_steps}, æŸå¤±: {loss:.4f})")

def convert_image_enhanced_async(task_id, image):
    """å¼‚æ­¥è½¬æ¢å›¾åƒ - å¢å¼ºç‰ˆ"""
    try:
        # è®¾ç½®è¿›åº¦å›è°ƒ
        ghibli_enhanced_model.set_progress_callback(update_enhanced_progress, task_id)
        
        # å¼€å§‹è½¬æ¢
        result_image = ghibli_enhanced_model.apply_enhanced_ghibli_style(image)
        
        print(f"âœ… å¢å¼ºç‰ˆä»»åŠ¡ {task_id} è½¬æ¢å®Œæˆ")
        return result_image
        
    except Exception as e:
        print(f"âŒ å¢å¼ºç‰ˆä»»åŠ¡ {task_id} è½¬æ¢å¤±è´¥: {e}")
        # å›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
        from .real_ghibli_transfer import RealGhibliStyleTransfer
        ghibli_model = RealGhibliStyleTransfer()
        return ghibli_model.apply_real_ghibli_style(image, use_neural=False)

def apply_cartoon_effect(img_bgr):
    """åº”ç”¨å¡é€šåŒ–æ•ˆæœ - æ”¹è¿›ç‰ˆæœ¬ï¼Œä¿æŒæ¸…æ™°åº¦å’Œç»“æ„"""
    # 1. è½»åº¦åŒè¾¹æ»¤æ³¢ - å‡å°‘å¼ºåº¦ä»¥ä¿æŒç»†èŠ‚
    bilateral = cv2.bilateralFilter(img_bgr, 9, 60, 60)
    
    # 2. å¤šå±‚æ¬¡è¾¹ç¼˜æ£€æµ‹ - è·å–æ›´ä¸°å¯Œçš„è¾¹ç¼˜ä¿¡æ¯
    gray = cv2.cvtColor(bilateral, cv2.COLOR_BGR2GRAY)
    
    # Cannyè¾¹ç¼˜æ£€æµ‹ - è·å–ä¸»è¦è½®å»“
    edges_canny = cv2.Canny(gray, 30, 100)
    
    # è‡ªé€‚åº”é˜ˆå€¼ - è·å–ç»†èŠ‚è¾¹ç¼˜
    edges_adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                         cv2.THRESH_BINARY, 7, 2)
    
    # åˆå¹¶è¾¹ç¼˜ç»“æœ
    edges_combined = cv2.bitwise_or(edges_canny, edges_adaptive)
    
    # 3. å¢å¼ºé¢œè‰²é‡åŒ– - å¢åŠ èšç±»æ•°é‡ä¿æŒç»†èŠ‚
    data = bilateral.reshape((-1, 3))
    data = np.float32(data)
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
    _, labels, centers = cv2.kmeans(data, 16, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)  # å¢åŠ åˆ°16ä¸ªé¢œè‰²
    centers = np.uint8(centers)
    quantized = centers[labels.flatten()].reshape(bilateral.shape)
    
    # 4. æ™ºèƒ½è¾¹ç¼˜å åŠ  - ä¿æŒç»“æ„æ¸…æ™°
    edges_colored = cv2.cvtColor(edges_combined, cv2.COLOR_GRAY2BGR)
    
    # åˆ†å±‚æ··åˆï¼šä¸»è¦ä½¿ç”¨é‡åŒ–å›¾åƒï¼Œè½»åº¦å åŠ è¾¹ç¼˜
    cartoon = cv2.addWeighted(quantized, 0.9, edges_colored, 0.1, 0)
    
    # 5. è½»å¾®é”åŒ– - æ¢å¤ä¸€äº›æ¸…æ™°åº¦
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    sharpened = cv2.filter2D(cartoon, -1, kernel)
    
    # æ··åˆé”åŒ–ç»“æœ
    result = cv2.addWeighted(cartoon, 0.8, sharpened, 0.2, 0)
    
    return result

def apply_ghibli_color_style(img_bgr):
    """åº”ç”¨å®«å´éªè‰²å½©é£æ ¼ - æ”¹è¿›ç‰ˆæœ¬ï¼Œä¿æŒç»“æ„æ¸…æ™°"""
    # è½¬æ¢åˆ°HSVè‰²å½©ç©ºé—´
    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv)
    
    # é€‚åº¦å¢å¼ºé¥±å’Œåº¦ - é¿å…è¿‡åº¦é¥±å’Œå¯¼è‡´å¤±çœŸ
    s = cv2.add(s, 25)  # å›ºå®šå¢é‡è€Œéå€æ•°
    s = np.clip(s, 0, 240)
    
    # è½»å¾®è°ƒæ•´è‰²è°ƒ - åå‘æ¸©æš–è‰²è°ƒ
    h = cv2.add(h, 5)
    h = np.clip(h, 0, 179)
    
    # é€‚åº¦å¢å¼ºäº®åº¦
    v = cv2.add(v, 15)
    v = np.clip(v, 0, 255)
    
    # é‡æ–°ç»„åˆ
    hsv_enhanced = cv2.merge([h, s, v])
    result = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
    
    # åº”ç”¨LABè‰²å½©ç©ºé—´ç²¾è°ƒ - ä¿æŒè‡ªç„¶æ„Ÿ
    lab = cv2.cvtColor(result, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    
    # å¢å¼ºå¯¹æ¯”åº¦ä½†ä¸å¤±çœŸ
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    l = clahe.apply(l)
    
    # è½»å¾®è‰²å½©è°ƒæ•´
    a = cv2.add(a, 8)
    b = cv2.add(b, 5)
    a = np.clip(a, 0, 255)
    b = np.clip(b, 0, 255)
    
    lab_enhanced = cv2.merge([l, a, b])
    final = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
    
    return final

def apply_final_lighting(img_bgr):
    """åº”ç”¨æœ€ç»ˆå…‰ç…§ä¼˜åŒ– - æ”¹è¿›ç‰ˆæœ¬ï¼Œä¿æŒæ¸…æ™°åº¦"""
    height, width = img_bgr.shape[:2]
    
    # åˆ›å»ºæ›´è‡ªç„¶çš„å¾„å‘å…‰ç…§æ•ˆæœ - å‡å°‘å¼ºåº¦
    y_coords, x_coords = np.ogrid[:height, :width]
    center_y, center_x = height // 2, width // 2
    
    distance = np.sqrt((x_coords - center_x)**2 + (y_coords - center_y)**2)
    max_distance = np.sqrt(center_x**2 + center_y**2)
    
    # å…‰ç…§é®ç½© - æ›´æŸ”å’Œçš„å…‰ç…§æ•ˆæœ
    light_mask = 1.0 - (distance / max_distance) * 0.08  # å‡å°‘å…‰ç…§å¼ºåº¦
    light_mask = np.clip(light_mask, 0.92, 1.0)
    
    # åº”ç”¨å…‰ç…§
    result = img_bgr.astype(np.float32) * light_mask[:, :, np.newaxis]
    result = np.clip(result, 0, 255).astype(np.uint8)
    
    # è½»å¾®é”åŒ– - æ¢å¤ç»†èŠ‚
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    sharpened = cv2.filter2D(result, -1, kernel)
    
    # æ··åˆé”åŒ–ç»“æœ
    final = cv2.addWeighted(result, 0.9, sharpened, 0.1, 0)
    
    # éå¸¸è½»å¾®çš„æŸ”åŒ– - å»é™¤é”åŒ–å¸¦æ¥çš„å™ªç‚¹
    final = cv2.bilateralFilter(final, 3, 30, 30)
    
    return final