#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢ç³»ç»Ÿ
é›†æˆäººè„¸æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€èƒŒæ™¯å¤„ç†ç­‰ä¸“ä¸šåŠŸèƒ½
"""

import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, models
import os
import time
import threading

class GhibliEnhancedTransfer:
    """å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢ç³»ç»Ÿ
    é›†æˆäººè„¸æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€èƒŒæ™¯å¤„ç†ç­‰ä¸“ä¸šåŠŸèƒ½
    """
    
    def __init__(self, use_face_detection=True, use_background_separation=True):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.use_face_detection = use_face_detection
        self.use_background_separation = use_background_separation
        
        # åŠ è½½æ¨¡å‹
        self.face_detector = None
        self.seg_model = None
        self.face_landmark_model = None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()
        
        # è¿›åº¦å›è°ƒ
        self.progress_callback = None
        self.task_id = None
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
        print("ğŸ”§ åˆå§‹åŒ–å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢æ¨¡å‹...")
        
        # 1. äººè„¸æ£€æµ‹æ¨¡å‹
        if self.use_face_detection:
            self._initialize_face_detector()
        
        # 2. è¯­ä¹‰åˆ†å‰²æ¨¡å‹
        if self.use_background_separation:
            self._initialize_segmentation_model()
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_face_detector(self):
        """åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨"""
        try:
            # ä½¿ç”¨OpenCVçš„DNNæ¨¡å—åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹
            model_path = "models/face_detector/opencv_face_detector_uint8.pb"
            config_path = "models/face_detector/opencv_face_detector.pbtxt"
            
            # å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…ç½®çš„Haarçº§è”åˆ†ç±»å™¨
            if os.path.exists(model_path) and os.path.exists(config_path):
                self.face_detector = cv2.dnn.readNetFromTensorflow(model_path, config_path)
                print("âœ… åŠ è½½DNNäººè„¸æ£€æµ‹æ¨¡å‹æˆåŠŸ")
            else:
                # ä½¿ç”¨OpenCVå†…ç½®çš„Haarçº§è”åˆ†ç±»å™¨
                cascade_path = cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
                self.face_detector = cv2.CascadeClassifier(cascade_path)
                print("âœ… åŠ è½½Haarçº§è”äººè„¸æ£€æµ‹å™¨æˆåŠŸ")
                
        except Exception as e:
            print(f"âš ï¸ äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.face_detector = None
    
    def _initialize_segmentation_model(self):
        """åˆå§‹åŒ–è¯­ä¹‰åˆ†å‰²æ¨¡å‹"""
        try:
            # ä½¿ç”¨é¢„è®­ç»ƒçš„DeepLabV3æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²
            self.seg_model = models.segmentation.deeplabv3_resnet50(
                weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT
            ).eval().to(self.device)
            print("âœ… åŠ è½½è¯­ä¹‰åˆ†å‰²æ¨¡å‹æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰åˆ†å‰²æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.seg_model = None
    
    def detect_faces(self, image):
        """æ£€æµ‹å›¾åƒä¸­çš„äººè„¸"""
        if not self.use_face_detection or self.face_detector is None:
            return []
        
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                if len(img_np.shape) == 3:
                    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
                else:
                    gray = img_np
            else:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # æ£€æµ‹äººè„¸
            if isinstance(self.face_detector, cv2.dnn_Net):
                # DNNæ¨¡å‹æ£€æµ‹
                blob = cv2.dnn.blobFromImage(gray, 1.0, (300, 300), [104, 117, 123])
                self.face_detector.setInput(blob)
                detections = self.face_detector.forward()
                
                faces = []
                for i in range(detections.shape[2]):
                    confidence = detections[0, 0, i, 2]
                    if confidence > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                        x1 = int(detections[0, 0, i, 3] * gray.shape[1])
                        y1 = int(detections[0, 0, i, 4] * gray.shape[0])
                        x2 = int(detections[0, 0, i, 5] * gray.shape[1])
                        y2 = int(detections[0, 0, i, 6] * gray.shape[0])
                        faces.append((x1, y1, x2-x1, y2-y1))
            else:
                # Haarçº§è”æ£€æµ‹
                faces = self.face_detector.detectMultiScale(
                    gray, 
                    scaleFactor=1.1, 
                    minNeighbors=5, 
                    minSize=(30, 30)
                )
            
            print(f"ğŸ‘¤ æ£€æµ‹åˆ° {len(faces)} å¼ äººè„¸")
            return faces
            
        except Exception as e:
            print(f"âŒ äººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def segment_person(self, image):
        """è¯­ä¹‰åˆ†å‰²ï¼Œæå–äººç‰©å‰æ™¯"""
        if not self.use_background_separation or self.seg_model is None:
            # å¦‚æœæ²¡æœ‰åˆ†å‰²æ¨¡å‹ï¼Œè¿”å›å…¨å›¾æ©ç 
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                mask = np.ones(img_np.shape[:2], dtype=np.uint8) * 255
            else:
                mask = np.ones(image.shape[:2], dtype=np.uint8) * 255
            return mask
        
        try:
            # é¢„å¤„ç†å›¾åƒ
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                rgb = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            else:
                rgb = image
            
            # è°ƒæ•´å°ºå¯¸
            h, w = rgb.shape[:2]
            max_size = 520
            if max(h, w) > max_size:
                scale = max_size / max(h, w)
                new_w, new_h = int(w * scale), int(h * scale)
                rgb_resized = cv2.resize(rgb, (new_w, new_h))
            else:
                rgb_resized = rgb
            
            # è½¬æ¢ä¸ºtensor
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            input_tensor = transform(rgb_resized).unsqueeze(0).to(self.device)
            
            # æ¨ç†
            with torch.no_grad():
                output = self.seg_model(input_tensor)['out'][0]
            
            # è·å–äººç‰©ç±»åˆ«ï¼ˆCOCOæ•°æ®é›†ä¸­äººç‰©ç±»åˆ«ä¸º15ï¼‰
            person_class = 15
            mask = (output.argmax(0) == person_class).cpu().numpy().astype(np.uint8) * 255
            
            # è°ƒæ•´åˆ°åŸå›¾å°ºå¯¸
            mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # å¹³æ»‘è¾¹ç¼˜
            mask = cv2.GaussianBlur(mask, (9, 9), 0)
            
            print("ğŸ¯ è¯­ä¹‰åˆ†å‰²å®Œæˆ")
            return mask
            
        except Exception as e:
            print(f"âŒ è¯­ä¹‰åˆ†å‰²å¤±è´¥: {e}")
            # è¿”å›å…¨å›¾æ©ç ä½œä¸ºå›é€€
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                mask = np.ones(img_np.shape[:2], dtype=np.uint8) * 255
            else:
                mask = np.ones(image.shape[:2], dtype=np.uint8) * 255
            return mask
    
    def enhance_faces(self, image, face_regions):
        """å¢å¼ºäººè„¸åŒºåŸŸ - å®«å´éªé£æ ¼ç¾åŒ–"""
        if not face_regions:
            return image
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(image, Image.Image):
            img_np = np.array(image)
            if len(img_np.shape) == 3:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            else:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        else:
            img_bgr = image.copy()
        
        enhanced_image = img_bgr.copy()
        
        for (x, y, w, h) in face_regions:
            # æå–äººè„¸åŒºåŸŸ
            face_roi = img_bgr[y:y+h, x:x+w]
            
            if face_roi.size == 0:
                continue
            
            # åº”ç”¨å®«å´éªé£æ ¼çš„äººè„¸ç¾åŒ–
            enhanced_face = self._ghibli_face_enhancement(face_roi)
            
            # å°†å¢å¼ºåçš„äººè„¸æ”¾å›åŸå›¾
            enhanced_image[y:y+h, x:x+w] = enhanced_face
        
        return enhanced_image
    
    def _ghibli_face_enhancement(self, face_region):
        """å®«å´éªé£æ ¼çš„äººè„¸ç¾åŒ–"""
        # 1. çš®è‚¤å¹³æ»‘
        smoothed = cv2.bilateralFilter(face_region, 9, 75, 75)
        
        # 2. çœ¼ç›å¢å¼º
        enhanced_eyes = self._enhance_eyes(smoothed)
        
        # 3. å˜´å”‡å¢å¼º
        enhanced_lips = self._enhance_lips(enhanced_eyes)
        
        # 4. è‰²å½©è°ƒæ•´
        final_face = self._adjust_face_colors(enhanced_lips)
        
        return final_face
    
    def _enhance_eyes(self, face_region):
        """å¢å¼ºçœ¼ç›åŒºåŸŸ"""
        h, w = face_region.shape[:2]
        
        # å®šä¹‰çœ¼ç›åŒºåŸŸï¼ˆç›¸å¯¹ä½ç½®ï¼‰
        eye_top = int(h * 0.25)
        eye_bottom = int(h * 0.45)
        eye_left = int(w * 0.25)
        eye_right = int(w * 0.75)
        
        # å¢å¼ºçœ¼ç›åŒºåŸŸäº®åº¦
        eye_region = face_region[eye_top:eye_bottom, eye_left:eye_right]
        
        if eye_region.size > 0:
            # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´
            lab = cv2.cvtColor(eye_region, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            
            # å¢å¼ºäº®åº¦
            l = cv2.add(l, 20)
            l = np.clip(l, 0, 255)
            
            # åˆå¹¶é€šé“
            lab_enhanced = cv2.merge([l, a, b])
            enhanced_eyes = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
            
            # å°†å¢å¼ºåçš„çœ¼ç›åŒºåŸŸæ”¾å›
            face_region[eye_top:eye_bottom, eye_left:eye_right] = enhanced_eyes
        
        return face_region
    
    def _enhance_lips(self, face_region):
        """å¢å¼ºå˜´å”‡åŒºåŸŸ"""
        h, w = face_region.shape[:2]
        
        # å®šä¹‰å˜´å”‡åŒºåŸŸï¼ˆç›¸å¯¹ä½ç½®ï¼‰
        lip_top = int(h * 0.6)
        lip_bottom = int(h * 0.75)
        lip_left = int(w * 0.35)
        lip_right = int(w * 0.65)
        
        # å¢å¼ºå˜´å”‡åŒºåŸŸé¥±å’Œåº¦
        lip_region = face_region[lip_top:lip_bottom, lip_left:lip_right]
        
        if lip_region.size > 0:
            # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´
            hsv = cv2.cvtColor(lip_region, cv2.COLOR_BGR2HSV)
            h, s, v = cv2.split(hsv)
            
            # å¢å¼ºé¥±å’Œåº¦
            s = cv2.add(s, 30)
            s = np.clip(s, 0, 255)
            
            # åˆå¹¶é€šé“
            hsv_enhanced = cv2.merge([h, s, v])
            enhanced_lips = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
            
            # å°†å¢å¼ºåçš„å˜´å”‡åŒºåŸŸæ”¾å›
            face_region[lip_top:lip_bottom, lip_left:lip_right] = enhanced_lips
        
        return face_region
    
    def _adjust_face_colors(self, face_region):
        """è°ƒæ•´é¢éƒ¨è‰²å½© - å®«å´éªé£æ ¼"""
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´è¿›è¡Œç²¾ç¡®è°ƒæ•´
        lab = cv2.cvtColor(face_region, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # è°ƒæ•´äº®åº¦ï¼ˆå®«å´éªé£æ ¼çš®è‚¤æ˜äº®ï¼‰
        l = cv2.add(l, 10)
        l = np.clip(l, 0, 255)
        
        # è°ƒæ•´è‰²å½©å¹³è¡¡ï¼ˆåå‘æ¸©æš–è‰²è°ƒï¼‰
        a = cv2.add(a, 5)  # åå‘çº¢è‰²
        b = cv2.add(b, 3)  # åå‘é»„è‰²
        
        # åˆå¹¶é€šé“
        lab_balanced = cv2.merge([l, a, b])
        final_face = cv2.cvtColor(lab_balanced, cv2.COLOR_LAB2BGR)
        
        return final_face
    
    def process_background(self, image, person_mask, style_template="classic"):
        """å¤„ç†èƒŒæ™¯ - å®«å´éªé£æ ¼èƒŒæ™¯ä¼˜åŒ–"""
        if not self.use_background_separation:
            return image
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(image, Image.Image):
            img_np = np.array(image)
            if len(img_np.shape) == 3:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            else:
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        else:
            img_bgr = image.copy()
        
        # åˆ†ç¦»å‰æ™¯å’ŒèƒŒæ™¯
        foreground = img_bgr.copy()
        background = img_bgr.copy()
        
        # åˆ›å»ºèƒŒæ™¯æ©ç 
        bg_mask = cv2.bitwise_not(person_mask)
        
        # å¯¹èƒŒæ™¯åº”ç”¨å®«å´éªé£æ ¼å¤„ç†
        if style_template == "fantasy":
            bg_processed = self._fantasy_ghibli_style(background, intensity=0.6)
        elif style_template == "nostalgic":
            bg_processed = self._nostalgic_ghibli_style(background, intensity=0.7)
        elif style_template == "vibrant":
            bg_processed = self._vibrant_ghibli_style(background, intensity=0.8)
        else:
            bg_processed = self._classic_ghibli_style(background, intensity=0.5)
        
        # åˆå¹¶å‰æ™¯å’ŒèƒŒæ™¯
        result = np.zeros_like(img_bgr)
        
        # ä½¿ç”¨æ©ç æ··åˆå‰æ™¯å’ŒèƒŒæ™¯
        for i in range(3):  # å¯¹æ¯ä¸ªé€šé“è¿›è¡Œå¤„ç†
            result[:,:,i] = (
                foreground[:,:,i] * (person_mask / 255.0) + 
                bg_processed[:,:,i] * (bg_mask / 255.0)
            ).astype(np.uint8)
        
        return result
    
    def _classic_ghibli_style(self, image, intensity=0.5):
        """ç»å…¸å®«å´éªé£æ ¼èƒŒæ™¯"""
        # æŸ”å’Œè‰²å½©ï¼Œæ¢¦å¹»å…‰å½±
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦
        s = cv2.add(s, int(30 * intensity))
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒåå‘æ¸©æš–
        h = cv2.add(h, int(5 * intensity))
        h = np.clip(h, 0, 179)
        
        # åº”ç”¨æŸ”å’Œæ»¤é•œ
        hsv_enhanced = cv2.merge([h, s, v])
        enhanced = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        # æ·»åŠ æ¢¦å¹»å…‰å½±
        final = self._add_dreamy_lighting(enhanced, intensity)
        
        return final
    
    def _fantasy_ghibli_style(self, image, intensity=0.6):
        """æ¢¦å¹»å®«å´éªé£æ ¼èƒŒæ™¯"""
        # æ›´å¼ºçƒˆçš„è‰²å½©å’Œå…‰å½±æ•ˆæœ
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # å¢å¼ºè‰²å½©é²œè‰³åº¦
        a = cv2.add(a, int(40 * intensity))
        b = cv2.add(b, int(30 * intensity))
        
        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        lab_enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # æ›´å¼ºçš„æ¢¦å¹»å…‰å½±
        final = self._add_dreamy_lighting(enhanced, intensity * 1.2)
        
        return final
    
    def _nostalgic_ghibli_style(self, image, intensity=0.7):
        """æ€€æ—§å®«å´éªé£æ ¼èƒŒæ™¯"""
        # æŸ”å’Œçš„æ€€æ—§è‰²è°ƒ
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # é™ä½é¥±å’Œåº¦ï¼Œåˆ›é€ æ€€æ—§æ„Ÿ
        s = cv2.subtract(s, int(20 * intensity))
        s = np.clip(s, 0, 255)
        
        # è°ƒæ•´è‰²è°ƒåå‘æš–é»„
        h = cv2.add(h, int(10 * intensity))
        h = np.clip(h, 0, 179)
        
        hsv_nostalgic = cv2.merge([h, s, v])
        nostalgic = cv2.cvtColor(hsv_nostalgic, cv2.COLOR_HSV2BGR)
        
        # æ·»åŠ è½»å¾®çš„èƒ¶ç‰‡é¢—ç²’æ•ˆæœ
        noise = np.random.normal(0, 3 * intensity, nostalgic.shape).astype(np.uint8)
        final = cv2.add(nostalgic, noise)
        
        return final
    
    def _vibrant_ghibli_style(self, image, intensity=0.8):
        """é²œè‰³å®«å´éªé£æ ¼èƒŒæ™¯"""
        # é«˜é¥±å’Œåº¦é²œè‰³è‰²å½©
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¤§å¹…å¢å¼ºé¥±å’Œåº¦
        s = cv2.add(s, int(50 * intensity))
        s = np.clip(s, 0, 255)
        
        # å¢å¼ºäº®åº¦
        v = cv2.add(v, int(20 * intensity))
        v = np.clip(v, 0, 255)
        
        hsv_vibrant = cv2.merge([h, s, v])
        vibrant = cv2.cvtColor(hsv_vibrant, cv2.COLOR_HSV2BGR)
        
        # é”åŒ–å¢å¼ºç»†èŠ‚
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        final = cv2.filter2D(vibrant, -1, kernel)
        
        return final
    
    def _add_dreamy_lighting(self, image, intensity=1.0):
        """æ·»åŠ æ¢¦å¹»å…‰å½±æ•ˆæœ"""
        h, w = image.shape[:2]
        
        # åˆ›å»ºä¸­å¿ƒæ˜äº®çš„å…‰ç…§æ•ˆæœ
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # åˆ›å»ºå…‰ç…§é®ç½©
        light_mask = 1.0 - (distance / max_distance) * 0.1 * intensity
        light_mask = np.clip(light_mask, 0.9, 1.0)
        
        # åº”ç”¨å…‰ç…§æ•ˆæœ
        final = image.astype(np.float32) * light_mask[:,:,np.newaxis]
        final = np.clip(final, 0, 255).astype(np.uint8)
        
        return final
    
    def apply_enhanced_ghibli_style(self, image, use_face_enhancement=True, use_bg_separation=True):
        """åº”ç”¨å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢"""
        print("ğŸ¨ å¼€å§‹åº”ç”¨å¢å¼ºç‰ˆå®«å´éªé£æ ¼...")
        
        # æ›´æ–°è¿›åº¦
        if self.progress_callback and self.task_id:
            self.progress_callback(self.task_id, 10, 1, 10, 0)
        
        try:
            # 1. äººè„¸æ£€æµ‹
            faces = []
            if use_face_enhancement:
                faces = self.detect_faces(image)
                print(f"ğŸ‘¤ æ£€æµ‹åˆ° {len(faces)} å¼ äººè„¸")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 30, 2, 10, 0)
            
            # 2. è¯­ä¹‰åˆ†å‰²
            person_mask = None
            if use_bg_separation:
                person_mask = self.segment_person(image)
                print("ğŸ¯ è¯­ä¹‰åˆ†å‰²å®Œæˆ")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 50, 3, 10, 0)
            
            # 3. è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(image, Image.Image):
                img_np = np.array(image)
                if len(img_np.shape) == 3:
                    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
                else:
                    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
            else:
                img_bgr = image.copy()
            
            # 4. äººè„¸ç¾åŒ–
            if faces and use_face_enhancement:
                img_bgr = self.enhance_faces(img_bgr, faces)
                print("ğŸ’„ äººè„¸ç¾åŒ–å®Œæˆ")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 70, 4, 10, 0)
            
            # 5. èƒŒæ™¯å¤„ç†
            if person_mask is not None and use_bg_separation:
                img_bgr = self.process_background(img_bgr, person_mask)
                print("ğŸŒ… èƒŒæ™¯å¤„ç†å®Œæˆ")
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 90, 5, 10, 0)
            
            # 6. æœ€ç»ˆå®«å´éªé£æ ¼å¤„ç†
            from .real_ghibli_transfer import RealGhibliStyleTransfer
            ghibli_model = RealGhibliStyleTransfer()
            
            # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œå¤„ç†
            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(img_rgb)
            
            # åº”ç”¨å®«å´éªé£æ ¼
            result = ghibli_model.apply_real_ghibli_style(pil_image, use_neural=False)
            
            if self.progress_callback and self.task_id:
                self.progress_callback(self.task_id, 100, 10, 10, 0)
            
            print("âœ… å¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢å®Œæˆ")
            return result
            
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆé£æ ¼è½¬æ¢å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
            from .real_ghibli_transfer import RealGhibliStyleTransfer
            ghibli_model = RealGhibliStyleTransfer()
            return ghibli_model.apply_real_ghibli_style(image, use_neural=False)
    
    def set_progress_callback(self, callback, task_id):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
        self.task_id = task_id

# åˆ›å»ºå¢å¼ºç‰ˆå®«å´éªé£æ ¼è½¬æ¢æ¨¡å‹
ghibli_enhanced_model = GhibliEnhancedTransfer()

def update_enhanced_progress(task_id, progress, current_step, total_steps, loss):
    """æ›´æ–°å¢å¼ºç‰ˆè½¬æ¢è¿›åº¦"""
    print(f"ğŸ“Š å¢å¼ºç‰ˆä»»åŠ¡ {task_id}: {progress}% (æ­¥éª¤ {current_step}/{total_steps}, æŸå¤±: {loss:.4f})")

def convert_image_enhanced_async(task_id, image):
    """å¼‚æ­¥è½¬æ¢å›¾åƒ - å¢å¼ºç‰ˆ"""
    try:
        # è®¾ç½®è¿›åº¦å›è°ƒ
        ghibli_enhanced_model.set_progress_callback(update_enhanced_progress, task_id)
        
        # å¼€å§‹è½¬æ¢
        result_image = ghibli_enhanced_model.apply_enhanced_ghibli_style(image)
        
        print(f"âœ… å¢å¼ºç‰ˆä»»åŠ¡ {task_id} è½¬æ¢å®Œæˆ")
        return result_image
        
    except Exception as e:
        print(f"âŒ å¢å¼ºç‰ˆä»»åŠ¡ {task_id} è½¬æ¢å¤±è´¥: {e}")
        # å›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
        from .real_ghibli_transfer import RealGhibliStyleTransfer
        ghibli_model = RealGhibliStyleTransfer()
        return ghibli_model.apply_real_ghibli_style(image, use_neural=False)