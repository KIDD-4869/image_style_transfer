#!/usr/bin/env python3
"""
é«˜çº§åŠ¨æ¼«é£æ ¼å¤„ç†æ¨¡å—
å®ç°SLICè¶…åƒç´ åˆ†å‰²ã€XDoGçº¿æ¡æå–ã€å¤šå°ºåº¦èåˆç­‰å…³é”®æŠ€æœ¯
ä¸ºç«¯åˆ°ç«¯GANæ¶æ„å¥ å®šåŸºç¡€
"""

import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from skimage.segmentation import slic
from skimage.color import label2rgb
from skimage.util import img_as_float
import warnings
warnings.filterwarnings('ignore')

class AdvancedAnimeProcessor:
    """é«˜çº§åŠ¨æ¼«é£æ ¼å¤„ç†å™¨ - é•¿æœŸç›®æ ‡çš„æ ¸å¿ƒæ¨¡å—"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–å‚æ•° - ä¼˜åŒ–åŠ¨æ¼«è‰²å—æ•ˆæœ
        self.slic_segments = 300  # è¶…åƒç´ æ•°é‡ï¼Œå‡å°‘ä»¥è·å¾—æ›´å¤§çš„è‰²å—
        self.slic_compactness = 15  # ç´§å‡‘åº¦å‚æ•°ï¼Œé™ä½ä»¥è·å¾—æ›´è‡ªç„¶çš„è¾¹ç•Œ
        self.slic_sigma = 0.8  # é«˜æ–¯æ ¸æ ‡å‡†å·®ï¼Œé™ä½ä»¥ä¿æŒæ›´å¤šç»†èŠ‚
        
        # XDoGå‚æ•° - ä¼˜åŒ–çº¿æ¡æ¸…æ™°åº¦
        self.xdog_k = 1.6  # é«˜æ–¯æ¨¡ç³Šæ¯”ä¾‹ï¼Œé™ä½ä»¥å¢å¼ºçº¿æ¡ç»†èŠ‚
        self.xdog_sigma = 0.8  # åŸºç¡€é«˜æ–¯æ¨¡ç³Šæ ‡å‡†å·®ï¼Œé™ä½ä»¥ä¿æŒç»†èŠ‚
        self.xdog_epsilon = -0.15  # é˜ˆå€¼åç§»ï¼Œè°ƒæ•´ä»¥è·å¾—æ›´å¥½çš„çº¿æ¡å¯¹æ¯”åº¦
        self.xdog_phi = 15  # å¯¹æ¯”åº¦å¢å¼ºå‚æ•°ï¼Œå¢å¼ºä»¥è·å¾—æ›´æ¸…æ™°çš„çº¿æ¡
        
        # å¤šå°ºåº¦å‚æ•°
        self.pyramid_levels = 4  # é‡‘å­—å¡”å±‚æ•°
        self.scale_factor = 0.8  # ç¼©æ”¾å› å­
        
        print("ğŸ¨ é«˜çº§åŠ¨æ¼«é£æ ¼å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def slic_superpixel_segmentation(self, img_bgr):
        """
        SLICè¶…åƒç´ åˆ†å‰² - åˆ›å»ºè‡ªç„¶çš„åŠ¨æ¼«è‰²å—è¾¹ç•Œ
        
        Args:
            img_bgr: BGRæ ¼å¼å›¾åƒ
            
        Returns:
            segmented_img: åˆ†å‰²åçš„å›¾åƒ
            segments: åˆ†å‰²æ ‡ç­¾
        """
        try:
            # è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆskimageä½¿ç”¨RGBï¼‰
            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            img_float = img_as_float(img_rgb)
            
            # SLICè¶…åƒç´ åˆ†å‰²
            print(f"ğŸ”§ æ‰§è¡ŒSLICè¶…åƒç´ åˆ†å‰²ï¼Œç›®æ ‡åˆ†å‰²æ•°: {self.slic_segments}")
            segments = slic(
                img_float, 
                n_segments=self.slic_segments,
                compactness=self.slic_compactness,
                sigma=self.slic_sigma,
                start_label=1
            )
            
            # ç”Ÿæˆå¹³å‡é¢œè‰²çš„åˆ†å‰²å›¾åƒ
            segmented_rgb = label2rgb(segments, img_rgb, kind='avg')
            segmented_bgr = cv2.cvtColor((segmented_rgb * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)
            
            print(f"âœ… SLICåˆ†å‰²å®Œæˆï¼Œå®é™…åˆ†å‰²æ•°: {len(np.unique(segments))}")
            return segmented_bgr, segments
            
        except Exception as e:
            print(f"âŒ SLICåˆ†å‰²å¤±è´¥: {e}")
            # å›é€€åˆ°å‡å€¼æ¼‚ç§»æ»¤æ³¢
            try:
                fallback = cv2.pyrMeanShiftFiltering(img_bgr, 15, 30)
                print("âš ï¸ ä½¿ç”¨å‡å€¼æ¼‚ç§»æ»¤æ³¢ä½œä¸ºå›é€€æ–¹æ¡ˆ")
                return fallback, None
            except Exception as e2:
                print(f"âŒ å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
                return img_bgr, None
    
    def xdog_line_extraction(self, gray_img):
        """
        XDoGçº¿æ¡æå– - ç”Ÿæˆæ‰‹ç»˜æ„Ÿçš„åŠ¨æ¼«çº¿æ¡ï¼Œä¼˜åŒ–æ¸…æ™°åº¦
        
        Args:
            gray_img: ç°åº¦å›¾åƒ
            
        Returns:
            xdog_edges: XDoGè¾¹ç¼˜å›¾åƒ
        """
        try:
            print(f"ğŸ”§ æ‰§è¡ŒXDoGçº¿æ¡æå–")
            
            # é¢„å¤„ç†ï¼šè½»å¾®é”åŒ–å¢å¼ºè¾¹ç¼˜
            kernel_sharpen = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(gray_img.astype(np.uint8), -1, kernel_sharpen)
            gray_enhanced = cv2.addWeighted(gray_img, 0.7, sharpened, 0.3, 0)
            
            # ç¬¬ä¸€ä¸ªé«˜æ–¯æ¨¡ç³Š
            g1 = cv2.GaussianBlur(gray_enhanced.astype(np.float32), (0, 0), self.xdog_sigma)
            
            # ç¬¬äºŒä¸ªé«˜æ–¯æ¨¡ç³Šï¼ˆæ›´å¤§çš„sigmaï¼‰
            g2 = cv2.GaussianBlur(gray_enhanced.astype(np.float32), (0, 0), self.xdog_sigma * self.xdog_k)
            
            # è®¡ç®—å·®åˆ†
            difference = g1 - g2
            
            # å½’ä¸€åŒ–
            if np.max(np.abs(difference)) > 0:
                difference = difference / np.max(np.abs(difference))
            
            # XDoGå¤„ç† - ä¼˜åŒ–çº¿æ¡å¯¹æ¯”åº¦
            edges = np.ones_like(difference)
            
            # åº”ç”¨é˜ˆå€¼å’Œå¢å¼ºï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é˜ˆå€¼
            mask = difference < self.xdog_epsilon
            edges[mask] = 1 + np.tanh(self.xdog_phi * (difference[mask] - self.xdog_epsilon))
            
            # è½¬æ¢ä¸º0-255èŒƒå›´
            edges = (edges * 255).astype(np.uint8)
            
            # åè½¬è¾¹ç¼˜ï¼ˆçº¿æ¡ä¸ºé»‘è‰²ï¼ŒèƒŒæ™¯ä¸ºç™½è‰²ï¼‰
            xdog_edges = 255 - edges
            
            # åå¤„ç†ï¼šå¢å¼ºçº¿æ¡æ¸…æ™°åº¦
            # è½»å¾®è†¨èƒ€è®©çº¿æ¡æ›´è¿è´¯
            kernel = np.ones((2,2), np.uint8)
            xdog_edges = cv2.dilate(xdog_edges, kernel, iterations=1)
            
            # è½»å¾®è…èš€ä¿æŒçº¿æ¡ç²¾ç»†åº¦
            xdog_edges = cv2.erode(xdog_edges, kernel, iterations=1)
            
            # åº”ç”¨è‡ªé€‚åº”é˜ˆå€¼å¢å¼ºçº¿æ¡å¯¹æ¯”åº¦
            xdog_edges = cv2.adaptiveThreshold(xdog_edges, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                              cv2.THRESH_BINARY, 3, 2)
            
            print("âœ… XDoGçº¿æ¡æå–å®Œæˆ")
            return xdog_edges
            
        except Exception as e:
            print(f"âŒ XDoGçº¿æ¡æå–å¤±è´¥: {e}")
            # å›é€€åˆ°å¢å¼ºçš„è¾¹ç¼˜æ£€æµ‹
            try:
                # ä½¿ç”¨æ›´æ•æ„Ÿçš„Cannyå‚æ•°
                edges = cv2.Canny(gray_img, 30, 100)
                # è½»å¾®è†¨èƒ€è¿æ¥æ–­å¼€çš„çº¿æ¡
                kernel = np.ones((2,2), np.uint8)
                edges = cv2.dilate(edges, kernel, iterations=1)
                print("âš ï¸ ä½¿ç”¨å¢å¼ºCannyè¾¹ç¼˜æ£€æµ‹ä½œä¸ºå›é€€æ–¹æ¡ˆ")
                return edges
            except Exception as e2:
                print(f"âŒ å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
                return np.zeros_like(gray_img)
    
    def multi_scale_fusion(self, img_bgr, processing_func):
        """
        å¤šå°ºåº¦èåˆå¤„ç† - é‡‘å­—å¡”å¤„ç†ä¿æŒå…¨å±€å’Œå±€éƒ¨ç‰¹å¾
        
        Args:
            img_bgr: è¾“å…¥å›¾åƒ
            processing_func: å¤„ç†å‡½æ•°
            
        Returns:
            fused_result: èåˆç»“æœ
        """
        try:
            print(f"ğŸ”§ æ‰§è¡Œå¤šå°ºåº¦èåˆå¤„ç†ï¼Œå±‚æ•°: {self.pyramid_levels}")
            
            # æ„å»ºé«˜æ–¯é‡‘å­—å¡”
            pyramid = [img_bgr.copy()]
            current_img = img_bgr.copy()
            
            # ä¸‹é‡‡æ ·æ„å»ºé‡‘å­—å¡”
            for i in range(1, self.pyramid_levels):
                h, w = current_img.shape[:2]
                new_h, new_w = int(h * self.scale_factor), int(w * self.scale_factor)
                current_img = cv2.resize(current_img, (new_w, new_h), interpolation=cv2.INTER_AREA)
                pyramid.append(current_img)
            
            # åœ¨æ¯ä¸€å±‚è¿›è¡Œå¤„ç†
            processed_pyramid = []
            for i, level_img in enumerate(pyramid):
                print(f"   å¤„ç†ç¬¬ {i+1} å±‚ï¼Œå°ºå¯¸: {level_img.shape[:2]}")
                processed_level = processing_func(level_img)
                processed_pyramid.append(processed_level)
            
            # ä¸Šé‡‡æ ·å¹¶èåˆ
            result = processed_pyramid[-1].copy().astype(np.float32)
            
            for i in range(len(pyramid) - 2, -1, -1):
                h, w = pyramid[i].shape[:2]
                result_upsampled = cv2.resize(result, (w, h), interpolation=cv2.INTER_LINEAR)
                
                # åŠ æƒèåˆ - å¢å¼ºç»†èŠ‚ä¿æŒ
                weight = 0.6  # é™ä½ä¸Šä¸€å±‚æƒé‡ï¼Œå¢å¼ºå½“å‰å±‚ç»†èŠ‚
                result = result_upsampled * weight + processed_pyramid[i].astype(np.float32) * (1 - weight)
            
            fused_result = np.clip(result, 0, 255).astype(np.uint8)
            
            print("âœ… å¤šå°ºåº¦èåˆå®Œæˆ")
            return fused_result
            
        except Exception as e:
            print(f"âŒ å¤šå°ºåº¦èåˆå¤±è´¥: {e}")
            # å›é€€åˆ°å•å±‚å¤„ç†
            print("âš ï¸ ä½¿ç”¨å•å±‚å¤„ç†ä½œä¸ºå›é€€æ–¹æ¡ˆ")
            return processing_func(img_bgr)
    
    def intelligent_color_mapping(self, img_bgr, reference_images=None):
        """
        æ™ºèƒ½è‰²å½©æ˜ å°„ - åŸºäºå®«å´éªå‚è€ƒå›¾ç‰‡çš„ä¸“ä¸šè°ƒè‰²
        
        Args:
            img_bgr: è¾“å…¥å›¾åƒ
            reference_images: å‚è€ƒå›¾åƒåˆ—è¡¨
            
        Returns:
            color_mapped_img: è‰²å½©æ˜ å°„åçš„å›¾åƒ
        """
        try:
            print("ğŸ”§ æ‰§è¡Œæ™ºèƒ½è‰²å½©æ˜ å°„")
            
            # å¦‚æœæ²¡æœ‰æä¾›å‚è€ƒå›¾åƒï¼Œä½¿ç”¨é»˜è®¤çš„å®«å´éªè‰²å½©é£æ ¼
            if reference_images is None or (isinstance(reference_images, (list, tuple)) and len(reference_images) == 0):
                return self._apply_default_ghibli_palette(img_bgr)
            
            # åˆ†æå‚è€ƒå›¾åƒçš„è‰²å½©åˆ†å¸ƒ
            ref_hists = []
            for ref_img in reference_images:
                ref_hist = self._analyze_color_distribution(ref_img)
                ref_hists.append(ref_hist)
            
            # å¹³å‡å‚è€ƒå›¾åƒçš„è‰²å½©åˆ†å¸ƒ
            avg_ref_hist = np.mean(ref_hists, axis=0)
            
            # åº”ç”¨è‰²å½©æ˜ å°„
            color_mapped_img = self._apply_color_transfer(img_bgr, avg_ref_hist)
            
            print("âœ… æ™ºèƒ½è‰²å½©æ˜ å°„å®Œæˆ")
            return color_mapped_img
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½è‰²å½©æ˜ å°„å¤±è´¥: {e}")
            # å›é€€åˆ°é»˜è®¤å®«å´éªè°ƒè‰²æ¿
            print("âš ï¸ ä½¿ç”¨é»˜è®¤å®«å´éªè°ƒè‰²æ¿ä½œä¸ºå›é€€æ–¹æ¡ˆ")
            return self._apply_default_ghibli_palette(img_bgr)
    
    def _analyze_color_distribution(self, img_bgr):
        """åˆ†æå›¾åƒçš„è‰²å½©åˆ†å¸ƒ"""
        # è½¬æ¢åˆ°LABè‰²å½©ç©ºé—´è¿›è¡Œåˆ†æ
        lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)
        
        # è®¡ç®—æ¯ä¸ªé€šé“çš„ç›´æ–¹å›¾
        hist_l = cv2.calcHist([lab], [0], None, [256], [0, 256])
        hist_a = cv2.calcHist([lab], [1], None, [256], [0, 256])
        hist_b = cv2.calcHist([lab], [2], None, [256], [0, 256])
        
        # å½’ä¸€åŒ–ç›´æ–¹å›¾
        hist_l = hist_l / (img_bgr.shape[0] * img_bgr.shape[1])
        hist_a = hist_a / (img_bgr.shape[0] * img_bgr.shape[1])
        hist_b = hist_b / (img_bgr.shape[0] * img_bgr.shape[1])
        
        return np.array([hist_l.flatten(), hist_a.flatten(), hist_b.flatten()])
    
    def _apply_color_transfer(self, img_bgr, target_hist):
        """åº”ç”¨è‰²å½©ä¼ é€’"""
        # ç®€åŒ–çš„è‰²å½©ä¼ é€’å®ç°
        lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # åº”ç”¨ç›´æ–¹å›¾åŒ¹é…
        l_matched = self._histogram_match(l, target_hist[0])
        a_matched = self._histogram_match(a, target_hist[1])
        b_matched = self._histogram_match(b, target_hist[2])
        
        # é‡æ–°ç»„åˆ
        lab_matched = cv2.merge([l_matched, a_matched, b_matched])
        result = cv2.cvtColor(lab_matched, cv2.COLOR_LAB2BGR)
        
        return result
    
    def _histogram_match(self, source, target_hist):
        """ç›´æ–¹å›¾åŒ¹é…"""
        source_flat = source.flatten()
        source_hist = cv2.calcHist([source], [0], None, [256], [0, 256])
        source_hist = source_hist / source_flat.size
        
        # è®¡ç®—ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
        source_cdf = np.cumsum(source_hist)
        target_cdf = np.cumsum(target_hist)
        
        # åˆ›å»ºæ˜ å°„è¡¨
        lut = np.zeros(256, dtype=source.dtype)
        for i in range(256):
            diff = np.abs(target_cdf - source_cdf[i])
            lut[i] = np.argmin(diff)
        
        # åº”ç”¨æ˜ å°„
        matched = cv2.LUT(source, lut)
        return matched
    
    def _apply_default_ghibli_palette(self, img_bgr):
        """åº”ç”¨é»˜è®¤çš„å®«å´éªè°ƒè‰²æ¿ - ä¿æŒåŸå›¾è‰²å½©ç‰¹å¾"""
        # è½¬æ¢åˆ°LABè‰²å½©ç©ºé—´è¿›è¡Œæ›´ç²¾ç»†çš„è‰²å½©æ§åˆ¶
        lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # ä¿æŒåŸæœ‰äº®åº¦åˆ†å¸ƒï¼Œä»…åšè½»å¾®å¢å¼º
        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))
        l_enhanced = clahe.apply(l)
        
        # æ··åˆåŸå§‹å’Œå¢å¼ºçš„äº®åº¦ï¼Œä¿æŒè‡ªç„¶æ„Ÿ
        l_final = cv2.addWeighted(l, 0.7, l_enhanced, 0.3, 0)
        
        # è½»å¾®è°ƒæ•´è‰²å½©é€šé“ï¼Œå¢å¼ºå®«å´éªé£æ ¼ä½†ä¸è¿‡åº¦æ”¹å˜
        # aé€šé“ï¼ˆç»¿-çº¢ï¼‰è½»å¾®åæš–
        a_adjusted = cv2.add(a, 5)
        a_adjusted = np.clip(a_adjusted, 0, 255)
        
        # bé€šé“ï¼ˆè“-é»„ï¼‰è½»å¾®è°ƒæ•´
        b_adjusted = cv2.add(b, 3)
        b_adjusted = np.clip(b_adjusted, 0, 255)
        
        # é‡æ–°ç»„åˆLABå›¾åƒ
        lab_enhanced = cv2.merge([l_final, a_adjusted, b_adjusted])
        
        # è½¬æ¢å›BGR
        result = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        # è½¬æ¢åˆ°HSVè¿›è¡Œæœ€ç»ˆè°ƒè‰²
        hsv = cv2.cvtColor(result, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # è½»å¾®å¢å¼ºé¥±å’Œåº¦ï¼Œä¿æŒè‡ªç„¶
        s_enhanced = cv2.add(s, 10)
        s_enhanced = np.clip(s_enhanced, 0, 220)
        
        # è½»å¾®è°ƒæ•´äº®åº¦ï¼Œé¿å…è¿‡åº¦æ›å…‰
        v_enhanced = cv2.add(v, 8)
        v_enhanced = np.clip(v_enhanced, 0, 245)
        
        # æ··åˆåŸå§‹å’Œè°ƒæ•´åçš„é¥±å’Œåº¦ã€äº®åº¦
        s_final = cv2.addWeighted(s, 0.8, s_enhanced, 0.2, 0)
        v_final = cv2.addWeighted(v, 0.9, v_enhanced, 0.1, 0)
        
        # é‡æ–°ç»„åˆHSV
        hsv_final = cv2.merge([h, s_final, v_final])
        final_result = cv2.cvtColor(hsv_final, cv2.COLOR_HSV2BGR)
        
        # ä¸åŸå›¾è¿›è¡Œè‰²å½©ä¿æŒæ··åˆ
        final_result = cv2.addWeighted(img_bgr, 0.6, final_result, 0.4, 0)
        
        return final_result
    
    def process_anime_style(self, img_bgr, use_slic=True, use_xdog=True, use_multiscale=True, use_color_mapping=True):
        """
        å®Œæ•´çš„åŠ¨æ¼«é£æ ¼å¤„ç†æµç¨‹
        
        Args:
            img_bgr: è¾“å…¥å›¾åƒ
            use_slic: æ˜¯å¦ä½¿ç”¨SLICè¶…åƒç´ åˆ†å‰²
            use_xdog: æ˜¯å¦ä½¿ç”¨XDoGçº¿æ¡æå–
            use_multiscale: æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦èåˆ
            use_color_mapping: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½è‰²å½©æ˜ å°„
            
        Returns:
            result_img: å¤„ç†åçš„å›¾åƒ
        """
        print("ğŸ¨ å¼€å§‹é«˜çº§åŠ¨æ¼«é£æ ¼å¤„ç†...")
        
        result = img_bgr.copy()
        
        try:
            # 1. SLICè¶…åƒç´ åˆ†å‰²
            if use_slic:
                print("\nğŸ“ æ­¥éª¤1: SLICè¶…åƒç´ åˆ†å‰²")
                result, segments = self.slic_superpixel_segmentation(result)
            else:
                segments = None
            
            # 2. å®šä¹‰å¤šå°ºåº¦å¤„ç†å‡½æ•°
            def multiscale_process(img):
                # XDoGçº¿æ¡æå–
                if use_xdog:
                    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                    xdog_edges = self.xdog_line_extraction(gray)
                    edges_colored = cv2.cvtColor(xdog_edges, cv2.COLOR_GRAY2BGR)
                    
                    # çº¿æ¡å åŠ  - å¢å¼ºçº¿æ¡æ¸…æ™°åº¦
                    img_processed = cv2.addWeighted(img, 0.9, edges_colored, 0.1, 0)
                    return img_processed
                else:
                    return img
            
            # 3. å¤šå°ºåº¦èåˆå¤„ç†
            if use_multiscale:
                print("\nğŸ”€ æ­¥éª¤2: å¤šå°ºåº¦èåˆå¤„ç†")
                result = self.multi_scale_fusion(result, multiscale_process)
            elif use_xdog:
                print("\nâœï¸ æ­¥éª¤2: XDoGçº¿æ¡æå–")
                gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
                xdog_edges = self.xdog_line_extraction(gray)
                edges_colored = cv2.cvtColor(xdog_edges, cv2.COLOR_GRAY2BGR)
                result = cv2.addWeighted(result, 0.9, edges_colored, 0.1, 0)
            
            # 4. æ™ºèƒ½è‰²å½©æ˜ å°„
            if use_color_mapping:
                print("\nğŸ¨ æ­¥éª¤3: æ™ºèƒ½è‰²å½©æ˜ å°„")
                result = self.intelligent_color_mapping(result)
            
            # 5. æœ€ç»ˆä¼˜åŒ–
            print("\nâœ¨ æ­¥éª¤4: æœ€ç»ˆä¼˜åŒ–")
            result = self._final_optimization(result)
            
            print("âœ… é«˜çº§åŠ¨æ¼«é£æ ¼å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            print(f"âŒ é«˜çº§å¤„ç†å¤±è´¥: {e}")
            return img_bgr
    
    def _final_optimization(self, img_bgr):
        """æœ€ç»ˆä¼˜åŒ–å¤„ç†"""
        # è½»å¾®é”åŒ–
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(img_bgr, -1, kernel)
        
        # æ··åˆé”åŒ–ç»“æœ
        result = cv2.addWeighted(img_bgr, 0.9, sharpened, 0.1, 0)
        
        # è½»å¾®é™å™ª
        denoised = cv2.fastNlMeansDenoisingColored(result, None, 3, 3, 7, 21)
        
        return denoised

# åˆ›å»ºå…¨å±€å¤„ç†å™¨å®ä¾‹
advanced_processor = AdvancedAnimeProcessor()

def process_with_advanced_techniques(img_bgr, **kwargs):
    """
    ä½¿ç”¨é«˜çº§æŠ€æœ¯å¤„ç†å›¾åƒçš„ä¾¿æ·å‡½æ•°
    
    Args:
        img_bgr: è¾“å…¥å›¾åƒ
        **kwargs: å¤„ç†å‚æ•°
        
    Returns:
        å¤„ç†åçš„å›¾åƒ
    """
    return advanced_processor.process_anime_style(img_bgr, **kwargs)