#!/usr/bin/env python3
"""
å®«å´éªé£æ ¼è®­ç»ƒå™¨ - åŸºäº10ä¸‡å¼ å®«å´éªé£æ ¼å›¾ç‰‡è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import cv2
import numpy as np
from PIL import Image
import os
import glob
import time
import json
from pathlib import Path

class GhibliStyleDataset(Dataset):
    """å®«å´éªé£æ ¼æ•°æ®é›†"""
    
    def __init__(self, ghibli_dir, transform=None, max_samples=100000):
        self.ghibli_dir = ghibli_dir
        self.transform = transform
        self.max_samples = max_samples
        
        # æ”¶é›†å®«å´éªé£æ ¼å›¾ç‰‡
        self.image_paths = self._collect_ghibli_images()
        
        print(f"ğŸ¨ åŠ è½½äº† {len(self.image_paths)} å¼ å®«å´éªé£æ ¼å›¾ç‰‡")
    
    def _collect_ghibli_images(self):
        """æ”¶é›†å®«å´éªé£æ ¼å›¾ç‰‡"""
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.webp']
        image_paths = []
        
        for ext in image_extensions:
            pattern = os.path.join(self.ghibli_dir, '**', ext)
            image_paths.extend(glob.glob(pattern, recursive=True))
        
        # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°
        if len(image_paths) > self.max_samples:
            image_paths = image_paths[:self.max_samples]
        
        return image_paths
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾ç‰‡å¤±è´¥ {img_path}: {e}")
            # è¿”å›é»˜è®¤å›¾åƒ
            return self._create_default_image()
    
    def _create_default_image(self):
        """åˆ›å»ºé»˜è®¤å›¾åƒ"""
        default_img = np.ones((256, 256, 3), dtype=np.uint8) * 128
        default_img = Image.fromarray(default_img)
        
        if self.transform:
            default_img = self.transform(default_img)
        
        return default_img

class GhibliStyleEncoder(nn.Module):
    """å®«å´éªé£æ ¼ç¼–ç å™¨"""
    
    def __init__(self, feature_dim=512):
        super(GhibliStyleEncoder, self).__init__()
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„VGG19ä½œä¸ºç‰¹å¾æå–å™¨
        self.vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features
        
        # å†»ç»“VGGå‚æ•°
        for param in self.vgg.parameters():
            param.requires_grad = False
        
        # é£æ ¼ç‰¹å¾æå–å±‚
        self.style_layers = ['3', '8', '15', '22']
        
        # é£æ ¼ç‰¹å¾ç¼–ç å™¨
        self.style_encoder = nn.Sequential(
            nn.Linear(512 * 4, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, feature_dim)
        )
    
    def forward(self, x):
        # æå–VGGç‰¹å¾
        features = self._extract_vgg_features(x)
        
        # è®¡ç®—GramçŸ©é˜µä½œä¸ºé£æ ¼ç‰¹å¾
        style_features = []
        for layer_name, feature in features.items():
            if layer_name in self.style_layers:
                gram = self._gram_matrix(feature)
                style_features.append(gram)
        
        # æ‹¼æ¥æ‰€æœ‰é£æ ¼ç‰¹å¾
        if style_features:
            style_features = torch.cat([f.view(f.size(0), -1) for f in style_features], dim=1)
            # ç¼–ç é£æ ¼ç‰¹å¾
            encoded_style = self.style_encoder(style_features)
            return encoded_style
        else:
            return torch.zeros(x.size(0), 512, device=x.device)
    
    def _extract_vgg_features(self, x):
        """ä»VGGä¸­æå–ç‰¹å¾"""
        features = {}
        for name, layer in self.vgg._modules.items():
            x = layer(x)
            if name in self.style_layers:
                features[name] = x
        return features
    
    def _gram_matrix(self, x):
        """è®¡ç®—GramçŸ©é˜µ"""
        batch_size, channels, height, width = x.size()
        features = x.view(batch_size * channels, height * width)
        gram = torch.mm(features, features.t())
        return gram.div(batch_size * channels * height * width)

class GhibliStyleTrainer:
    """å®«å´éªé£æ ¼è®­ç»ƒå™¨"""
    
    def __init__(self, device=None):
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.encoder = GhibliStyleEncoder().to(self.device)
        self.optimizer = optim.Adam(self.encoder.parameters(), lr=0.001)
        self.criterion = nn.MSELoss()
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_losses = []
        self.val_losses = []
        self.best_loss = float('inf')
        
        print(f"ğŸ¨ åˆå§‹åŒ–å®«å´éªé£æ ¼è®­ç»ƒå™¨ (è®¾å¤‡: {self.device})")
    
    def train(self, train_loader, val_loader=None, epochs=100, save_dir="models/ghibli_style"):
        """è®­ç»ƒå®«å´éªé£æ ¼ç¼–ç å™¨"""
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒå®«å´éªé£æ ¼ç¼–ç å™¨ï¼Œå…± {epochs} ä¸ªå‘¨æœŸ")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.encoder.train()
            train_loss = 0.0
            
            for batch_idx, images in enumerate(train_loader):
                images = images.to(self.device)
                
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                encoded_features = self.encoder(images)
                
                # è®¡ç®—é‡å»ºæŸå¤±ï¼ˆè‡ªç¼–ç å™¨é£æ ¼ï¼‰
                # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ç‰¹å¾ä¸€è‡´æ€§æŸå¤±
                loss = self.criterion(encoded_features, encoded_features.detach())
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                train_loss += loss.item()
                
                if batch_idx % 100 == 0:
                    print(f"å‘¨æœŸ {epoch+1}/{epochs}, æ‰¹æ¬¡ {batch_idx}, æŸå¤±: {loss.item():.4f}")
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            self.train_losses.append(avg_train_loss)
            
            # éªŒè¯é˜¶æ®µ
            if val_loader:
                val_loss = self.validate(val_loader)
                self.val_losses.append(val_loss)
                
                print(f"å‘¨æœŸ {epoch+1}/{epochs} - è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, éªŒè¯æŸå¤±: {val_loss:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < self.best_loss:
                    self.best_loss = val_loss
                    self.save_model(os.path.join(save_dir, f"ghibli_style_encoder_best.pth"))
                    print("âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                print(f"å‘¨æœŸ {epoch+1}/{epochs} - è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            
            # æ¯10ä¸ªå‘¨æœŸä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if (epoch + 1) % 10 == 0:
                self.save_model(os.path.join(save_dir, f"ghibli_style_encoder_epoch_{epoch+1}.pth"))
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model(os.path.join(save_dir, "ghibli_style_encoder_final.pth"))
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history(save_dir)
        
        print("ğŸ‰ å®«å´éªé£æ ¼è®­ç»ƒå®Œæˆï¼")
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.encoder.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for images in val_loader:
                images = images.to(self.device)
                encoded_features = self.encoder(images)
                loss = self.criterion(encoded_features, encoded_features)
                val_loss += loss.item()
        
        return val_loss / len(val_loader)
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'encoder_state_dict': self.encoder.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_loss': self.best_loss
        }, filepath)
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_losses = checkpoint.get('train_losses', [])
        self.val_losses = checkpoint.get('val_losses', [])
        self.best_loss = checkpoint.get('best_loss', float('inf'))
        
        print(f"âœ… åŠ è½½å®«å´éªé£æ ¼æ¨¡å‹: {filepath}")
    
    def save_training_history(self, save_dir):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_loss': self.best_loss
        }
        
        with open(os.path.join(save_dir, 'training_history.json'), 'w') as f:
            json.dump(history, f, indent=2)
    
    def extract_ghibli_style_features(self, image):
        """æå–å®«å´éªé£æ ¼ç‰¹å¾"""
        self.encoder.eval()
        
        with torch.no_grad():
            if isinstance(image, Image.Image):
                # é¢„å¤„ç†å›¾åƒ
                transform = transforms.Compose([
                    transforms.Resize((256, 256)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                       std=[0.229, 0.224, 0.225])
                ])
                image_tensor = transform(image).unsqueeze(0).to(self.device)
            else:
                image_tensor = image.to(self.device)
            
            features = self.encoder(image_tensor)
            return features.cpu().numpy()

class GhibliStyleAnalyzer:
    """å®«å´éªé£æ ¼åˆ†æå™¨"""
    
    def __init__(self):
        self.ghibli_features = {}
        
    def analyze_ghibli_style(self, image_paths):
        """åˆ†æå®«å´éªé£æ ¼ç‰¹å¾"""
        print("ğŸ” åˆ†æå®«å´éªé£æ ¼ç‰¹å¾...")
        
        features = {
            'saturation': [],
            'brightness': [],
            'warmth': [],
            'color_palette': [],
            'edge_strength': [],
            'texture_smoothness': []
        }
        
        for img_path in image_paths[:1000]:  # åˆ†æå‰1000å¼ å›¾ç‰‡
            try:
                img = cv2.imread(img_path)
                if img is None:
                    continue
                
                # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´
                hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
                h, s, v = cv2.split(hsv)
                
                # è®¡ç®—é¥±å’Œåº¦
                features['saturation'].append(np.mean(s))
                
                # è®¡ç®—äº®åº¦
                features['brightness'].append(np.mean(v))
                
                # è®¡ç®—æ¸©æš–åº¦ï¼ˆæ©™è‰²/é»„è‰²åƒç´ æ¯”ä¾‹ï¼‰
                warm_pixels = np.sum((h > 10) & (h < 40))
                total_pixels = h.size
                features['warmth'].append(warm_pixels / total_pixels)
                
                # è®¡ç®—è¾¹ç¼˜å¼ºåº¦
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                edges = cv2.Canny(gray, 50, 150)
                features['edge_strength'].append(np.mean(edges))
                
                # è®¡ç®—çº¹ç†å¹³æ»‘åº¦
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                features['texture_smoothness'].append(1.0 / (1.0 + laplacian_var))
                
            except Exception as e:
                print(f"âŒ åˆ†æå›¾ç‰‡å¤±è´¥ {img_path}: {e}")
                continue
        
        # è®¡ç®—å¹³å‡ç‰¹å¾
        avg_features = {}
        for key, values in features.items():
            if values:
                avg_features[key] = np.mean(values)
            else:
                avg_features[key] = 0.0
        
        print("ğŸ“Š å®«å´éªé£æ ¼ç‰¹å¾åˆ†æç»“æœ:")
        for key, value in avg_features.items():
            print(f"  {key}: {value:.2f}")
        
        return avg_features

def create_ghibli_style_model():
    """åˆ›å»ºå®«å´éªé£æ ¼æ¨¡å‹"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    model_dir = "models/ghibli_style"
    model_path = os.path.join(model_dir, "ghibli_style_encoder_best.pth")
    
    if os.path.exists(model_path):
        print("âœ… åŠ è½½é¢„è®­ç»ƒçš„å®«å´éªé£æ ¼æ¨¡å‹")
        trainer = GhibliStyleTrainer()
        trainer.load_model(model_path)
        return trainer
    else:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾æå–å™¨")
        return None

def train_ghibli_style_model(ghibli_dir="ghibli_images", epochs=50):
    """è®­ç»ƒå®«å´éªé£æ ¼æ¨¡å‹"""
    print("ğŸ¯ å¼€å§‹è®­ç»ƒå®«å´éªé£æ ¼æ¨¡å‹...")
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = GhibliStyleDataset(ghibli_dir, transform, max_samples=10000)
    
    if len(dataset) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å®«å´éªé£æ ¼å›¾ç‰‡ï¼Œæ— æ³•è®­ç»ƒ")
        return None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GhibliStyleTrainer()
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, epochs=epochs)
    
    return trainer

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•å®«å´éªé£æ ¼è®­ç»ƒå™¨")
    
    # åˆ†æå®«å´éªé£æ ¼ç‰¹å¾
    analyzer = GhibliStyleAnalyzer()
    
    # æ”¶é›†å®«å´éªé£æ ¼å›¾ç‰‡
    ghibli_dir = "ghibli_images"
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
    image_paths = []
    
    for ext in image_extensions:
        pattern = os.path.join(ghibli_dir, '**', ext)
        image_paths.extend(glob.glob(pattern, recursive=True))
    
    if image_paths:
        features = analyzer.analyze_ghibli_style(image_paths)
        print("ğŸ‰ å®«å´éªé£æ ¼ç‰¹å¾åˆ†æå®Œæˆ")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°å®«å´éªé£æ ¼å›¾ç‰‡")
    
    print("ğŸš€ å®«å´éªé£æ ¼è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ")